{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# CNN-based GAN Baseline (Full Pipeline)\n",
                "\n",
                "**Features:**\n",
                "1. **Training**: Tanh-based GAN to prevent mode collapse.\n",
                "2. **Saving**: Saves 'best' (latest) generator checkpoint.\n",
                "3. **Generation**: Generates 10,000 synthetic samples.\n",
                "4. **Post-Processing**: Concatenates with REAL Time Features to create a complete dataset compatibile with Diffusion Models."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "from torch.utils.data import DataLoader, Dataset\n",
                "\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --------------------\n",
                "# 1. LOAD & PREPARE DATA\n",
                "# --------------------\n",
                "CSV_FILE = r'C:\\Users\\Raymond Tie\\Desktop\\DiffusionModel_NILM\\Data\\datasets\\dishwasher_multivariate.csv'\n",
                "OUTPUT_DIR = r'C:\\Users\\Raymond Tie\\Desktop\\DiffusionModel_NILM\\Synthetic_Data\\dishwasher'\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "\n",
                "WINDOW_SIZE = 512\n",
                "BATCH_SIZE = 64\n",
                "\n",
                "df = pd.read_csv(CSV_FILE)\n",
                "\n",
                "# Identify Power Column\n",
                "power_col = next((c for c in df.columns if 'power' in c.lower() or 'dishwasher' in c.lower()), df.columns[-1])\n",
                "print(f\"Target Power Column: {power_col}\")\n",
                "\n",
                "# Separate Power and Time Features\n",
                "# Assume all other columns are Time Features\n",
                "time_cols = [c for c in df.columns if c != power_col]\n",
                "print(f\"Time Columns: {time_cols}\")\n",
                "\n",
                "raw_power = df[power_col].values.astype(float)\n",
                "raw_time = df[time_cols].values.astype(float)\n",
                "\n",
                "# Normalize Power to [-1, 1]\n",
                "p_min, p_max = raw_power.min(), raw_power.max()\n",
                "raw_power_norm = (raw_power - p_min) / (p_max - p_min)\n",
                "raw_power_norm = raw_power_norm * 2 - 1\n",
                "\n",
                "class NILM_Dataset(Dataset):\n",
                "    def __init__(self, power, time):\n        self.windows = []\n        stride = 64\n        for i in range(0, len(power) - WINDOW_SIZE, stride):\n            p_win = power[i:i+WINDOW_SIZE]\n            t_win = time[i:i+WINDOW_SIZE]\n            \n            # Filter silent windows slightly\n            if np.max(p_win) > -0.9: \n                self.windows.append((p_win, t_win))\n        print(f\"Training Windows: {len(self.windows)}\")\n        \n    def __len__(self): return len(self.windows)\n    def __getitem__(self, idx):\n        p, t = self.windows[idx]\n        return torch.from_numpy(p).float().unsqueeze(0), torch.from_numpy(t).float()\n\ntrain_loader = DataLoader(NILM_Dataset(raw_power_norm, raw_time), batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --------------------\n",
                "# 2. MODEL DEFINITION\n",
                "# --------------------\n",
                "class Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        self.fc = nn.Linear(100, 128 * 16)\n        \n        def up_block(in_c, out_c):\n            return nn.Sequential(\n                nn.Upsample(scale_factor=2, mode='linear', align_corners=False),\n                nn.Conv1d(in_c, out_c, 3, 1, 1),\n                nn.BatchNorm1d(out_c),\n                nn.ReLU(True)\n            )\n\n        self.model = nn.Sequential(\n            up_block(128, 64),\n            up_block(64, 32),\n            up_block(32, 16),\n            up_block(16, 8),\n            nn.Upsample(scale_factor=2, mode='linear', align_corners=False),\n            nn.Conv1d(8, 1, 3, 1, 1),\n            nn.Tanh() # Output [-1, 1]\n        )\n\n    def forward(self, z):\n        x = self.fc(z).view(-1, 128, 16)\n        return self.model(x)\n\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv1d(1, 16, 4, 2, 1), nn.LeakyReLU(0.2),\n            nn.Conv1d(16, 32, 4, 2, 1), nn.BatchNorm1d(32), nn.LeakyReLU(0.2),\n            nn.Conv1d(32, 64, 4, 2, 1), nn.BatchNorm1d(64), nn.LeakyReLU(0.2),\n            nn.Conv1d(64, 128, 4, 2, 1), nn.BatchNorm1d(128), nn.LeakyReLU(0.2),\n            nn.AdaptiveAvgPool1d(1), nn.Flatten(), nn.Linear(128, 1), nn.Sigmoid()\n        )\n    def forward(self, x):\n        return self.conv(x)\n\nG = Generator().to(device)\nD = Discriminator().to(device)\nopt_G = torch.optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\nopt_D = torch.optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\ncriterion = nn.BCELoss()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --------------------\n",
                "# 3. TRAINING LOOP (With Save)\n",
                "# --------------------\n",
                "from IPython.display import clear_output\n",
                "\n",
                "def plot_status(real, fake, epoch, d_loss, g_loss):\n    clear_output(wait=True)\n    r = (real[0,0].detach().cpu().numpy() + 1) / 2\n    f = (fake[0,0].detach().cpu().numpy() + 1) / 2\n    \n    plt.figure(figsize=(15, 6))\n    plt.suptitle(f\"Epoch {epoch} | D: {d_loss:.4f} G: {g_loss:.4f}\")\n    plt.subplot(1,2,1)\n    plt.plot(r, label='Real (Rescaled)', alpha=0.6)\n    plt.plot(f, label='Fake (Rescaled)', alpha=0.8)\n    plt.legend(); plt.ylim(-0.1, 1.1)\n    plt.show()\n\nprint(\"Training...\")\nfor epoch in range(1, 101): # 100 Epochs is usually enough for simple CNN\n    for batch_idx, (p, t) in enumerate(train_loader):\n        real = p.to(device)\n        bs = real.size(0)\n        \n        # Train D\n        opt_D.zero_grad()\n        z = torch.randn(bs, 100).to(device)\n        fake = G(z)\n        loss_d = (criterion(D(real), torch.full((bs,1), 0.9).to(device)) + \\\n                  criterion(D(fake.detach()), torch.zeros(bs,1).to(device))) / 2\n        loss_d.backward()\n        opt_D.step()\n        \n        # Train G (3 times)\n        for _ in range(3):\n            opt_G.zero_grad()\n            z = torch.randn(bs, 100).to(device)\n            fake = G(z)\n            loss_g = criterion(D(fake), torch.ones(bs,1).to(device))\n            loss_g.backward()\n            opt_G.step()\n        \n    if epoch % 5 == 0:\n        plot_status(real, fake, epoch, loss_d.item(), loss_g.item())\n        \n    # Checkpoint every 50 epochs\n    if epoch % 50 == 0:\n        torch.save(G.state_dict(), os.path.join(OUTPUT_DIR, f'generator_epoch_{epoch}.pth'))\n\n# Save Final Model\ntorch.save(G.state_dict(), os.path.join(OUTPUT_DIR, 'generator_latest.pth'))\nprint(\"✅ Training Complete. Model Saved.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --------------------\n",
                "# 4. GENERATION & CONCATENATION\n",
                "# --------------------\n",
                "print(\"Generating Synthetic Data...\")\n",
                "G.eval()\n",
                "NUM_SAMPLES = 10000\n",
                "generated_samples = []\n",
                "\n",
                "# We need to pair generated power with REAL time features.\n",
                "# Strategy: Randomly sample windows of Time Features from the real dataset.\n",
                "\n",
                "with torch.no_grad():\n    for _ in range(NUM_SAMPLES // BATCH_SIZE + 1):\n        z = torch.randn(BATCH_SIZE, 100).to(device)\n        fake_power = G(z).cpu().numpy() # (B, 1, 512)\n        \n        # Rescale back from [-1, 1] to [0, 1] (or original scale)\n        fake_power = (fake_power + 1) / 2\n        fake_power = fake_power.squeeze(1) # (B, 512)\n        \n        generated_samples.append(fake_power)\n\n# Concatenate all batches\nall_power = np.concatenate(generated_samples, axis=0)[:NUM_SAMPLES] # (10000, 512)\n\n# Get Random Time Feature Windows\n# We reload dataset to get just time windows\nall_time_windows = []\npossible_indices = list(range(len(train_loader.dataset)))\nselected_indices = np.random.choice(possible_indices, NUM_SAMPLES, replace=True)\n\nfor idx in selected_indices:\n    _, t_win = train_loader.dataset[idx]\n    all_time_windows.append(t_win.numpy()) # (512, TimeDim)\n\nall_time = np.array(all_time_windows) # (10000, 512, TimeDim)\n\nprint(f\"Generated Power Shape: {all_power.shape}\")\nprint(f\"Sampled Time Shape:    {all_time.shape}\")\n\n# --------------------\n# 5. MERGE & SAVE\n# --------------------\n# Combine (N, L, 1) and (N, L, T) -> (N, L, 1+T)\nall_power_expanded = np.expand_dims(all_power, axis=2)\nfinal_data = np.concatenate([all_power_expanded, all_time], axis=2)\n\nprint(f\"Final Dataset Shape: {final_data.shape}\")\n\n# Save as .npy\nsave_path = os.path.join(OUTPUT_DIR, 'synthetic_dishwasher.npy')\nnp.save(save_path, final_data)\nprint(f\"✅ Saved Synthetic Dataset to: {save_path}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}