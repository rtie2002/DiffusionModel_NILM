# DiffusionModel_NILM å®Œæ•´æ•°æ®æµç¨‹å›¾ï¼ˆè¶…è¯¦ç»†ç‰ˆï¼‰

## ğŸ“š å®Œæ•´æ¨¡å—ç´¢å¼•

### Models ç›®å½•ç»“æ„
```
Models/
â”œâ”€â”€ diffusion/
â”‚   â”œâ”€â”€ agent_transformer.py    â† DiT ä¸»æ¨¡å‹ (Transformeréª¨å¹²)
â”‚   â”œâ”€â”€ gaussian_diffusion.py   â† Diffusion åŒ…è£…å™¨ (åŠ å™ª/å»å™ªé€»è¾‘)
â”‚   â””â”€â”€ model_utils.py          â† å·¥å…·å‡½æ•° (ä½ç½®ç¼–ç , AdaLNç­‰)
â””â”€â”€ ts2vec/                     â† (æœªä½¿ç”¨åœ¨æ­¤æµç¨‹)
```

### æ–‡ä»¶åŠŸèƒ½è¯´æ˜
| æ–‡ä»¶ | ä½œç”¨ | å…³é”®ç±»/å‡½æ•° |
|------|------|------------|
| `agent_transformer.py` | DiT æ ¸å¿ƒ | `Transformer`, `DiTBlock`, AdaLN-Zero |
| `gaussian_diffusion.py` | æ‰©æ•£åŒ…è£…å™¨ | `Diffusion.forward`, `q_sample`, `p_sample` |
| `model_utils.py` | å·¥å…·ç»„ä»¶ | `SinusoidalPosEmb`, `AdaLayerNorm`, `extract` |
| `real_datasets.py` | æ•°æ®é¢„å¤„ç† | `load_csv`, `minmax_scaler`, `create_windows` |
| `solver.py` | è®­ç»ƒ/é‡‡æ ·æ§åˆ¶ | `Trainer.train`, `Trainer.sample` |
| `main.py` | é¡¹ç›®å…¥å£ | `parse_args`, `load_config` |

---

## å›¾ä¾‹è¯´æ˜

- ğŸŸ¦ **è“è‰²æ¡†**ï¼šæ•°æ®é¢„å¤„ç† (`real_datasets.py`)
- ğŸŸ© **ç»¿è‰²æ¡†**ï¼šæ¨¡å‹å‰å‘ä¼ æ’­ (`agent_transformer.py`)
- ğŸŸ¨ **é»„è‰²æ¡†**ï¼šè®­ç»ƒ/é‡‡æ ·æ§åˆ¶ (`solver.py`)
- ğŸŸª **ç´«è‰²æ¡†**ï¼šAdaLN-Zero æœºåˆ¶
- ğŸŸ§ **æ©™è‰²æ¡†**ï¼šå…¥å£/é…ç½® (`main.py`)

---

```mermaid
flowchart TB
    %% ========== å…¥å£å±‚ ==========
    subgraph ENTRY ["ğŸŸ§ å…¥å£å±‚ (main.py)"]
        A1["è¿è¡Œå‘½ä»¤:<br/>python main.py --config Config/microwave.yaml --sample"]
        A2["è§£æå‚æ•°:<br/>â€¢ config_path<br/>â€¢ --train/--sample<br/>â€¢ --sample_num<br/>â€¢ --device"]
        A3["åŠ è½½ YAML é…ç½®:<br/>load_config()"]
        A1 --> A2 --> A3
    end

    %% ========== æ•°æ®é¢„å¤„ç†å±‚ ==========
    subgraph PREPROCESS ["ğŸŸ¦ æ•°æ®é¢„å¤„ç† (Utils/Data_utils/real_datasets.py)"]
        direction TB
        
        subgraph P1 ["æ­¥éª¤ 1: è¯»å–åŸå§‹ CSV"]
            P1A["load_csv(csv_path)<br/>â†“<br/>np.loadtxt(delimiter=',', skiprows=1)"]
            P1B["è¾“å‡º: np.ndarray<br/>shape: (N, 9)<br/>9 = 1åŠŸç‡ + 8æ—¶é—´ç‰¹å¾"]
            P1A --> P1B
        end
        
        subgraph P2 ["æ­¥éª¤ 2: Min-Max å½’ä¸€åŒ–"]
            P2A["minmax_scaler(arr)<br/>â†“<br/>å¯¹æ¯ä¸€åˆ—: (x - min) / (max - min + 1e-7)"]
            P2B["è¾“å‡º:<br/>â€¢ scaled: (N, 9)<br/>â€¢ min_val: (1, 9)<br/>â€¢ max_val: (1, 9)"]
            P2A --> P2B
        end
        
        subgraph P3 ["æ­¥éª¤ 3: çª—å£åˆ‡åˆ†"]
            P3A["create_windows(arr, seq_len=512, style='non_overlapping')<br/>â†“<br/>æ¯ 512 ä¸ªç‚¹ä¸ºä¸€ä¸ªçª—å£"]
            P3B["è¾“å‡º: np.ndarray<br/>shape: (W, 512, 9)<br/>W = çª—å£æ•°é‡"]
            P3A --> P3B
        end
        
        subgraph P4 ["æ­¥éª¤ 4: è½¬ PyTorch Dataset"]
            P4A["NILMDataset(windows)<br/>â†“<br/>torch.from_numpy().float()"]
            P4B["DataLoader<br/>â†“<br/>batch_size=64, shuffle=True"]
            P4C["è¾“å‡º: batch<br/>shape: (B, L, 9)<br/>B=64, L=512"]
            P4A --> P4B --> P4C
        end
        
        P1 --> P2 --> P3 --> P4
    end

    %% ========== è®­ç»ƒ/é‡‡æ ·æ§åˆ¶å±‚ ==========
    subgraph SOLVER ["ğŸŸ¨ è®­ç»ƒ/é‡‡æ ·æ§åˆ¶ (engine/solver.py)"]
        direction TB
        
        subgraph S_TRAIN ["è®­ç»ƒæ¨¡å¼: Trainer.train()"]
            ST1["ä» DataLoader è·å– batch<br/>shape: (B, L, 9)"]
            ST2["éšæœºé‡‡æ · diffusion step<br/>t ~ Uniform(0, T-1)<br/>shape: (B,)"]
            ST3["åŠ å™ª: LinearScheduler.q_sample(batch, t)<br/>å…¬å¼: x_t = âˆšÎ±Ì…_t Â· x0 + âˆš(1-Î±Ì…_t) Â· Îµ<br/>è¾“å‡º: x_t (B,L,9), Îµ (B,L,9)"]
            ST4["æå–æ¡ä»¶å‘é‡<br/>c = batch[..., 1:].mean(dim=1)<br/>shape: (B, 8)"]
            ST5["ğŸ“ è°ƒç”¨æ¨¡å‹å‰å‘ä¼ æ’­<br/>eps_pred = DiT.forward(x_t, t, c)<br/>shape: (B, L, 9)"]
            ST6["è®¡ç®— MSE Loss<br/>loss = ((eps_pred - Îµ)Â²).mean()"]
            ST7["åå‘ä¼ æ’­ & å‚æ•°æ›´æ–°<br/>optimizer.step()"]
            
            ST1 --> ST2 --> ST3 --> ST4 --> ST5 --> ST6 --> ST7
        end
        
        subgraph S_SAMPLE ["é‡‡æ ·æ¨¡å¼: Trainer.sample()"]
            SS1["åˆå§‹åŒ–çº¯å™ªå£°<br/>x = torch.randn(N, L, 9)"]
            SS2["é€†æ‰©æ•£å¾ªç¯: t = T-1 â†’ 0"]
            SS3["æ„é€ ç›®æ ‡æ—¶é—´æ¡ä»¶<br/>c = build_condition(target_time)<br/>shape: (N, 8)"]
            SS4["ğŸ“ è°ƒç”¨æ¨¡å‹å‰å‘ä¼ æ’­<br/>eps_pred = DiT.forward(x_t, t, c)<br/>shape: (N, L, 9)"]
            SS5["é€†æ‰©æ•£å…¬å¼<br/>x_{t-1} = (1/âˆšÎ±_t)(x_t - (Î²_t/âˆš(1-Î±Ì…_t))Â·ÎµÌ‚) + Ïƒ_tÂ·z"]
            SS6["åå½’ä¸€åŒ–<br/>x_real = x_0 Â· (max - min) + min"]
            SS7["ä¿å­˜ä¸º .npy æ–‡ä»¶<br/>np.save('sample_i.npy', x_real[i])"]
            
            SS1 --> SS2 --> SS3 --> SS4 --> SS5 --> SS6 --> SS7
        end
    end

    %% ========== æ¨¡å‹å±‚ ==========
    subgraph MODEL ["ğŸŸ© DiT æ¨¡å‹ (Models/diffusion/agent_transformer.py)"]
        direction TB
        
        M_INPUT["ğŸ”¹ è¾“å…¥:<br/>â€¢ x_t: (B, L, 9)<br/>â€¢ t: (B,)<br/>â€¢ c: (B, 8)"]
        
        %% æ¡ä»¶åµŒå…¥
        subgraph M_COND ["æ¡ä»¶åµŒå…¥æ¨¡å—"]
            direction LR
            MC1["TimestepEmbedding(t)<br/>â†“<br/>Sinusoidal + MLP<br/>è¾“å‡º: t_emb (B, hidden_dim)"]
            MC2["Linear Projection<br/>c_emb = Linear(c)<br/>è¾“å‡º: (B, hidden_dim)"]
            MC3["èåˆæ¡ä»¶<br/>cond = t_emb + c_emb<br/>shape: (B, hidden_dim)"]
            MC1 --> MC3
            MC2 --> MC3
        end
        
        %% è¾“å…¥æŠ•å½±
        M_PROJ["è¾“å…¥æŠ•å½±<br/>x = Linear(9 â†’ hidden_dim)(x_t)<br/>è¾“å‡º: (B, L, hidden_dim)"]
        
        M_INPUT --> M_COND
        M_INPUT --> M_PROJ
        
        %% DiT Block å¾ªç¯
        M_LOOP["è¿›å…¥ N ä¸ª DiT Block å¾ªç¯<br/>(N = num_layers, é»˜è®¤ 12)"]
        
        M_COND --> M_LOOP
        M_PROJ --> M_LOOP
        
        %% DiT Block è¯¦ç»†ç»“æ„
        subgraph DIT_BLOCK ["ğŸŸª å•ä¸ª DiT Block (åŒ…å« AdaLN-Zero)"]
            direction TB
            
            DB_IN["è¾“å…¥:<br/>â€¢ x: (B, L, hidden_dim)<br/>â€¢ cond: (B, hidden_dim)"]
            
            %% Modulation Network
            subgraph ADALN_MOD ["AdaLN-Zero Modulation Network"]
                AM1["è¾“å…¥: cond (B, hidden_dim)"]
                AM2["SiLU æ¿€æ´»"]
                AM3["Linear(hidden_dim â†’ 6Ã—hidden_dim)<br/>âš ï¸ æƒé‡åˆå§‹åŒ–ä¸º 0"]
                AM4["Split æˆ 6 ä»½:<br/>shift_msa, scale_msa, gate_msa<br/>shift_mlp, scale_mlp, gate_mlp<br/>æ¯ä»½: (B, hidden_dim)"]
                AM1 --> AM2 --> AM3 --> AM4
            end
            
            %% First Path: MSA
            subgraph MSA_PATH ["è·¯å¾„ 1: Multi-Head Self-Attention"]
                direction TB
                MSA1["LayerNorm(x)<br/>elementwise_affine=False<br/>x_norm = (x - Î¼) / Ïƒ"]
                MSA2["ğŸŸª AdaLN è°ƒåˆ¶<br/>x_mod = x_norm Â· (1 + scale_msa) + shift_msa<br/>â¬…ï¸ ä½¿ç”¨ scale_msa, shift_msa"]
                MSA3["Multi-Head Attention<br/>attn_out = Attention(x_mod, x_mod, x_mod)<br/>num_heads = 8"]
                MSA4["ğŸŸª Gate æ§åˆ¶<br/>gated_attn = gate_msa Â· attn_out<br/>â¬…ï¸ ä½¿ç”¨ gate_msa"]
                MSA5["Residual è¿æ¥<br/>x = x + gated_attn"]
                
                MSA1 --> MSA2 --> MSA3 --> MSA4 --> MSA5
            end
            
            %% Second Path: MLP
            subgraph MLP_PATH ["è·¯å¾„ 2: Feed-Forward MLP"]
                direction TB
                MLP1["LayerNorm(x)<br/>elementwise_affine=False"]
                MLP2["ğŸŸª AdaLN è°ƒåˆ¶<br/>x_mod = x_norm Â· (1 + scale_mlp) + shift_mlp<br/>â¬…ï¸ ä½¿ç”¨ scale_mlp, shift_mlp"]
                MLP3["Feed-Forward Network<br/>Linear(hidden_dim â†’ hidden_dimÃ—4)<br/>â†’ GELU<br/>â†’ Linear(hidden_dimÃ—4 â†’ hidden_dim)"]
                MLP4["ğŸŸª Gate æ§åˆ¶<br/>gated_mlp = gate_mlp Â· mlp_out<br/>â¬…ï¸ ä½¿ç”¨ gate_mlp"]
                MLP5["Residual è¿æ¥<br/>x = x + gated_mlp"]
                
                MLP1 --> MLP2 --> MLP3 --> MLP4 --> MLP5
            end
            
            DB_IN --> ADALN_MOD
            DB_IN --> MSA_PATH
            MSA_PATH --> MLP_PATH
            ADALN_MOD -.->|æä¾› 6 ä¸ªè°ƒåˆ¶å‚æ•°| MSA2
            ADALN_MOD -.->|æä¾› 6 ä¸ªè°ƒåˆ¶å‚æ•°| MSA4
            ADALN_MOD -.->|æä¾› 6 ä¸ªè°ƒåˆ¶å‚æ•°| MLP2
            ADALN_MOD -.->|æä¾› 6 ä¸ªè°ƒåˆ¶å‚æ•°| MLP4
            
            MLP_PATH --> DB_OUT["è¾“å‡º: x (B, L, hidden_dim)"]
        end
        
        M_LOOP --> DIT_BLOCK
        DIT_BLOCK --> M_NEXT{è¿˜æœ‰ä¸‹ä¸€ä¸ª Block?}
        M_NEXT -->|æ˜¯| DIT_BLOCK
        M_NEXT -->|å¦| M_FINAL
        
        %% æœ€ç»ˆè¾“å‡º
        subgraph M_FINAL ["æœ€ç»ˆè¾“å‡ºå±‚"]
            MF1["Final LayerNorm<br/>x = LayerNorm(x)"]
            MF2["Output Projection<br/>Linear(hidden_dim â†’ 9)"]
            MF3["ğŸ”¹ è¾“å‡º: ÎµÌ‚ (é¢„æµ‹çš„å™ªå£°)<br/>shape: (B, L, 9)"]
            MF1 --> MF2 --> MF3
        end
    end

    %% ========== è¿æ¥å„å±‚ ==========
    ENTRY --> PREPROCESS
    PREPROCESS --> SOLVER
    SOLVER --> MODEL
    
    %% ========== æ ·å¼ ==========
    style ENTRY fill:#ffe4b5,stroke:#ff8c00,stroke-width:3px
    style PREPROCESS fill:#e6f3ff,stroke:#0066cc,stroke-width:3px
    style SOLVER fill:#fff9e6,stroke:#ccaa00,stroke-width:3px
    style MODEL fill:#e6ffe6,stroke:#00aa00,stroke-width:3px
    style DIT_BLOCK fill:#f3e5f5,stroke:#9c27b0,stroke-width:3px
    style ADALN_MOD fill:#ede7f6,stroke:#673ab7,stroke-width:2px
    style MSA_PATH fill:#e8eaf6,stroke:#3f51b5,stroke-width:2px
    style MLP_PATH fill:#e8eaf6,stroke:#3f51b5,stroke-width:2px
```

---

## ğŸ“‹ è¯¦ç»†æ–‡ä»¶åŠŸèƒ½è¯´æ˜

### 1ï¸âƒ£ **main.py** (æ©™è‰²æ¡†)
- **ä½œç”¨**ï¼šé¡¹ç›®æ€»å…¥å£
- **å…³é”®å‡½æ•°**ï¼š
  - `parse_args()`: è§£æå‘½ä»¤è¡Œå‚æ•°
  - `load_config()`: è¯»å– YAML é…ç½®
  - `get_dataloader()`: åˆ›å»ºæ•°æ®åŠ è½½å™¨
  - `Trainer()`: å®ä¾‹åŒ–è®­ç»ƒå™¨

### 2ï¸âƒ£ **Utils/Data_utils/real_datasets.py** (è“è‰²æ¡†)
- **ä½œç”¨**ï¼šæ•°æ®é¢„å¤„ç†ç®¡é“
- **å…³é”®å‡½æ•°**ï¼š
  - `load_csv()`: è¯»å–åŸå§‹ CSVï¼ˆNè¡ŒÃ—9åˆ—ï¼‰
  - `minmax_scaler()`: Min-Max å½’ä¸€åŒ–ï¼Œè¿”å› `scaled, min_val, max_val`
  - `create_windows()`: æŒ‰ `seq_len=512` åˆ‡åˆ†çª—å£
  - `NILMDataset`: PyTorch Dataset å°è£…
  - `DataLoader`: æ‰¹é‡è¯»å–ï¼Œshape `(B, L, 9)`

### 3ï¸âƒ£ **engine/solver.py** (é»„è‰²æ¡†)
- **ä½œç”¨**ï¼šè®­ç»ƒ/é‡‡æ ·ä¸»å¾ªç¯
- **è®­ç»ƒæ¨¡å¼ (`Trainer.train`)**ï¼š
  - éšæœºé‡‡æ · diffusion step `t`
  - åŠ å™ªï¼š`q_sample(x0, t)` â†’ å¾—åˆ° `x_t` å’ŒçœŸå®å™ªå£° `Îµ`
  - æå–æ¡ä»¶ï¼š`c = batch[..., 1:].mean(dim=1)`
  - è°ƒç”¨æ¨¡å‹ï¼š`eps_pred = DiT.forward(x_t, t, c)`
  - è®¡ç®— Lossï¼š`MSE(eps_pred, Îµ)`
  - åå‘ä¼ æ’­ï¼š`optimizer.step()`
  
- **é‡‡æ ·æ¨¡å¼ (`Trainer.sample`)**ï¼š
  - åˆå§‹åŒ–å™ªå£°ï¼š`x_T ~ N(0, I)`
  - é€†æ‰©æ•£å¾ªç¯ï¼š`t = T-1 â†’ 0`
  - æ„é€ æ¡ä»¶ï¼š`build_condition(target_time)`
  - é€æ­¥å»å™ªï¼šä½¿ç”¨é€†æ‰©æ•£å…¬å¼
  - åå½’ä¸€åŒ–ï¼šæ¢å¤çœŸå®åŠŸç‡
  - ä¿å­˜ï¼š`.npy` æ–‡ä»¶

### 4ï¸âƒ£ **Models/diffusion/agent_transformer.py** (ç»¿è‰²æ¡†)
- **ä½œç”¨**ï¼šDiT æ¨¡å‹æ ¸å¿ƒå®ç°
- **ä¸»è¦ç»„ä»¶**ï¼š

#### A. æ¡ä»¶åµŒå…¥æ¨¡å—
```python
class TimestepEmbedding:
    # å°†ç¦»æ•£ step â†’ è¿ç»­å‘é‡
    # Input: t (B,)
    # Output: t_emb (B, hidden_dim)
```

#### B. è¾“å…¥æŠ•å½±
```python
self.input_proj = nn.Linear(9, hidden_dim)
# Input: (B, L, 9)
# Output: (B, L, hidden_dim)
```

#### C. DiT Blockï¼ˆğŸŸª AdaLN-Zero æ ¸å¿ƒï¼‰
```python
class DiTBlock:
    def __init__(self):
        # Modulation Network (AdaLN-Zero)
        self.modulation = nn.Sequential(
            nn.SiLU(),
            nn.Linear(hidden_dim, 6*hidden_dim, bias=True)
        )
        # âš ï¸ æœ€åä¸€å±‚æƒé‡åˆå§‹åŒ–ä¸º 0
        
        # LayerNorm (æ— å¯å­¦ä¹ å‚æ•°)
        self.norm1 = nn.LayerNorm(hidden_dim, elementwise_affine=False)
        self.norm2 = nn.LayerNorm(hidden_dim, elementwise_affine=False)
        
        # Multi-Head Attention
        self.attn = nn.MultiheadAttention(hidden_dim, num_heads=8)
        
        # Feed-Forward MLP
        self.mlp = ...
    
    def forward(self, x, cond):
        # 1ï¸âƒ£ ç”Ÿæˆ 6 ä¸ªè°ƒåˆ¶å‚æ•°
        mod = self.modulation(cond).chunk(6, dim=-1)
        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = mod
        
        # 2ï¸âƒ£ ç¬¬ä¸€è·¯å¾„ï¼šAdaLN + Attention
        x_norm = self.norm1(x)
        x_mod = x_norm * (1 + scale_msa) + shift_msa  # ğŸŸª AdaLN è°ƒåˆ¶
        attn_out = self.attn(x_mod, x_mod, x_mod)
        x = x + gate_msa * attn_out  # ğŸŸª Gate æ§åˆ¶
        
        # 3ï¸âƒ£ ç¬¬äºŒè·¯å¾„ï¼šAdaLN + MLP
        x_norm = self.norm2(x)
        x_mod = x_norm * (1 + scale_mlp) + shift_mlp  # ğŸŸª AdaLN è°ƒåˆ¶
        mlp_out = self.mlp(x_mod)
        x = x + gate_mlp * mlp_out  # ğŸŸª Gate æ§åˆ¶
        
        return x
```

#### D. æœ€ç»ˆè¾“å‡ºå±‚
```python
self.final_norm = nn.LayerNorm(hidden_dim)
self.out_proj = nn.Linear(hidden_dim, 9)
# Output: ÎµÌ‚ (é¢„æµ‹çš„å™ªå£°) shape (B, L, 9)
```

---

## ğŸ¯ AdaLN-Zero çš„ 4 ä¸ªå…³é”®ä½ç½®

| ä½ç½® | ä½œç”¨ | å…¬å¼ |
|------|------|------|
| **1. Modulation Network** | æ ¹æ®æ¡ä»¶ç”Ÿæˆè°ƒåˆ¶å‚æ•° | `mod = MLP(cond)` â†’ 6 ä¸ªå‚æ•° |
| **2. MSA å‰çš„ Scale & Shift** | è°ƒåˆ¶å½’ä¸€åŒ–åçš„ç‰¹å¾ | `x_mod = x_norm Â· (1 + scale) + shift` |
| **3. MSA åçš„ Gate** | æ§åˆ¶æ³¨æ„åŠ›ä¿¡æ¯æµ | `x = x + gate Â· attn_out` |
| **4. MLP è·¯å¾„ï¼ˆåŒä¸Šï¼‰** | å¯¹ MLP è·¯å¾„åšç›¸åŒå¤„ç† | åŒä¸Š |

---

## ğŸ”‘ ä¸ºä»€ä¹ˆè¦ç”¨ Zero-Initï¼Ÿ

```python
# åœ¨ __init__ ä¸­
nn.init.zeros_(self.modulation[-1].weight)
nn.init.zeros_(self.modulation[-1].bias)
```

**åŸå› **ï¼š
- åˆå§‹æ—¶ï¼š`scale=0`, `shift=0`, `gate=0`
- æ­¤æ—¶ AdaLN é€€åŒ–ä¸ºæ™®é€š LayerNorm + Residual
- æ¨¡å‹è®­ç»ƒç¨³å®šï¼Œä¸å—æœªè®­ç»ƒçš„æ¡ä»¶å¹²æ‰°
- éšç€è®­ç»ƒè¿›è¡Œï¼Œæ¨¡å‹é€æ¸å­¦ä¼šå¦‚ä½•ä½¿ç”¨æ¡ä»¶ä¿¡æ¯

---

## ğŸ“Š æ•°æ®ç»´åº¦å¯¹ç…§è¡¨

| é˜¶æ®µ | å˜é‡å | Shape | æ–‡ä»¶ |
|------|--------|-------|------|
| **åŸå§‹ CSV** | `data` | `(N, 9)` | `real_datasets.py` |
| **å½’ä¸€åŒ–å** | `scaled` | `(N, 9)` | `real_datasets.py` |
| **çª—å£åŒ–** | `windows` | `(W, 512, 9)` | `real_datasets.py` |
| **Batch** | `batch` | `(64, 512, 9)` | `solver.py` |
| **åŠ å™ªå** | `x_t` | `(64, 512, 9)` | `solver.py` |
| **æ¡ä»¶å‘é‡** | `c` | `(64, 8)` | `solver.py` |
| **æ—¶é—´æ­¥åµŒå…¥** | `t_emb` | `(64, 256)` | `agent_transformer.py` |
| **èåˆæ¡ä»¶** | `cond` | `(64, 256)` | `agent_transformer.py` |
| **æŠ•å½±å** | `x` | `(64, 512, 256)` | `agent_transformer.py` |
| **è°ƒåˆ¶å‚æ•°** | `scale_msa` | `(64, 256)` | `DiTBlock` |
| **é¢„æµ‹å™ªå£°** | `ÎµÌ‚` | `(64, 512, 9)` | `agent_transformer.py` |

---

## ä½¿ç”¨è¯´æ˜

åœ¨ VSCode ä¸­æ‰“å¼€æ­¤æ–‡ä»¶ï¼Œä½¿ç”¨ Mermaid Viewer æ‰©å±•å³å¯æŸ¥çœ‹å®Œæ•´æµç¨‹å›¾ã€‚
