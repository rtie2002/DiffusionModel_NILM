{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Train CNN CGAN Baseline\n",
    "\n",
    "This notebook is the local version of the training script, optimized for running on your RTX 4090 with live plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils import spectral_norm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# ==========================================\n",
    "# CONFIGURATION\n",
    "# ==========================================\n",
    "APPLIANCES = [\"dishwasher\", \"fridge\", \"kettle\", \"microwave\", \"washingmachine\"]\n",
    "WINDOW_SIZE = 512\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS_PER_APP = 10000\n",
    "BASE_DIR = r\"C:\\Users\\Raymond Tie\\Desktop\\DiffusionModel_NILM\" # You can change this if needed\n",
    "\n",
    "# Check Device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"‚úÖ Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"‚ö†Ô∏è GPU not found. Using CPU!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# MODEL DEFINITIONS\n",
    "# ==========================================\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(100, 128 * 16)\n",
    "        def up(ic, oc): return nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='linear', align_corners=False),\n",
    "            nn.Conv1d(ic, oc, 3, 1, 1),\n",
    "            nn.BatchNorm1d(oc),\n",
    "            nn.ReLU(True))\n",
    "        self.model = nn.Sequential(up(128,64), up(64,32), up(32,16), up(16,8),\n",
    "                                nn.Upsample(scale_factor=2, mode='linear', align_corners=False),\n",
    "                                nn.Conv1d(8, 1, 3, 1, 1), nn.Tanh())\n",
    "    def forward(self, z): return self.model(self.fc(z).view(-1, 128, 16))\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        def cb(ic, oc, s=2): return nn.Sequential(spectral_norm(nn.Conv1d(ic, oc, 4, s, 1)), nn.LeakyReLU(0.2))\n",
    "        self.conv = nn.Sequential(cb(1, 16), cb(16, 32), cb(32, 64), cb(64, 128),\n",
    "                                  nn.AdaptiveAvgPool1d(1), nn.Flatten(), nn.Linear(128,1), nn.Sigmoid())\n",
    "    def forward(self, x): return self.conv(x)\n",
    "\n",
    "class NILM_Dataset(Dataset):\n",
    "    def __init__(self, p, t):\n",
    "        self.data = []\n",
    "        stride = 64\n",
    "        for i in range(0, len(p) - WINDOW_SIZE, stride):\n",
    "            if np.max(p[i:i+WINDOW_SIZE]) > -0.9: self.data.append((p[i:i+WINDOW_SIZE], t[i:i+WINDOW_SIZE]))\n",
    "    def __len__(self): return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        p, t = self.data[idx]\n",
    "        return torch.from_numpy(p).float().unsqueeze(0), torch.from_numpy(t).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# TRAINING LOOP\n",
    "# ==========================================\n",
    "print(f\"Starting Training...\")\n",
    "\n",
    "for appliance in APPLIANCES:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PROCESSING: {appliance.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Path Setup\n",
    "    CSV_PATH = os.path.join(BASE_DIR, \"Data\", \"datasets\", f\"{appliance}_multivariate.csv\")\n",
    "    OUT_DIR = os.path.join(BASE_DIR, \"Synthetic_Data\", appliance)\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "    if not os.path.exists(CSV_PATH):\n",
    "        print(f\"‚ö†Ô∏è Skipping {appliance}: CSV not found at {CSV_PATH}\")\n",
    "        continue\n",
    "\n",
    "    # Data Prep\n",
    "    print(f\"Loading data...\")\n",
    "    try:\n",
    "        df = pd.read_csv(CSV_PATH)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV: {e}\")\n",
    "        continue\n",
    "        \n",
    "    power_col = next((c for c in df.columns if 'power' in c.lower() or appliance in c.lower()), df.columns[-1])\n",
    "    p_min, p_max = df[power_col].min(), df[power_col].max()\n",
    "    raw_p_norm = (df[power_col].values - p_min) / (p_max - p_min) * 2 - 1\n",
    "\n",
    "    # Convert time features\n",
    "    time_features_df = df.drop(columns=[power_col]).apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "    dataset = NILM_Dataset(raw_p_norm, time_features_df.values)\n",
    "    \n",
    "    if len(dataset) == 0:\n",
    "        print(f\"Warning: No valid windows found for {appliance}\")\n",
    "        continue\n",
    "        \n",
    "    train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "    # Model Init\n",
    "    G, D = Generator().to(device), Discriminator().to(device)\n",
    "    opt_G = torch.optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    opt_D = torch.optim.Adam(D.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
    "    sched_G = torch.optim.lr_scheduler.StepLR(opt_G, step_size=300, gamma=0.5)\n",
    "    sched_D = torch.optim.lr_scheduler.StepLR(opt_D, step_size=300, gamma=0.5)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # Training Loop\n",
    "    print(f\"üî• Training for {EPOCHS_PER_APP} epochs...\")\n",
    "    for epoch in range(1, EPOCHS_PER_APP + 1):\n",
    "        last_real_p = None\n",
    "        last_fake_p = None\n",
    "\n",
    "        for i, (real_p, _) in enumerate(train_loader):\n",
    "            real_p = real_p.to(device); bs = real_p.size(0)\n",
    "\n",
    "            # Train Discriminator\n",
    "            opt_D.zero_grad()\n",
    "            z = torch.randn(bs, 100).to(device); fake_p = G(z)\n",
    "            loss_d = (criterion(D(real_p), torch.full((bs,1), 0.9).to(device)) + criterion(D(fake_p.detach()), torch.zeros(bs,1).to(device))) / 2\n",
    "            loss_d.backward(); opt_D.step()\n",
    "\n",
    "            # Train Generator\n",
    "            for _ in range(2):\n",
    "                opt_G.zero_grad()\n",
    "                fake_p = G(torch.randn(bs, 100).to(device))\n",
    "                loss_g = criterion(D(fake_p), torch.ones(bs,1).to(device)) + 0.2 * torch.mean(torch.abs(fake_p[:, :, 1:] - fake_p[:, :, :-1]))\n",
    "                loss_g.backward(); opt_G.step()\n",
    "\n",
    "            last_real_p = real_p.detach().cpu()\n",
    "            last_fake_p = fake_p.detach().cpu()\n",
    "\n",
    "        sched_G.step(); sched_D.step()\n",
    "\n",
    "        # Live Plotting in Notebook\n",
    "        if epoch % 10 == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"Current Appliance: {appliance.upper()} | Epoch {epoch}/{EPOCHS_PER_APP}\")\n",
    "            plt.figure(figsize=(12, 4))\n",
    "            if last_real_p is not None and last_fake_p is not None:\n",
    "                plt.plot((last_real_p[0,0]+1)/2, label='Real (Reference)', alpha=0.5)\n",
    "                plt.plot((last_fake_p[0,0]+1)/2, label='Generated', color='orange', alpha=0.8)\n",
    "            plt.title(f\"{appliance.upper()} Epoch {epoch}\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "    # Sampling 200%\n",
    "    NUM_VALS = len(dataset)\n",
    "    NUM_GEN = NUM_VALS * 2\n",
    "    print(f\"Generating synthetic data ({NUM_GEN} samples)...\")\n",
    "\n",
    "    G.eval()\n",
    "    all_f = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(NUM_GEN // BATCH_SIZE + 1):\n",
    "            all_f.append((G(torch.randn(BATCH_SIZE, 100).to(device)).cpu().numpy() + 1) / 2)\n",
    "\n",
    "    final_p = np.concatenate(all_f, axis=0)[:NUM_GEN]\n",
    "    indices = np.random.choice(len(dataset), NUM_GEN, replace=True)\n",
    "    final_t = np.array([dataset[idx][1].numpy() for idx in indices])\n",
    "\n",
    "    final_merged = np.concatenate([np.expand_dims(final_p.squeeze(1), axis=2), final_t], axis=2)\n",
    "\n",
    "    np_path = os.path.join(OUT_DIR, f\"synthetic_{appliance}.npy\")\n",
    "    np.save(np_path, final_merged)\n",
    "    torch.save(G.state_dict(), os.path.join(OUT_DIR, f\"{appliance}_generator.pth\"))\n",
    "    print(f\"‚úÖ Saved synthetic data to: {np_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
