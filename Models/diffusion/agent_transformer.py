import math
import torch
import numpy as np
import torch.nn.functional as F

from torch import nn
from einops import rearrange, reduce, repeat
from Models.diffusion.model_utils import LearnablePositionalEncoding, Conv_MLP, \
    AdaLayerNorm, Transpose, GELU2, series_decomp


class AgentAttention(nn.Module):
    def __init__(self, d_model, n_heads, n_agents, dropout=0.1):
        super(AgentAttention, self).__init__()
        self.n_agents = n_agents
        self.d_model = d_model
        self.n_heads = n_heads
        self.head_dim = d_model // n_heads

        self.q_linear = nn.Linear(self.head_dim, self.head_dim)
        self.k_linear = nn.Linear(self.head_dim, self.head_dim)
        self.v_linear = nn.Linear(self.head_dim, self.head_dim)
        self.out = nn.Linear(d_model, d_model)
        self.dropout = nn.Dropout(dropout)

        self.agent_tokens = nn.Parameter(torch.randn(1, n_heads, n_agents, self.head_dim))

    def forward(self, q, k, v):
        B, nh, N, hs = q.size()

        agents = self.agent_tokens.expand(B, nh, self.n_agents, hs)

        q_agents = self.q_linear(agents)  #A
        k_agents = self.k_linear(k)      #Q
        v_agents = self.v_linear(v)

        # Agent Aggregation #Rightside
        scores1 = torch.matmul(q_agents, k_agents.transpose(-2, -1)) / math.sqrt(hs)
        scores1 = F.softmax(scores1, dim=-1)
        context1 = torch.matmul(scores1, v_agents)

        # Agent Broadcast #leftside
        scores2 = torch.matmul(q, context1.transpose(-2, -1)) / math.sqrt(hs)
        scores2 = F.softmax(scores2, dim=-1)
        context2 = torch.matmul(scores2, context1)

        context2 = context2.transpose(1, 2).contiguous().view(B, N, nh * hs)
        output = self.dropout(self.out(context2))
        return output, scores2


class TrendBlock(nn.Module):
    """
    Model trend of time series using the polynomial regressor.
    """

    def __init__(self, in_dim, out_dim, in_feat, out_feat, act):
        super(TrendBlock, self).__init__()
        trend_poly = 3
        self.trend = nn.Sequential(
            nn.Conv1d(in_channels=in_dim, out_channels=trend_poly, kernel_size=3, padding=1),
            act,
            Transpose(shape=(1, 2)),
            nn.Conv1d(in_feat, out_feat, 3, stride=1, padding=1)
        )

        lin_space = torch.arange(1, out_dim + 1, 1) / (out_dim + 1)
        self.poly_space = torch.stack([lin_space ** float(p + 1) for p in range(trend_poly)], dim=0)

    def forward(self, input):
        b, c, h = input.shape
        x = self.trend(input).transpose(1, 2)
        trend_vals = torch.matmul(x.transpose(1, 2), self.poly_space.to(x.device))
        trend_vals = trend_vals.transpose(1, 2)
        return trend_vals


class MovingBlock(nn.Module):
    """
    Model trend of time series using the moving average.
    """

    def __init__(self, out_dim):
        super(MovingBlock, self).__init__()
        size = max(min(int(out_dim / 4), 24), 4)
        self.decomp = series_decomp(size)

    def forward(self, input):
        b, c, h = input.shape
        x, trend_vals = self.decomp(input)
        return x, trend_vals


class FourierLayer(nn.Module):
    """
    Model seasonality of time series using the inverse DFT.
    """

    def __init__(self, d_model, low_freq=1, factor=1):
        super().__init__()
        self.d_model = d_model
        self.factor = factor
        self.low_freq = low_freq

    def forward(self, x):
        """x: (b, t, d)"""
        b, t, d = x.shape
        x_freq = torch.fft.rfft(x, dim=1)

        if t % 2 == 0:
            x_freq = x_freq[:, self.low_freq:-1]
            f = torch.fft.rfftfreq(t)[self.low_freq:-1]
        else:
            x_freq = x_freq[:, self.low_freq:]
            f = torch.fft.rfftfreq(t)[self.low_freq:]

        x_freq, index_tuple = self.topk_freq(x_freq)
        f = repeat(f, 'f -> b f d', b=x_freq.size(0), d=x_freq.size(2)).to(x_freq.device)
        f = rearrange(f[index_tuple], 'b f d -> b f () d').to(x_freq.device)
        return self.extrapolate(x_freq, f, t)

    def extrapolate(self, x_freq, f, t):
        x_freq = torch.cat([x_freq, x_freq.conj()], dim=1)
        f = torch.cat([f, -f], dim=1)
        t = rearrange(torch.arange(t, dtype=torch.float),
                      't -> () () t ()').to(x_freq.device)

        amp = rearrange(x_freq.abs(), 'b f d -> b f () d')
        phase = rearrange(x_freq.angle(), 'b f d -> b f () d')
        x_time = amp * torch.cos(2 * math.pi * f * t + phase)
        return reduce(x_time, 'b f t d -> b t d', 'sum')

    def topk_freq(self, x_freq):
        length = x_freq.shape[1]
        top_k = int(self.factor * math.log(length))
        values, indices = torch.topk(x_freq.abs(), top_k, dim=1, largest=True, sorted=True)
        mesh_a, mesh_b = torch.meshgrid(torch.arange(x_freq.size(0)), torch.arange(x_freq.size(2)), indexing='ij')
        index_tuple = (mesh_a.unsqueeze(1), indices, mesh_b.unsqueeze(1))
        x_freq = x_freq[index_tuple]
        return x_freq, index_tuple


class SeasonBlock(nn.Module):
    """
    Model seasonality of time series using the Fourier series.
    """

    def __init__(self, in_dim, out_dim, factor=1):
        super(SeasonBlock, self).__init__()
        season_poly = factor * min(32, int(out_dim // 2))
        self.season = nn.Conv1d(in_channels=in_dim, out_channels=season_poly, kernel_size=1, padding=0)
        fourier_space = torch.arange(0, out_dim, 1) / out_dim
        p1, p2 = (season_poly // 2, season_poly // 2) if season_poly % 2 == 0 \
            else (season_poly // 2, season_poly // 2 + 1)
        s1 = torch.stack([torch.cos(2 * np.pi * p * fourier_space) for p in range(1, p1 + 1)], dim=0)
        s2 = torch.stack([torch.sin(2 * np.pi * p * fourier_space) for p in range(1, p2 + 1)], dim=0)
        self.poly_space = torch.cat([s1, s2])

    def forward(self, input):
        b, c, h = input.shape
        x = self.season(input)
        season_vals = torch.matmul(x.transpose(1, 2), self.poly_space.to(x.device))
        season_vals = season_vals.transpose(1, 2)
        return season_vals


class FullAttention(nn.Module):
    def __init__(self,
                 n_embd,  # the embed dim
                 n_head,  # the number of heads
                 n_agents=64,  # number of agent tokens
                 attn_pdrop=0.1,  # attention dropout prob
                 resid_pdrop=0.1,  # residual attention dropout prob
                 ):
        super().__init__()
        assert n_embd % n_head == 0
        self.n_head = n_head
        self.d_model = n_embd
        self.head_dim = n_embd // n_head

        self.qkv = nn.Linear(n_embd, 3 * n_embd)
        self.agent_attention = AgentAttention(n_embd, n_head, n_agents, attn_pdrop)

        # regularization
        self.resid_drop = nn.Dropout(resid_pdrop)
        # output projection
        self.proj = nn.Linear(n_embd, n_embd)

    def forward(self, x, mask=None):
        B, T, C = x.size()
        qkv = self.qkv(x)
        q, k, v = qkv.chunk(3, dim=-1)

        q = q.view(B, T, self.n_head, self.head_dim).transpose(1, 2)  # (B, nh, T, hs)
        k = k.view(B, T, self.n_head, self.head_dim).transpose(1, 2)  # (B, nh, T, hs)
        v = v.view(B, T, self.n_head, self.head_dim).transpose(1, 2)  # (B, nh, T, hs)

        y, att = self.agent_attention(q, k, v)  # agent attention

        # output projection
        y = self.resid_drop(self.proj(y))
        return y, att


class CrossAttention(nn.Module):
    def __init__(self,
                 n_embd,  # the embed dim
                 condition_embd,  # condition dim
                 n_head,  # the number of heads
                 n_agents=64,  # number of agent tokens
                 attn_pdrop=0.1,  # attention dropout prob
                 resid_pdrop=0.1,  # residual attention dropout prob
                 ):
        super().__init__()
        assert n_embd % n_head == 0
        self.n_head = n_head
        self.d_model = n_embd
        self.head_dim = n_embd // n_head

        self.q = nn.Linear(n_embd, n_embd)
        self.kv = nn.Linear(condition_embd, 2 * n_embd)
        self.agent_attention = AgentAttention(n_embd, n_head, n_agents, attn_pdrop)

        # regularization
        self.resid_drop = nn.Dropout(resid_pdrop)
        # output projection
        self.proj = nn.Linear(n_embd, n_embd)

    def forward(self, x, encoder_output, mask=None):
        B, T, C = x.size()
        q = self.q(x)
        kv = self.kv(encoder_output)
        k, v = kv.chunk(2, dim=-1)

        q = q.view(B, T, self.n_head, self.head_dim).transpose(1, 2)  # (B, nh, T, hs)
        k = k.view(B, encoder_output.size(1), self.n_head, self.head_dim).transpose(1, 2)  # (B, nh, T, hs)
        v = v.view(B, encoder_output.size(1), self.n_head, self.head_dim).transpose(1, 2)  # (B, nh, T, hs)

        y, att = self.agent_attention(q, k, v)

        # output projection
        y = self.resid_drop(self.proj(y))
        return y, att


class EncoderBlock(nn.Module):
    def __init__(self,
                 n_embd=1024,
                 n_head=16,
                 attn_pdrop=0.1,
                 resid_pdrop=0.1,
                 mlp_hidden_times=4,
                 activate='GELU'
                 ):
        super().__init__()

        # DiT uses separate AdaLN for attention and MLP branches
        self.ln1 = AdaLayerNorm(n_embd)
        self.ln2 = AdaLayerNorm(n_embd) # NEW: Using AdaLN instead of LN
        
        self.attn = FullAttention(
            n_embd=n_embd,
            n_head=n_head,
            attn_pdrop=attn_pdrop,
            resid_pdrop=resid_pdrop,
        )

        assert activate in ['GELU', 'GELU2']
        act = nn.GELU() if activate == 'GELU' else GELU2()

        self.mlp = nn.Sequential(
            nn.Linear(n_embd, mlp_hidden_times * n_embd),
            act,
            nn.Linear(mlp_hidden_times * n_embd, n_embd),
            nn.Dropout(resid_pdrop),
        )

    def forward(self, x, timestep, mask=None, label_emb=None):
        # 1. Attention Branch with alpha_1 (gate)
        # norm1_x is modulated by gamma and beta inside AdaLayerNorm
        norm1_x, alpha1 = self.ln1(x, timestep, label_emb)
        attn_out, att = self.attn(norm1_x, mask=mask)
        x = x + alpha1 * attn_out # alpha = gate
        
        # 2. MLP Branch with alpha_2 (gate)
        norm2_x, alpha2 = self.ln2(x, timestep, label_emb)
        mlp_out = self.mlp(norm2_x)
        x = x + alpha2 * mlp_out
        
        return x, att


class Encoder(nn.Module):
    def __init__(
            self,
            n_layer=14,
            n_embd=1024,
            n_head=16,
            attn_pdrop=0.,
            resid_pdrop=0.,
            mlp_hidden_times=4,
            block_activate='GELU',
    ):
        super().__init__()

        self.blocks = nn.Sequential(*[EncoderBlock(
            n_embd=n_embd,
            n_head=n_head,
            attn_pdrop=attn_pdrop,
            resid_pdrop=resid_pdrop,
            mlp_hidden_times=mlp_hidden_times,
            activate=block_activate,
        ) for _ in range(n_layer)])

    def forward(self, input, t, padding_masks=None, label_emb=None):
        x = input
        for block_idx in range(len(self.blocks)):
            x, _ = self.blocks[block_idx](x, t, mask=padding_masks, label_emb=label_emb)
        return x


class DecoderBlock(nn.Module):
    def __init__(self,
                 n_channel,
                 n_feat,
                 n_embd=1024,
                 n_head=16,
                 attn_pdrop=0.1,
                 resid_pdrop=0.1,
                 mlp_hidden_times=4,
                 activate='GELU',
                 condition_dim=1024,
                 ):
        super().__init__()

        # Full DiT-Style: All normalizations are Adaptive and gated
        self.ln1 = AdaLayerNorm(n_embd) # Self-Attn
        self.ln1_1 = AdaLayerNorm(n_embd) # Cross-Attn
        self.ln2 = AdaLayerNorm(n_embd) # MLP

        self.attn1 = FullAttention(
            n_embd=n_embd,
            n_head=n_head,
            n_agents=64,
            attn_pdrop=attn_pdrop,
            resid_pdrop=resid_pdrop,
        )
        self.attn2 = CrossAttention(
            n_embd=n_embd,
            condition_embd=condition_dim,
            n_head=n_head,
            n_agents=64,
            attn_pdrop=attn_pdrop,
            resid_pdrop=resid_pdrop,
        )

        assert activate in ['GELU', 'GELU2']
        act = nn.GELU() if activate == 'GELU' else GELU2()

        self.trend = TrendBlock(n_channel, n_channel, n_embd, n_feat, act=act)
        self.seasonal = FourierLayer(d_model=n_embd)

        self.mlp = nn.Sequential(
            nn.Linear(n_embd, mlp_hidden_times * n_embd),
            act,
            nn.Linear(mlp_hidden_times * n_embd, n_embd),
            nn.Dropout(resid_pdrop),
        )

        self.proj = nn.Conv1d(n_channel, n_channel * 2, 1)
        self.linear = nn.Linear(n_embd, n_feat)

    def forward(self, x, encoder_output, timestep, mask=None, label_emb=None):
        # 1. Self-Attention with Gating
        norm1_x, alpha1 = self.ln1(x, timestep, label_emb)
        a1, att1 = self.attn1(norm1_x, mask=mask)
        x = x + alpha1 * a1
        
        # 2. Cross-Attention with Gating
        norm1_1_x, alpha1_1 = self.ln1_1(x, timestep, label_emb)
        a2, att2 = self.attn2(norm1_1_x, encoder_output, mask=mask)
        x = x + alpha1_1 * a2
        
        # Trend/Seasonality Decomposition Branch
        # These are usually decoupled, but we apply MLP modulation for consistency
        x1, x2 = self.proj(x).chunk(2, dim=1)
        trend, season = self.trend(x1), self.seasonal(x2)
        
        # 3. MLP with Gating
        norm2_x, alpha2 = self.ln2(x, timestep, label_emb)
        mlp_out = self.mlp(norm2_x)
        x = x + alpha2 * mlp_out
        
        m = torch.mean(x, dim=1, keepdim=True)
        return x - m, self.linear(m), trend, season


class Decoder(nn.Module):
    def __init__(
            self,
            n_channel,
            n_feat,
            n_embd=1024,
            n_head=16,
            n_layer=10,
            attn_pdrop=0.1,
            resid_pdrop=0.1,
            mlp_hidden_times=4,
            block_activate='GELU',
            condition_dim=512
    ):
        super().__init__()
        self.d_model = n_embd
        self.n_feat = n_feat
        self.blocks = nn.Sequential(*[DecoderBlock(
            n_feat=n_feat,
            n_channel=n_channel,
            n_embd=n_embd,
            n_head=n_head,
            attn_pdrop=attn_pdrop,
            resid_pdrop=resid_pdrop,
            mlp_hidden_times=mlp_hidden_times,
            activate=block_activate,
            condition_dim=condition_dim,
        ) for _ in range(n_layer)])

    def forward(self, x, t, enc, padding_masks=None, label_emb=None):
        b, c, _ = x.shape
        # att_weights = []
        mean = []
        season = torch.zeros((b, c, self.d_model), device=x.device)
        trend = torch.zeros((b, c, self.n_feat), device=x.device)
        for block_idx in range(len(self.blocks)):
            x, residual_mean, residual_trend, residual_season = \
                self.blocks[block_idx](x, enc, t, mask=padding_masks, label_emb=label_emb)
            season += residual_season
            trend += residual_trend
            mean.append(residual_mean)

        mean = torch.cat(mean, dim=1)
        return x, mean, trend, season


class Transformer(nn.Module):
    def __init__(
            self,
            n_feat, # Usually 1 for power
            n_channel,
            condition_dim=8, # NEW: Number of time features (8)
            n_layer_enc=5,
            n_layer_dec=14,
            n_embd=1024,
            n_heads=16,
            attn_pdrop=0.1,
            resid_pdrop=0.1,
            mlp_hidden_times=4,
            block_activate='GELU',
            max_len=2048,
            conv_params=None,
            **kwargs
    ):
        super().__init__()
        # POWER EMBEDDING (1D -> n_embd)
        self.power_emb = Conv_MLP(n_feat, n_embd, resid_pdrop=resid_pdrop)
        self.inverse = Conv_MLP(n_embd, n_feat, resid_pdrop=resid_pdrop)
        
        # CONDITION ENCODER (8D -> n_embd)
        # This takes the 8 time features and creates an embedding to guide AdaLN
        self.cond_emb_mlp = nn.Sequential(
            nn.Linear(condition_dim, n_embd),
            nn.SiLU(),
            nn.Linear(n_embd, n_embd)
        )

        if conv_params is None or conv_params[0] is None:
            if n_feat < 32 and n_channel < 64:
                kernel_size, padding = 1, 0
            else:
                kernel_size, padding = 5, 2
        else:
            kernel_size, padding = conv_params

        self.combine_s = nn.Conv1d(n_embd, n_feat, kernel_size=kernel_size, stride=1, padding=padding,
                                   padding_mode='circular', bias=False)
        self.combine_m = nn.Conv1d(n_layer_dec, 1, kernel_size=1, stride=1, padding=0,
                                   padding_mode='circular', bias=False)

        self.encoder = Encoder(n_layer_enc, n_embd, n_heads, attn_pdrop, resid_pdrop, mlp_hidden_times, block_activate)
        self.pos_enc = LearnablePositionalEncoding(n_embd, dropout=resid_pdrop, max_len=max_len)

        self.decoder = Decoder(n_channel, n_feat, n_embd, n_heads, n_layer_dec, attn_pdrop, resid_pdrop,
                               mlp_hidden_times,
                               block_activate, condition_dim=n_embd)
        self.pos_dec = LearnablePositionalEncoding(n_embd, dropout=resid_pdrop, max_len=max_len)
        self.n_feat = n_feat

    def forward(self, input, t, padding_masks=None):
        # DECOUPLE INPUT: Separate 1D power from 8D conditions
        # input shape expected: (B, L, 1+8) = (B, L, 9)
        x_power = input[:, :, :self.n_feat]
        x_cond = input[:, :, self.n_feat:]
        
        # 1. Encode Condition into label_emb
        # We take the mean or specific pooling to get a global condition vector
        label_emb = self.cond_emb_mlp(x_cond) # (B, L, n_embd)
        
        # 2. Process Power Signal
        emb = self.power_emb(x_power)
        inp_enc = self.pos_enc(emb)
        
        # 3. Encoder Stage with AdaLN Guidance
        enc_cond = self.encoder(inp_enc, t, padding_masks=padding_masks, label_emb=label_emb)

        # 4. Decoder Stage with AdaLN Guidance
        inp_dec = self.pos_dec(emb)
        output, mean, trend, season = self.decoder(inp_dec, t, enc_cond, padding_masks=padding_masks, label_emb=label_emb)

        res = self.inverse(output)
        res_m = torch.mean(res, dim=1, keepdim=True)
        season_error = self.combine_s(season.transpose(1, 2)).transpose(1, 2) + res - res_m
        trend = self.combine_m(mean) + res_m + trend

        return trend, season_error


if __name__ == '__main__':
    pass