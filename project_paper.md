A diffusion model-based framework to enhance the robustness of  non-intrusive load disaggregation  Zeyi Geng   a , Linfeng Yang   a , * , Wuqing Yu b  a   School of Computer Electronics and Information, Guangxi Key Laboratory of Multimedia Communication and Network Technology, Guangxi University, 530004,  Nanning, China b   School of Artificial Intelligence, Beijing Normal University, 102206, Beijing, China  A R T I C L E   I N F O   Keywords:  Non-intrusive load monitoring Diffusion model Robust optimization Data augmentation  A B S T R A C T   Non-intrusive load monitoring (NILM) can extract energy consumption information for each appliance in an  economical and efficient way by analyzing the total power signal, which in turn promotes the construction of  smart grids. However, existing NILM models require substantial labeled energy consumption data for training.  Collecting long-term load data is time-consuming and challenging, and existing datasets often contain significant  noise, hindering the models ’    ability to accurately extract load features. To address these issues, this study pro - poses a framework to improve model robustness and reduce data requirements. Specifically, we introduce a  NILM data augmentation architecture based on a diffusion model. We optimize the diffusion model to generate  multi-state, low-noise load data. Using these synthetic data, the decomposition ability of the NILM model can be  improved and the need for large amounts of training data can be reduced. Furthermore, this study also designs a  loss function and post-processing algorithm that is more suitable for the NILM task to enhance the model ’ s noise  resistance and decomposition stability. Experimental results demonstrate that our approach significantly im - proves  the  model ’ s  MAE,  SAE,  and  F1  score  performance  in  both  origin-household  and  cross-household  scenarios.  1. Introduction  1 As energy demand continues to grow, the construction of smart  grids has become essential for enhancing energy efficiency and power  system reliability [ 1 ]. In the context of energy conservation, emission  reduction, and environmental protection, the importance of smart grids  is steadily increasing. By collecting and analyzing users ’    energy usage  data, smart grids can identify energy consumption behavior patterns,  thereby optimizing usage and achieving the goals of reducing energy  costs and protecting the environment [ 2 ]. However, a major challenge  facing smart grids is obtaining detailed and accurate energy consump - tion feedback for specific appliances based on users ’    total electricity  meter data [ 3 ]. Non-intrusive load monitoring (NILM) is an effective solution to this  problem [ 4 ]. NILM utilizes only the aggregated power data from the  total electricity meter to infer the power consumption of each appliance  through algorithms such as machine learning and mathematical opti - mization. Since Kelly et al. [ 5 ] proposed using deep learning methods for  NILM,  neural  networks  have  addressed  many  key  issues  with  their  automatic feature extraction and powerful modeling capabilities. Sub - sequent researchers have extensively studied network models [ 6 – 8 ] and  decomposition strategies [ 9 – 11 ], continuously improving NILM model  performance. However, despite significant progress in extracting load  features, the vast parameter space of neural networks makes them sus - ceptible to noise interference [ 12 ] and overfitting. These issues have  prevented deep learning methods from fully resolving the problems of  poor performance and limited generalization ability in NILM models for  certain appliances [ 13 ]. The quality of the dataset is a major factor affecting these issues [ 14 ].  NILM  is  most  commonly  implemented  using  supervised  learning.  Building a posterior probability model of appliance power consumption  distribution using accurate total power data and appliance power data is  the main challenge in applying supervised learning algorithms to NILM  [ 15 ].  The performance of supervised  learning algorithms largely de - pends on whether the training data can represent the true distribution of  household electricity consumption characteristics. As the demand for   *   Corresponding author.  E-mail addresses:  miles_gzy@163.com   (Z. Geng),  ylf@gxu.edu.cn   (L. Yang),  2113301058@st.gxu.edu.cn   (W. Yu).  1   Code available at  https://github.com/linfengYang/DiffusionModel_NILM .  Contents lists available at  ScienceDirect  Energy  journal homepage:  www.elsevier.com/locate/energy  https://doi.org/10.1016/j.energy.2025.135423 Received 12 August 2024; Received in revised form 23 October 2024; Accepted 2 March 2025    Energy   320   (2025)   135423    Available   online   3   March   2025    0360-5442/©   2025   Elsevier   Ltd.   All   rights   are   reserved,   including   those   for   text   and   data   mining,   AI   training,   and   similar   technologies.  

NILM model accuracy grows, a large number of high-quality labeled  power datasets are necessary. However, NILM dataset usually needs to  be  collected  through  invasive  equipment,  involving  high  costs  and  personal privacy issues [ 16 ]. Collecting high-quality load data has al - ways been a key focus of NILM research. Although many public datasets  are created for this purpose, these datasets are difficult to share due to  reasons such as inconsistent duration and sampling frequency. Addi - tionally, existing datasets generally contain a lot of noise, which seri - ously affects model performance [ 17 ]. In  this  context,  using  generative  models  to  synthesize  data  can  effectively solve these problems [ 18 ]. Generative models can capture the  core features of the workload and generate rich and diverse synthetic  data. This can effectively compensate for the scarcity of the dataset and  improve the decomposition model ’ s generalization performance outside  the distribution, which is crucial for training a robust NILM model [ 19 ].  Unfortunately, the operating modes of electrical loads are very complex,  and synthesizing electrical load data is challenging. With the introduc - tion of the diffusion model, which has made great achievements in the  field of generation, it has become possible to synthesize high-quality  load  data  using  a  generative  model.  This  paper  proposes  using  the  diffusion model to generate real electrical load data to solve the problem  of the NILM dataset. Compared with previous work using generative  adversarial networks (GANs) to synthesize data, the diffusion model is  more interpretable and stable [ 20 ]. We optimize the diffusion model  based on the task characteristics of NILM and realize the synthesis of  multi-state and low-noise load data. We integrate synthetic data with  real datasets for NILM model training, effectively addressing the limi - tations posed by insufficient data [ 21 ] and reducing the risk of model  overfitting [ 22 ], thereby significantly improving the model ’ s decom - position performance. In terms of load decomposition, this paper also  thoroughly analyzes the reasons why mainstream NILM loss functions  are  susceptible  to  noise  interference,  optimizes  the  loss  function  for  noise resistance, designs a loss function more suitable for NILM tasks,  and proposes a post-processing algorithm to mitigate the issue of large  fluctuations in the decomposed signal. This paper conducts an in-depth investigation through experiments  to study the effects of the proposed framework on data augmentation in  different environments. Multiple experiments with various augmenta - tion  ratios  were  conducted  in  both  origin-household  and  cross-  household  scenarios.  The  results  demonstrate  that  our  framework  significantly enhances the performance of load decomposition models in  terms of MAE, SAE, and F1 metrics. In summary, the contributions of our  study are threefold.   •   This paper is the first to propose NILM data augmentation based on  the  diffusion  model.  We  addressed  the  large  computational  complexity and noise reduction challenges specific to NILM datasets,  enabling the diffusion model to generate realistic, low-noise elec - trical load data.  •   We analyzed the reasons for the lack of robustness in current main - stream NILM decomposition models and optimized their loss func - tions by combining the advantages of L1 and L2 norm loss functions.  Additionally, we designed a post-processing algorithm to enhance  the stability of the decomposed signals.  •   Our general framework enhances the robustness of NILM models,  effectively addressing the issues of data scarcity and model over - fitting in the NILM domain. This framework can support the devel - opment  of  robust  NILM  models  in  the  future,  improving  their  performance across different scenarios.  2. Related work  2.1. Diffusion model  The diffusion model was originally inspired by the diffusion process  in physics and chemistry, and it was subsequently introduced into the  generative model field of machine learning and deep learning. The core  idea of the Diffusion model is to simulate the physical diffusion process  by incrementally adding random noise to the data through a series of  diffusion  steps,  eventually  making  the  data  approximate  a  standard  Gaussian distribution. In the reverse process, the model leverages the  principles of Bayes ’  theorem to gradually remove the noise and restore  the  original  data  distribution.  This  process  can  be  parameterized  as   p ( x τ     1 | x τ   ) =   p ( x τ   | x τ     1 ) p ( x τ     1 ) / p ( x τ   ) ,  where    p ( x τ   | x τ     1 )   represents  the  probability of generating the current noisy data  x τ    conditioned on the  previous  step    x τ     1 .  The  model  learns  this  conditional  probability  to  progressively denoise, iteratively tracing back from the current diffusion  step  to  the  original  data  distribution,  ultimately  generating  samples  consistent with the original data distribution. In  2020,  Ho  et  al.  proposed  the  denoising  diffusion  probabilistic  model  (DDPM)  [ 23 ],  which  forms  the  basis  of  current  mainstream  diffusion models. The diffusion model has since been widely adopted in  the field of generation, producing data of the highest quality in image,  speech, and text generation tasks [ 24 – 27 ]. The application of diffusion  models in the energy sector holds great promise. Previous studies [ 28 ]  have used diffusion models to generate high-quality samples for heating  systems, effectively addressing the issue of sample scarcity. In recent  years,  diffusion  models  have  also  been  extended  to  the  time  series  domain  [ 29 – 31 ].  Yuan  et  al.  [ 31 ]  modeled  the  trend  and  seasonal  components  of  data  by  applying  polynomial  regression  and  Fourier  transform  to  the  decoder  block ’ s  output.  This  approach  enabled  the  diffusion model to effectively capture time series features, resulting in  outstanding performance in downstream tasks such as forecasting.  2.2. Power data synthesizers  Synthetic data does not face the challenges of collection difficulty  and noise interference, making it promising for NILM applications. The  SynD and ANTgen models measure and statistically analyze real appli - ance  energy  consumption  data,  synthesizing  low-frequency  data  for  NILM through script-driven methods [ 32 , 33 ]. However, these generated  data rely on predefined patterns and configuration files, limiting the  diversity of the generated load data and making it difficult to simulate  complex and variable actual appliance usage. Unlike generative models,  these models cannot generate diverse load data by learning the distri - bution  of  actual  data.  As  the  performance  of  generative  models  im - proves,  recent  research  has  attempted  to  use  generative  models  to  synthesize load data. GAN has shown potential in generating data [ 34 , 35 ]. References [ 15 , 36 ] used the principles of GAN to design models for  generating load data. These works improved the NILM effect to a certain  extent by replacing the load data at startup with data generated by GAN.  However, the generated data of these works are limited to replacing the  startup data, and the proportion of replaced data is only about 5 %. Liu  et al. [ 36 ] also found that although the quality of the generated data is  very high, the effect decreases as the replacement ratio increases. It can  be said that previous work on using GAN to generate load data is more of  a regularization method for NILM, with relatively limited performance  improvement. Moreover, due to the adversarial training of GAN, these  works  require  multiple  complex  training  and  parameter  adjustments  when synthesizing data. Additionally, when faced with complex elec - trical appliances, GAN may experience mode collapse problems [ 20 ].  These issues limit GAN ’ s ability to synthesize high-quality load data. In view of this, we apply the diffusion model to NILM for load data  synthesis,  aiming  to  improve  the  model ’ s  decomposition  effect  and  reduce data scarcity. At the same time, we note that the main challenge  of the diffusion model is that the gradual denoising process requires  significant  computing  resources,  with  high-dimensional  matrix  and  probability  distribution  calculations  needed  at  each  time  step  [ 37 ].  When dealing with long sequence data such as electrical load data, the  diffusion model consumes substantial computing resources, so we aim to  optimize it for the NILM field.  Z. Geng et al.                                                                                                                                                                                                                                     Energy   320   (2025)   135423    2  

3. Methodology  Based on the above problems, this section first introduces how to use  the diffusion model to perform appliance-level modeling and generate  realistic, low-noise electrical load data. Then, for load decomposition,  the optimization difficulties of the mainstream NILM loss function are  analyzed, and an improved loss function is proposed. Finally, to address  the problem of large fluctuations in the model decomposition signal, a  post-processing algorithm is proposed to improve the stability of the  model decomposition. These methods collectively constitute the pro - posed framework, which effectively enhances the decomposition capa - bility of the NILM model. A brief outline of this process is presented in  Fig. 1 .  3.1. Problem formulation  We use the diffusion model to generate single appliance power data   x it , where  i   ∈{ 1 , ... , N } is the index of the appliance,  N  is the number of  appliances,  t   ∈{ 1 , ... , T }   is the time step, and  T  is the total number of  time steps. As shown in  Fig. 2 , the model includes a forward process and  a reverse process. First, the samples sampled from the appliance load  data distribution  x 0   ∼ q ( x )   are gradually noised to standard Gaussian  noise  x T d   ∼ N   ( 0 , I ) where  T d    is the total number of diffusion steps, and   x 0    is the initial sample from the data distribution  q ( x ) . The forward  process  is  parameterized  by    q ( x τ   | x τ     1 ) =   N     x τ   ;   ̅̅̅̅̅̅̅̅̅̅̅̅̅ 1       β τ  √   x τ     1 ,   β τ   I ) ,  where  τ   ∈{ 1 , ... , T d }   is the diffusion step,  β τ   ∈( 0 , 1 )   is the amount of  noise added in the diffusion step  τ . Then the neural network gradually  denoises the samples through the reverse transfer  p θ   ( x τ     1 | x τ   ) =   N   ( x τ     1 ;  μ θ   ( x τ   ,   τ ) ,   Σ θ   ( x τ   ,   τ )) . This process can be expressed as learning the pa - rameters  μ θ   ( x τ   , τ ) through the reverse process. The primary objective of  the  diffusion  model  is  to  enable  the  model  to  progressively  denoise  random Gaussian noise data  x T d    and restore it to the original sample data   x 0    through the reverse diffusion process. After obtaining  x it , the aggre - gate power  y  can be expressed as:  y t    =   ∑ N i = 1 x it . When performing load decomposition tasks, given the total power  consumption record  y   =( y 1 , y 2 ,..., y T )   containing the power consump - tion of all appliances during time  T ,  n t    is the assumed measurement  noise, The total power consumption can be expressed as  y t   = ∑ N i = 1   x it   +  n t .The NILM problem can be modeled as a model that uses the total  power  y t    to obtain the power consumption  x it    of a single appliance.  3.2. Synthesizing robust NILM data using diffusion models  In the NILM dataset, many appliances have very few effective startup  parts [ 15 ]. It is not very meaningful to let the diffusion network learn the  shutdown features of appliances. Additionally, Kong et al. [ 38 ] found  that  increasing  the  proportion  of  home  appliance  activation  data  significantly  improves  the  model ’ s  training  effect.  Therefore,  we  designed Algorithm 1 to select the effective part of the appliance. The  starting state  T    of appliance  i  at time  t  is defined as follows:   T   = { t ⃒⃒ x i t   ≥ x i threshold , t ∈[ 0 , T ] }   (1)   Fig. 1.   The execution process of the NILM task in our framework.  Fig. 2.   Diffusion model for implementing load noise addition and de-noising.  Table 1   This article uses predefined parameters for different appliances.  Appliances   On power threshold   Mean power   Standard deviation KT   200   700   1000 MCW   200   500   800 FRG   50   200   400 DW   10   700   1000 WME   20   400   700  Z. Geng et al.                                                                                                                                                                                                                                     Energy   320   (2025)   135423    3  

The  x ithreshold    values are defined as shown in  Table 1   [ 39 ]. We choose to save the load data for the startup phase of the appli - ance, as well as the load data for all time steps of a window length before  the appliance turns on and after it turns off. This approach ensures that  we do not miss critical load information that may fall below the on/off  threshold. After obtaining the effective load information, we input the  data into the diffusion network. The effective part selection algorithm  we designed is shown in Algorithm 1.   Algorithm 1 .   Data Cleaning and Selection for Appliance Power Data.  Input: power sequence  x   = [ x 1 ,..., x t ,... ] , appliance start threshold  x threshold , window  length  l window    , power noise threshold  x noise  Output: cleaned and selected power data x 1: Initialize  T   selected    as an empty list 2:  x  [ x  <   x noise ]  = 0 3:  T   start    =   where( x   > = x threshold ) 4: for index in  T   start    do 5:   T   start    =   max(0, index -  l window ) 6:   T   end    =   min(len( x ), index  +   l window    + 1) 7:   T   selected .extend(range( T   start ,  T   end )) 8: end for 9:  T   selected    = sorted(set( T   selected )) 10:  x  = x  [ T   selected ] 11: scaler  =   MinMaxScaler() 12:  x  = scaler.fit_transform( x ) 13: return  x  To achieve efficient load data generation, we use the Transformer  architecture [ 40 ] in the diffusion model, which can effectively capture  dependencies in sequence data. Applying it to the diffusion model helps  process sequence data efficiently in parallel. When the Transformer implements the self-attention mechanism, it  uses the Query, Keys, Values ( Q ,  K ,  V ) matrices to calculate the attention  score. The formula is as follows:  Attention ( Q , K , V )= softmax  ( QK T  ̅̅̅ d √  )  V   (2)  In the NILM field, to comprehensively capture the operational infor - mation of appliances, time windows are usually designed to be relatively  long.  This  requires  the  Transformer  to  adjust  and  optimize  a  large  number of parameters when processing appliance load data. Addition - ally, the pervasive noise in the load data can disturb the large parameter  space, making the model difficult to train. These factors may prevent the  model from achieving optimal performance. To overcome this challenge and improve the model ’ s versatility in  generating load data, we are inspired by Han et al. [ 41 ] and introduce  Agent  attention,  verifying  its  effectiveness  in  generating  time  series  data. Specifically, we use agent tokens ( A ) as the agents of  Q  and first  aggregate global information from  K  and  V :   V Agg ( A , K , V )= Softmax  ( AK T  ̅̅̅ d √  )  V   (3)  After  the  aggregation  operation  is  completed,  we  broadcast  the  weight matrix  V Agg    back to  Q , perform the second attention calculation,  and obtain the attention output matrix. The broadcast operation is as  follows:   V Bcast    Q , A , V Agg  ) = Softmax  ( QA T  ̅̅̅ d √  )  V Agg   (4)  The complete attention calculation can be expressed as:  Attention ( Q , A , K , V )= Softmax  ( QA T  ̅̅̅ d √  )(  Softmax  ( AK T  ̅̅̅ d √  )  V  )  (5)  To improve the calculation speed and robustness, the dimension  n  of   A  should be smaller than the sequence length  N . Considering that the  model may encounter various complex appliance load synthesis tasks in  the future, we adopted a conservative setting of  N /8. This configuration  reduces the complexity of the self-attention mechanism by nearly an  order of magnitude while ensuring the essential extraction of appliance  features. When the attention matrices  Q ,  K , and  V  are in the space  R N × d ,  the matrix  A  is in the space  R n × d . Through Agent attention, we reduce  the time complexity of attention calculation from  O     N 2 d )   to  O   ( Nnd ) . The reduction in complexity reduces the accumulation of errors in  calculations, making training more stable. The reduction in the number  of parameters also reduces the risk of overfitting. The proxy aggregation  and proxy broadcast mechanisms used can combine global and local  information. The aggregation of global information reduces sensitivity  to a single noise location, which is very suitable for NILM tasks with  noisy data. The more robust and faster attention calculation effect is  shown in  Fig. 3 . Additionally, in time series feature extraction, a common strategy is  to decompose the series into trend and seasonal components [ 42 ]. We  apply  this  decomposition  method  to  power  load  data.  By  extracting  trend  components,  the  data  can  be  effectively  smoothed  and  noise  interference reduced. Through seasonal components, the operating cycle  characteristics of electrical appliances can be expressed, revealing their  operating patterns. The formula for trend decomposition of appliance  power using polynomial regression is as follows:   Trend τ   =   ∑ K i = 1    R ⋅     w ⋅   X i , τ tr   + b ) + λ i , τ  tr  )   (6)   R =[ 1 , r , ... , r p ]   (7)  Where  R  is the power matrix of vector  r , vector  r   =[ 0 , 1 , 2 , ⋯ , T ] T / T ,  T  is  the total number of time steps,  p  is the polynomial power,  X i , τ tr    represents  the trend component of the  i th    decoder ’ s output at diffusion step  τ , is   Fig. 3.   Robust and fast global self-attention mechanism.  Z. Geng et al.                                                                                                                                                                                                                                     Energy   320   (2025)   135423    4  

subjected to a linear transformation via a fully connected layer, followed  by tensor multiplication with  R .  λ i , τ tr    is the average output of the  i th    de - coder ’ s block,  K  is the number of decoder layers. While extracting trend  information, Fourier transform is used to extract seasonal features of  power  data.  These  two  characteristics  are  then  used  to  describe  the  changes in electric power values. The formula for extracting seasonal  characteristics using Fourier transform is as follows:   A ( k )  i , τ   = ⃒⃒ F     X i , τ seas  )  k  ⃒⃒ , Φ ( k )  i , τ   = φ   F     X i , τ seas  )  k  )   (8)   κ ( 1 )  i , τ   , ⋯ , κ ( K )  i , τ   =   arg TopK k ∈{ 1 , ⋯ , ⌊ T / 2 ⌋ + 1 }  {  A ( k )  i , τ  }  (9)   Season i , τ   = ∑ K k = 1  A κ ( k )  i , τ  i , τ  [  cos  (  2 π f κ ( k )  i , τ  rT + Φ κ ( k )  i , τ  i , τ  )  + cos  (  2 π f κ ( k )  i , τ  rT + Φ κ ( k )  i , τ  i , τ  )]  (10)  Where  A ( k )  i , τ    and  Φ ( k )  i , τ    represent the amplitude and phase of the  k th    fre - quency  after  Fourier  transform,    f κ    represents  the  Fourier  frequency  corresponding to the  k th    layer,  f κ    is its conjugate, arg TopK is to obtain  the highest  K  amplitudes to obtain the most significant part after Fourier  transform. The basic network structure and loss function used in this paper are  inspired by Ref. [ 31 ]. To make the synthetic data robust and account for  the dataset ’ s noise problem, we use the L1 norm loss function:  Where    F   F   T   ( x 0 )    F   F   T   ( ̂ x 0 ( x τ   , τ , θ )   measures  the  difference  be - tween  the  real  data    x 0    and  the  generated  data    ̂ x 0    in  the  frequency  domain after the fast Fourier transform.  ‖ x 0       ̂ x 0 ( x τ   , τ , θ )‖ 1    is used to  calculate the difference between the data  ̂ x 0    reconstructed by model  θ   and the real data  x 0    given the noise data  x τ    and diffusion step  τ . The   Fig. 4.   Diffusion denoising network.  Fig. 5.   Power scatter plot of original and synthetic dishwasher data in the UK-  Dale dataset.  Fig. 6.   An example of a power segment of our synthetic data.  L   θ   = E τ , x 0   [ w τ   [ λ 1   ‖ x 0       ̂ x 0 ( x τ   , τ , θ )‖ 1   + λ 2 ‖ F   F   T   ( x 0 )    F   F   T   ( ̂ x 0 ( x τ   , τ , θ ))‖ 1 ]]   (11)   Z. Geng et al.                                                                                                                                                                                                                                     Energy   320   (2025)   135423    5  

parameters  λ 1    and  λ 2    are used to adjust the weights,  w τ    is the weight of  each  diffusion  step,  and  finally,  the  losses  of  all  diffusion  steps  are  averaged to improve the robustness of the model under different times  and noise conditions. The basic structure of the diffusion network used to generate load  data is shown in  Fig. 4 . The network decomposes the load data into trend  and seasonal components, which are then summed up to form complete  power data. We plotted scatter plots of dishwasher data from house 1, house 2,  and data generated using the dishwasher data from house 2, as shown in  Fig. 5 . These scatter plots illustrate the power load distributions. It can  be seen that the data we generated matches the load distribution of the  original appliance and has less noise than the original data. It can also be  noted that although the brands of the same appliances are different, the  data distribution is similar. This further demonstrates the feasibility of  the  generative  model  in  generating  complex  and  diverse  household  appliance energy consumption sequences. The synthesized aggregate power can be expressed as the sum of the  synthesized electrical appliance powers. Our synthetic dataset enables  the model to focus on the load decomposition of the five training ap - pliances. At the same time, the data is cleaner and the effective data  density is higher. The complete dataset synthesized by our diffusion  model is shown in  Fig. 6 . This paper augments the original dataset with  synthetic data to create a more robust dataset. We use this enhanced  dataset to train the NILM model, thereby improving its decomposition  capabilities.  3.3. Robust loss function for NILM tasks  In addition to the dataset issues, the loss function used by the NILM  model significantly impacts the decomposition performance. To accu - rately  capture  the  subtle  changes  in  electrical  appliances,  current  mainstream NILM models primarily use mean square error (MSE) as the  loss function. However, as an L2 norm loss function, MSE is very sen - sitive to noise.  Fig. 5   shows that there are numerous noise points in the  NILM dataset. Even with our synthetic data, some noise is inevitable.  Consequently,  when  MSE  is  used  as  the  loss  function,  noise  points  significantly increase the total loss value and interfere with the model  training process. Mean absolute error (MAE) is another common loss  function that does not over-penalize large noise errors. However, MAE  lacks  the  capability  and  convergence  of  MSE  to  capture  small  load  changes. In our experiments, we found that MAE often leads to under - fitting when training the NILM model. To achieve a robust loss function while maintaining good conver - gence, we adopted the idea of the Huber loss function and introduced a  loss function that combines the characteristics of L1 and L2 norms in  NILM. This allows the network to effectively handle a large number of  outlier noise values while maintaining smooth convergence. We also  considered that NILM, as a single-channel source separation regression  task, should still be evaluated based on appliance on/off classification  metrics.  Therefore,  we  included  an  on/off  detection  component  to  enhance  the  identification  of  appliance  switching  events.  The  loss  function we propose is as follows:   L   ( x , s )= 1  T  ∑ T i = 1  ⎛ ⎜⎝  ( ̂ x i       x i ) 2   if | ̂ x i       x i |≤ δ δ | ̂ x i       x i |    1 2 δ 2   if | ̂ x i       x i | >   δ  ⎞ ⎟⎠ + α  T  ∑ T i = 1  ( ̂ s i       s i ) 2  (12)  Specifically,  x i    is the power value, and  s i   ∈( 0 , 1 )   indicates whether  the appliance is on.  α    measures the proportion of load decomposition  and load identification in the loss function. The criteria for detecting  whether the appliance is on are shown in  Table 1 . Our loss function not  only improves the model ’ s ability to resist outliers but also retains the  convergence advantages of MSE as much as possible. The experimental  results shown in  Fig. 7   demonstrate that the loss function designed in  this paper exhibits higher effectiveness in the NILM task.  3.4. The postprocessing method  Since  the  data  itself  is  noisy  and  the  decomposition  network  is  affected by factors such as the loss function and training complexity, the  working sequence of the decomposed target electrical appliances has  fluctuations that do not conform to the actual working scenario (e.g.,  does not conform to the minimum time interval for turning off or on the  electrical appliances). We propose a post-processing algorithm that uses  the idea of an exponentially weighted moving average to consider the  impact of both historical and recent data on the decomposition results,  thereby reducing signal fluctuations caused by noise and outliers. When  the  algorithm  processes  the  decomposition  result    x t    of  the  model,  considering that  x t   = p t   + n t ,where  p t    is the actual electrical power and   n t    is  the  noise  term,  and  assuming  that  the  noise  conforms  to  the  Gaussian distribution  N     0 , σ 2 ) , the output signal of the post-processing  algorithm is  s t   = α ( p t   + n t )+( 1       α ) s t     1 . At this point, the variance of  the noise part is  ασ 2 / ( 2       α ) , When  α  < 1, the noise part of the output is  suppressed, and the stability of the decomposed signal is improved. The  post-processing method is implemented as shown in Algorithm 2.   Algorithm 2 .   Decomposition Signal Post-Processing Algorithm  Input: decomposed power results  x   =[ x 1 ,..., x t ,... ] , appliance start threshold  x threshold ,  α  Output: post-processed power data  s   = [ s 1 ,..., s t ,... ]  1: Initialize  s  as an array of zeros with the same shape as x 2: Initialize  f active    as False,  x last    as 0 3: for  t  in range ( len ( x ) ) do 4:    if  x t    >   x threshold    then 5:   if not  f active    then 6:   s t    = x t  7:   x last    = x t  8:   f active    = True 9:   else 10:   s t    =   α * x t   +( 1       α ) * x last  11:   x last    =   s t  12:   else 13:   s t    = x t  14:   f active    = False 15: return  s  Fig.    7.   Comparison    of    the    effects    of    three    loss    functions    when  training microwave.  Z. Geng et al.                                                                                                                                                                                                                                     Energy   320   (2025)   135423    6  

4. Experimental settings and results  This  section  details  the  experiments  performed  to  validate  the  effectiveness of our work. We first verify the quality of the synthetic load  data and then assess the improvement of the framework for NILM model.  The model is tested both within the original house and across different  house to determine whether its performance is improved. Finally, we  conducted an ablation experiment to evaluate the effectiveness of each  step in enhancing the model. We trained and tested the neural network  on a CUDA RTX 4060 Laptop device.  4.1. Dataset settings  Appliances: We use the five most commonly used appliances (mi - crowave, fridge, dishwasher, washing machine, kettle) from the UK-Dale  [ 43 ] low-frequency dataset (1/6 Hz) for our experiments. Since some  residential data lacks common electrical appliance information or the  sampling time is too short, we selected the dataset of house 2 for data  synthesis  and  decomposition  and  conducted  cross-household  decom - position tests on house 1. For the entire experiment, the training set,  validation set, and test set are split in the ratio of 6:2:2. Data pre-processing: The raw data needs to be preprocessed before  being input into the network. We use the data filling method [ 44 ] to  address the missing value problem when collecting data from electricity  meters. When synthesizing data, we execute Algorithm 1 on the training  data,  send  it  to  the  diffusion  model  for  synthetic  data  training,  and  perform inverse Min-Max normalization to obtain complete power data.  To improve the accuracy of the synthetic data, we incorporate a post hoc  mechanism to ensure that the synthetic power is greater than zero and  discard any samples that do not meet the threshold, as in Ref. [ 15 ].  These simple but effective mechanisms enhance the fidelity of synthetic  data  and  promote  its  usefulness  in  real-world  applications.  Before  training the NILM model, the synthetic dataset is randomly inserted into  the original dataset in varying quantities. The mixed dataset then un - dergoes  Z-score  normalization  to  accelerate  the  neural  network ’ s  convergence and simplify the NILM training process. The parameters  used for the entire data preprocessing are shown in  Table 1 .  4.2. Network settings  We conduct load data synthesis and load decomposition tests on five  appliances.  Table 2   lists the key parameters used by the diffusion model  for data synthesis.  Table 3   lists the main parameters used for training the  NILM model during the load decomposition tasks.  4.3. Evaluation of generated data quality  To evaluate the authenticity and validity of the generated data, we  conducted both qualitative analysis and quantitative evaluation on the  synthetic data. We performed a qualitative analysis on the synthetic data visually, as  shown in  Fig. 8 . It can be observed that while the power traces of the five  synthesized appliances differ from the actual power traces in terms of  operating  status,  their  power  characteristics  are  highly  similar.  This  demonstrates that the generated signals exhibit behaviors comparable to  the  actual  appliances  and  contain  the  main  characteristics  of  each  appliance. Despite some visible noise levels in the generated power data,  there is a significant improvement over the large amounts of noise in the  original data. When conducting quantitative evaluation of the synthetic data, it is  important to note that evaluating synthetic tasks is much more chal - lenging than tasks such as classification or regression, which have clear  evaluation  metrics  [ 45 ].  Currently,  there  is  no  universal  method  to  evaluate the feature similarity and diversity of two power datasets. We  selected  commonly  used  evaluation  metrics  in  the  generation  field:  Context-FID [ 46 ], SWD [ 47 ], Classification Score [ 35 ], Predictive Score  [ 35 ] to assess the quality of our synthetic data. Context-FID Score: The pre-trained TS2Vec model is used to extract  the features of the generated data and the real data, and the degree of  difference between the two in terms of contextual features is calculated.  The calculation formula is as follows:  FID   =‖ μ R       μ G  ⃒⃒ | 2   + Tr   Σ R   + Σ G       2 ( Σ R Σ G ) 1 / 2 )   (13)  Where  μ R  and  Σ R  are the mean and covariance of the real data features,   μ G    and  Σ G    are the mean and covariance of the generated data features. SWD: This metric measures the difference in signal distribution be - tween the generated data and the real data. The data is projected into  one-dimensional space, and the average of the one-dimensional Was - serstein distance in all projection directions is calculated. The calcula - tion formula is as follows:  SWD   = 1  N  ∑ N i = 1  W   P θ i   ( R ) , P θ i   ( G ) )   (14)   Where  N  is the number of projections,  P θ i    represents the distribution of  data in the  i  th    projection direction, and  W  represents the Wasserstein  distance. Classification  Score:  By  training  an  LSTM-based  classification  network to distinguish between real and generated data, if the accuracy  of the classification network is around 0.5, it indicates that the real data  and synthetic  data are  very similar and difficult for the classifier to  distinguish. Classification Score is calculated as |accuracy      0.5|. Predictive Score: This metric reflects whether the synthetic data re - tains the temporal dependencies and dynamic characteristics of the real  data. A GRU-based prediction network is trained using synthetic data to  forecast real sequences, and the MAE between the actual values and the  predictions is used to evaluate the predictive score. We used publicly available generative models GAN and TimeGAN as  baselines to compare with the diffusion model. Considering that Time - GAN has great difficulty processing long window sequences, we adjusted  the window length to 60 for experiments and iterated each indicator 10  times. The experimental results are shown in  Table 4 . The experimental  results indicate that the diffusion model outperforms in many metrics,   Table 2   Basic parameter settings of diffusion model for synthetic data.  The main parameters of diffusion model   The values Seq length   10000 Window length   512 Agent tokens   64 Model dimension   128 Initial learning rate   1.0e-6 Optimizer   Adam Batch size   128 Epochs   20000 Timesteps   2000 Algorithm1-window   100  Table 3   Basic parameter settings of NILM.  The main parameters of NILM model   The values Window length   600 Initial learning rate   1.0e-3 Optimizer   Adam Batch size   1024 Epochs   100 Patience   20 Loss Function- α   0.1 Loss Function- δ   0.5 Postprocessing- β   0.5,0.9  Z. Geng et al.                                                                                                                                                                                                                                     Energy   320   (2025)   135423    7  

proving that it can generate higher quality electrical load samples than  previous mainstream generation models.  4.4. Decomposition performance evaluation  We  conducted  comparative  experiments  on  origin-household  and  cross-household datasets to verify whether our work has improved the  robustness  and  versatility  of  the  NILM  model.  To  ensure  that  the  observed improvements are not due to fluctuations in network training,  we implemented a simple NILM model using one-dimensional convo - lution and fully connected layers for stable training. We used this model  and  conducted  five  experiments,  averaging  the  results  to  verify  the   Fig. 8.   Example comparison of generated appliance power traces and actual traces for UK-Dale. It can be observed that the signal generated by the diffusion model is  very close to the real data in terms of important characteristics such as power level, but it does not directly replicate the real trace.  Z. Geng et al.                                                                                                                                                                                                                                     Energy   320   (2025)   135423    8  

effectiveness of our work. We  tested  five  electrical  appliances.  The  baseline  model  uses  200,000  original  data  points.  The  comparison  model ’ s  dataset  is  a  mixture of the original dataset  D real  and the synthetic dataset  D syn  where   D mix   = D real   ∪ D syn    and    | D real | ,   | D syn |∈ { 10 5 ,   2   ×   10 5 } .  Data  mixing is  achieved by setting the Shuffle parameter of the data loader to True.  Except for the baseline model, all other models are trained using the loss  function and post-processing method designed in this paper. The vali - dation set and test set remain unchanged in all experiments. We chose Mean Absolute Error (MAE), Signal Aggregate Error (SAE),  and F1-score (F1) as evaluation metrics for decomposition effect. Denote   ̂ x t    as the prediction of a considered appliance at time t and  x t    as the  ground truth. MAE is a general metric to measure the estimation error at  each  time  point.  MAE  is  formally  computed  by: MAE   =  (∑ T t = 1  ⃒⃒̂ x t      Table 4   Quantitative evaluation and comparison of diffusion models.  Error metric   Model   MCW   FRG   DW   WME   KT  Context-FID  (Lower the Better) GAN   13.601  ± 1.811   43.840  ±   16.767   11.151  ± 1.841   4.283  ± 0.360   3.642  ± 0.533 TimeGAN   3.650  ±   0.425   2.583  ±   0.420   2.803  ±   0.135   3.334  ± 0.531   3.327  ± 0.213 Diffusion model   0.680  ±  0.049   0.214  ±  0.008   0.309  ±  0.017   0.556  ±  0.163   0.238  ±  0.017  SWD  (Lower the Better) GAN   0.404  ±   0.020   0.377  ±   0.031   0.323  ±   0.025   0.334  ± 0.023   0.368  ± 0.028 TimeGAN   0.150  ±   0.006   0.105  ±   0.007   0.200  ±   0.008   0.169  ± 0.008   0.182  ± 0.006 Diffusion model   0.056  ±  0.001   0.022  ±  0.001   0.050  ±  0.002   0.070  ±  0.004   0.057  ±  0.002  Classification   GAN   0.312  ±   0.098   0.374  ±   0.089   0.266  ±   0.067   0.386  ± 0.024   0.437  ± 0.009 Score   TimeGAN   0.328  ±   0.194   0.251  ±   0.188   0.324  ±   0.220   0.383  ± 0.214   0.424  ± 0.144 (Lower the Better)   Diffusion model   0.100  ±  0.002   0.043  ±  0.022   0.059  ±  0.011   0.179  ±  0.003   0.065  ±  0.003  Predictive   GAN   0.689  ±   0.012   0.812  ±   0.001   0.745  ±   0.000   0.809  ± 0.001   0.842  ± 0.001 Score   TimeGAN   0.179  ±   0.000   0.126  ±   0.000   0.243  ±   0.000   0.188  ± 0.000   0.184  ± 0.000 (Lower the Better)   Diffusion model   0.082  ±  0.000   0.115  ±  0.000   0.216  ±  0.000   0.137  ±  0.000   0.127  ±  0.000  Table 5   Experimental results of our framework on the UK-Dale dataset with different degrees of data augmentation of the origin-household (house2).  origin-  household  scene MCW   FRG   DW   WME   KT MAE  (Watt) SAE  (%) F1  (%) MAE  (Watt) SAE  (%) F1  (%) MAE  (Watt) SAE  (%) F1  (%) MAE  (Watt) SAE  (%) F1  (%) MAE  (Watt) SAE  (%) F1  (%) Origin(200k)   4.83   64.40   77.77   17.83   6.02   87.05   6.37   1.73   64.27   5.59   5.54   61.41   7.88   1.02   75.72 100k + 100k   2.78   24.26   82.28   15.37   1.41   87.46   5.10   4.83   71.53   7.90   27.60   45.48   4.86   7.15   92.40  100k + 200k   2.51   21.29   83.23   14.54   0.68   88.84   4.94   3.35   70.25   7.21   23.37   63.84   5.31   6.81   91.94 200k + 100k   2.66   18.15   83.42   15.39   2.11   87.47   4.74   3.62   77.50   7.15   24.09   68.47   4.74   8.58   90.54 200k + 200k   2.37   16.70   82.66   14.30   2.53   89.08   4.81   4.72   75.93   5.43   13.12   72.01   5.31   9.71   91.65  Fig. 9.   Comparison of decomposition results between the baseline model and the model trained with our framework.  Z. Geng et al.                                                                                                                                                                                                                                     Energy   320   (2025)   135423    9  

x t |) /   T . SAE measures the total estimation error over the total test time  and is formally computed by: SAE   =| ̂ r       r | / r  where  ̂ r i   =   ∑ T i = 1   ̂ x i ,  r i   = ∑ T i = 1   x i . F1 measures the accuracy of appliance switching predictions  and is calculated as  F 1   = 2   PR /   ( P +   R ) where Precision ( P ) is  P   =   TP /  ( TP + FP )   and Recall ( R ) is  R   = TP /   ( TP +   FN ), these metrics are based  on True Positives ( TP ), True Negatives ( TN ), False Positives ( FP ), and  False Negatives ( FN ). The threshold settings for electrical switches are  shown in  Table 1 . The decomposition effects of the five electrical ap - pliances in the origin-household are shown in  Table 5 . As seen in  Table 5 , most of the best metrics for individual appliances  come from models trained using our framework. While the SAE metrics  for  the  dishwasher,  washing  machine,  and  kettle  decreased,  the  improvement in MAE and F1 values suggests that the decomposition  effects for these appliances are still enhanced.  Fig. 9   shows the decom - position effects of the baseline model compared to the 200K  +   200K  model. It is evident that our work significantly improved the stability of the  NILM  decomposition  signal.  The  decomposition  signal  no  longer  ex - hibits small noise fluctuations, and the decomposition effect is much  better than the original model. To  verify  the  framework ’ s  ability  to  generalize  across  different  house, we conducted a decomposition test on the electrical load data of  house 1. The training results are shown in  Table 6 . Table  6   demonstrates  that  our  framework  improves  the  model ’ s  performance across house, with almost all metrics showing some level of  improvement. The overall decomposition effects for the origin-household and cross-  household scenarios are shown in  Table 7 . Additionally,  Fig. 10   illus - trates  the  improvement  ratio  of  the  model ’ s  overall  decomposition  performance under different data volumes. From  the  chart,  it  is  evident  that  the  model  trained  using  our  framework  outperforms  the  baseline  model  in  all  three  metrics  for  origin-household  decomposition.  The  best-performing  model,  trained  with 200,000 original data points and 200,000 synthetic data points,  shows an average MAE improvement of 24.24 %, with SAE improving  the most significantly by 40.53 %, indicating better long-term energy  decomposition performance. The F1 score, as a recognition indicator,  also improved by 12.33 %. When  performing  a  cross-household  decomposition  task,  our  framework still enhances the model ’ s decomposition effect. However,  the best results are not always from the model trained with 200,000  original data points and 200,000 synthetic data points. The differences  are  relatively  close  overall.  Given  the  instability  of  cross-household  scenarios, we still use the 200K  +   200K model as the optimal model,  achieving an average MAE improvement of 16.60 %, with SAE showing  the largest improvement at 53.67 %, and an average F1 improvement of  2.93 %. Based on the overall decomposition results of the NILM model in  both origin-household and cross-household experiments, although our   Table 6   Experimental results of our framework on the UK-Dale dataset with different degrees of data augmentation cross-household (house1).  Cross-  household  scenario MCW   FRG   DW   WME   KT  MAE  (Watt) SAE  (%) F1  (%) MAE  (Watt) SAE  (%) F1  (%) MAE  (Watt) SAE  (%) F1  (%) MAE  (Watt) SAE  (%) F1  (%) MAE  (Watt) SAE  (%) F1  (%) Origin(200k)   16.47   99.84   32.00   39.40   18.66   54.60   24.71   49.57   29.07   37.48   67.09   57.11   16.25   16.72   63.09  100k + 100k   11.71   6.24   30.66   38.47   15.17   56.39   19.14   22.84   35.92   24.59   27.09   56.39   13.82   6.35   62.51 100k + 200k   12.18   12.99   25.80   38.16   15.73   56.49   22.52   40.95   27.13   24.92   8.74   67.19   14.66   12.61   63.29 200k + 100k   11.06   14.67   25.31   37.55   2.61   52.16   23.08   32.92   30.05   25.98   18.64   65.56   13.80   3.16   64.99 200k + 200k   12.47   19.16   27.80   37.72   16.51   58.46   21.79   36.73   26.48   25.69   41.11   67.25   14.34   3.20   62.78  Table 7   Performance of the model on the origin-household and Cross-household.  Overall  Improve origin-household   cross-household MAE  (Watt) SAE  (%) F1  (%) MAE  (Watt) SAE  (%) F1  (%) Origin  (200k) 8.50   15.74   73.24   26.86   50.38   47.17 100k + 100k   7.20   13.05   75.83   21.55   15.54   48.37 100k + 200k   6.90   11.1   79.62   22.49   18.20   47.98 200k + 100k   6.94   11.31   81.48   22.29   14.40   47.61 200k + 200k   6.44   9.36   82.27   22.40   23.34   48.55  Fig.  10.   The  percentage  improvement  of  different  models  on  the  overall  decomposition index.  Z. Geng et al.                                                                                                                                                                                                                                     Energy   320   (2025)   135423    10  

framework  improves  decomposition  performance  when  addressing  insufficient original datasets (100K  +   100K, 100K  +   200K), the best  performance enhancement is achieved when the dataset is expanded  (200K  + 100K, 200K  + 200K). To validate the improvement brought by our framework to other  decomposition models, we selected two classic models, S2P [ 11 ] and  FCN  [ 48 ],  as  well  as  a  recently  published  model,  AugLPN  [ 39 ],  for  testing.  The  original  models  were  trained  on  200,000  samples  from  House 2 of the UK-Dale dataset. Models marked with an asterisk (*)  indicate those enhanced using our framework, which employed 200,000  original samples along with an additional 200,000 synthesized samples.  The specific parameter settings are consistent with the original paper.  The results of load decomposition are presented in  Tables 8 and 9 . The results show that our framework generally improves the disag - gregation  performance  of  mainstream  models.  However,  it  is  note - worthy that for the state-of-the-art model AugLPN, the improvement  brought by our framework is not as significant as for the other models,  and in some metrics, there is even a slight decrease. This outcome can be  understood in the context that our work mainly aims to enhance the  noise robustness of NILM models, while AugLPN, being a novel model in   Table 8   The enhancement effect of the framework on the origin-household(house2) in three public decomposition models.  origin-  household  scene MCW   FRG   DW   WME   KT MAE  (Watt) SAE  (%) F1  (%) MAE  (Watt) SAE  (%) F1  (%) MAE  (Watt) SAE  (%) F1  (%) MAE  (Watt) SAE  (%) F1  (%) MAE  (Watt) SAE  (%) F1  (%) S2P   4.65   70.03   76.94   15.48   10.14   86.88   6.36   3.07   69.26   10.61   24.00   35.04   11.15   39.04   95.50 S2P*   1.65   13.04   83.11   13.63   3.22   87.80   7.81   10.79   79.05   12.63   17.76   26.67   11.83   41.89   97.24  FCN   4.87   57.30   73.18   15.29   7.69   85.39   9.85   6.72   48.09   16.38   3.45   18.05   6.37   15.16   95.98 FCN*   1.52   3.01   89.27   10.24   5.14   90.09   5.27   1.10   83.25   11.74   60.30   37.87   5.81   13.30   96.22  AugLPN   4.64   63.96   85.38   9.52   4.56   92.43   3.80   0.04   80.07   4.03   10.70   74.80   5.63   11.41   97.00  AugLPN*   2.92   43.22   87.28   11.18   6.85   90.50   3.02   4.07   84.99   5.78   25.67   77.64   4.51   7.57   96.85  Table 9   The enhancement effect of the framework on the cross-household(house1) in three public decomposition models.  cross-  household  scene MCW   FRG   DW   WME   KT MAE  (Watt) SAE  (%) F1  (%) MAE  (Watt) SAE  (%) F1  (%) MAE  (Watt) SAE  (%) F1  (%) MAE  (Watt) SAE  (%) F1  (%) MAE  (Watt) SAE  (%) F1  (%) S2P   23.39   201.58   21.73   39.49   42.63   64.39   21.95   26.51   19.70   46.42   35.11   39.30   24.41   64.54   51.35 S2P*   15.01   28.84   14.43   36.25   12.75   58.82   34.26   64.39   27.26   36.98   2.54   33.40   23.09   49.87   59.61  FCN   18.66   108.30   27.89   41.08   22.42   51.90   31.68   86.34   24.17   47.66   34.06   22.14   15.67   46.45   60.23 FCN*   13.87   28.23   25.80   38.21   21.81   58.68   29.76   89.41   37.63   31.99   57.03   52.94   10.68   10.47   78.56  AugLPN   13.92   76.59   33.97   33.43   4.03   57.94   27.52   63.61   37.79   22.57   4.82   67.29   9.14   43.98   90.45  AugLPN*   14.81   85.95   26.72   31.93   10.24   66.79   28.98   91.76   35.32   21.14   32.08   75.91   9.79   37.98   89.36  Fig. 11.   Comparison of load decomposition performance of the AugLPN model enhanced by our framework.  Table 10   Time required for training the diffusion model and synthesizing 400 load data  samples with a window length of 512.  Scheme   N (Self-attention)   N/8   N/32   N/128  Training time   4h41m45s   1h50m34s   1h33m49s   1h29m3s Synthesis time   1h27m2s   37m8s   19m32s   19m26s  Z. Geng et al.                                                                                                                                                                                                                                     Energy   320   (2025)   135423    11  

the field, already possesses strong capabilities for extracting load fea - tures from noisy data. As a result, the effect of our framework is some - what  diminished.  However,  this  does  not  imply  that  the  framework  cannot  enhance  the  performance  of  this  relatively  noise-resistant  decomposition model. As shown in  Fig. 11 , we plotted the decomposi - tion  results  of  AugLPN,  revealing  that  our  framework  makes  its  decomposition  outcomes  more  stable.  This  suggests  that  with  more  optimal  parameter  settings,  the  framework  could  still  significantly  improve the performance of AugLPN.  4.5. Ablation study  To assess the practical value of the proposed framework, we first  analyze the training time of the diffusion model and the time required  for synthetic data generation. Keeping other parameters constant, we  compare the training and generation times for producing load samples of  length 204,800 using the original self-attention mechanism and diffu - sion models with varying numbers of Agent tokens  n  under a window  length of  N   = 512. The results are presented in  Table 10 . The experimental results show that our improvements to the Diffu - sion model significantly reduce the time cost of data synthesis, enabling  faster generation of high-quality load data and facilitating future ad - justments by researchers. However, the training and data generation  time  of  the  Diffusion  model  does  not  decrease  indefinitely  with  a  reduction in the number of Agent tokens. This can be explained by the  fact  that  once  the  number  of  Agent  tokens  drops  below  a  certain  threshold, the time complexity of the self-attention mechanism is no  longer the bottleneck for the Diffusion model ’ s operation. Nevertheless,  using Agent tokens still brings significant improvements in the speed of  the Diffusion model. At the same time, to evaluate the effectiveness of the proposed robust  synthetic data generation method, optimized loss function, and post-  processing algorithm for NILM tasks, we compared the model trained  using the complete framework with three variant models.  (1)  w/o Robust Data: The training network was not trained using the  robust data we generated. (2)  w/o  Robust  Loss:  The  training  network  does  not  use  the  loss  function we designed, but uses the mainstream MSE for training. (3)  w/o Post-proc: The training results do not use our post-processing  algorithm. The results are shown in  Table 11 . As shown in  Table 11 , the improvement brought by our work can be  attributed to two advantages of our framework: 1) Our synthetic data,  when added to the original dataset, enriches the diversity of training  samples. The synthetic data is of high density and low noise, which helps  the model focus on learning and decomposing the startup features of five  appliances.  2)  Through  the  robust  loss  function  and  post-processing  steps, the model effectively prevents noise interference during NILM  training. In summary, the experiments demonstrate that for most electrical  appliances, the work in this paper can effectively reduce reliance on the  original dataset. Additionally, under data augmentation conditions, the  decomposition ability of the model can be greatly improved. The clean  and high-density effective data we synthesized can enhance the overall  accuracy and generalization of the decomposition model. However, for  some  appliances,  the  improvement  is  minimal,  and  in  some  cases,  certain  metrics  have  decreased.  This  may  be  due  to  not  finding  the  appropriate parameters for each appliance. Due to training costs and  other  factors,  the  experiment  did  not  identify  the  optimal  training  parameter  settings  and  the  appropriate  proportion  of  synthetic  data  addition for each appliance ’ s characteristics. We have only validated the  effectiveness of certain parameter settings within the framework. It is  worth noting that in the field of non-intrusive load monitoring (NILM),  achieving  accuracy  in  cross-household  load  decomposition  remains  challenging due to the variability in appliance models and operating  states. Our work, through the Diffusion model, has shown potential in  artificially enriching existing datasets. However, it has not yet led to  substantial progress in improving cross-household load decomposition  accuracy.  We  speculate  that  this  is  because  we  have  not  found  the  appropriate  configurations  for  the  Diffusion  model  tailored  to  each  appliance. If we can control the Diffusion model to conditionally utilize  prior knowledge to generate load data specific to target appliances, this  cross-household challenge could be effectively addressed.  In the future, researchers can leverage our framework in conjunction  with the characteristics of various electrical appliances, using appro - priate parameter settings to enhance the decomposition capabilities of  any  NILM  model.  Furthermore,  they  could  explore  how  to  use  the  Diffusion model to conditionally generate more personalized appliance  load data, allowing for the  synthesis of targeted appliance loads for  specific  households,  thereby  improving  the  performance  of  load  decomposition models in cross-household scenarios.  5. Conclusion  In  this  paper,  we  address  the  issues  of  poor  robustness  in  NILM  models and the scarcity of datasets by applying the diffusion model for  NILM  data  augmentation.  We  propose  a  diffusion  model  capable  of  generating realistic, low-noise appliance load data. By augmenting the  original  dataset with synthetic  data, our  approach effectively avoids  learning of numerous off-state features and data noise, while also alle - viating the problem of insufficient datasets. Additionally, to reduce the  sensitivity of mainstream decomposition models to outliers, we designed  a loss function that is more robust to noisy data and a post-processing  algorithm that enhances decomposition stability. Experimental results  demonstrate that the proposed framework enhances the model ’ s reli - ability in both origin-household and cross-household scenarios, as well  as its generalization performance to unseen appliances. In the future, our  framework can be used to identify more suitable parameter settings for  each  appliance  and  can  be  combined  with  various  NILM  models  to  improve their decomposition performance. Our work provides a new  avenue for addressing the challenges that hinder the development of  NILM.  Table 11   Ablation study for model architecture.  Ablation  experiment MCW   FRG   DW   WME   KT MAE  (Watt) SAE  (%) F1  (%) MAE  (Watt) SAE  (%) F1  (%) MAE  (Watt) SAE  (%) F1  (%) MAE  (Watt) SAE  (%) F1  (%) MAE  (Watt) SAE  (%) F1  (%) 200k + 200k   2.37   16.70   82.66   14.30   2.53   89.08   4.81   4.72   75.93   5.43   13.12   72.01   5.31   9.71   91.65  w/o Robust  Data 4.69   48.30   70.93   12.10   1.99   93.22   5.83   4.59   72.65   7.14   22.34   69.13   9.41   19.47   77.37 w/o Robust  Loss 4.08   54.95   79.49   15.29   1.55   87.87   5.10   1.51   65.71   8.34   38.11   33.43   7.13   4.27   87.03 w/o Post-proc   2.31   17.56   82.66   14.30   2.54   89.09   4.82   4.72   75.94   5.43   13.12   72.01   5.32   9.69   91.65  Origin(200k)   4.83   64.40   77.77   17.83   6.02   87.06   6.37   1.73   64.27   5.59   5.54   61.41   7.88   1.02   75.72  Z. Geng et al.                                                                                                                                                                                                                                     Energy   320   (2025)   135423    12  

CRediT authorship contribution statement Zeyi Geng:  Writing  –    original draft, Methodology, Conceptualiza - tion.  Linfeng Yang:  Writing  –  review  &  editing, Writing  –  original draft,  Supervision.  Wuqing Yu:  Writing  –  review  &  editing.  Declaration of competing interest  The authors declare that they have no known competing financial  interests or personal relationships that could have appeared to influence  the work reported in this paper.  Acknowledgment  This work was supported by the Natural Science Foundation of China  (72361003)  and  the  Key  Research  and  Development  Program  of  Guangxi (GuiKe-AB23075199).  Data availability  I have shared the link to my code in the paper  References  [1]   Razzak A, Islam MT, Roy P, Razzaque MA, Hassan MR, Hassan MM. Leveraging  deep Q-learning to maximize consumer quality of experience in smart grid. Energy  2024;290:130165 . [2]   Wu X, Jiao D, Liang K, Han X. A fast online load identification algorithm based on  VI characteristics of high-frequency data under user operational constraints.  Energy 2019;188:116012 . [3]   Çimen H, Çetinkaya N, Vasquez JC, Guerrero JM. A microgrid energy management  system based on non-intrusive load monitoring via multitask learning. IEEE Trans  Smart Grid 2020;12(2):977 – 87 . [4]   Hart GW. Nonintrusive appliance load monitoring. Proc IEEE 1992;80(12):  1870 – 91 . [5]   Kelly J, Knottenbelt W. Neural NILM: deep neural networks applied to energy  disaggregation. In: Proceedings of the 2nd ACM international conference on  embedded systems for energy-efficient built environments; 2015. p. 55 – 64 . [6]   Çimen H, Wu Y, Wu Y, Terriche Y, Vasquez JC, Guerrero JM. Deep learning-based  probabilistic autoencoder for residential energy disaggregation: an adversarial  approach. IEEE Trans Ind Inf 2022;18(12):8399 – 408 . [7]   Zhou K, Zhang Z, Lu X. Non-intrusive load monitoring based on an efficient deep  learning model with local feature extraction. IEEE Trans Ind Inf 2024;20(7):  9497 – 507 . [8]   Pan Y, Liu K, Shen Z, Cai X, Jia Z. Sequence-to-subsequence learning with  conditional GAN for power disaggregation. In: ICASSP 2020-2020 IEEE  international conference on acoustics, speech and signal processing (ICASSP);  2020. p. 3202 – 6 . [9]   Li K, Feng J, Zhang J, Xiao Q. Adaptive fusion feature transfer learning method for  NILM. IEEE Trans Instrum Meas 2023;72:1 – 12 . [10]   Ding D, Li J, Wang H, Wang K. Load recognition with few-shot transfer learning  based on meta-learning and relational network in non-intrusive load monitoring.  IEEE Trans Smart Grid 2024;15(5):4861 – 76 . [11]   Zhang C, Zhong M, Wang Z, Goddard N, Sutton C. Sequence-to-point learning with  neural networks for non-intrusive load monitoring. In: Proceedings of the AAAI  conference on artificial intelligence, vol. 32; 2018 . [12]   D ’ Incecco M, Squartini S, Zhong M. Transfer learning for non-intrusive load  monitoring. IEEE Trans Smart Grid 2019;11(2):1419 – 29 . [13]   He J, Zhang Z, Ma L, Zhang Z, Meng Li, Khoussainov B, et al. InFocus: amplifying  critical feature influence on non-intrusive load monitoring through self-attention  mechanisms. IEEE Trans Smart Grid 2023;14(5):3828 – 40 . [14]   Kaselimi M, Voulodimos A, Doulamis N, Doulamis A, Protopapadakis E. A robust to  noise adversarial recurrent model for non-intrusive load monitoring. In: ICASSP  2021-2021 IEEE international conference on acoustics, speech and signal  processing (ICASSP); 2021. p. 3335 – 9 . [15]   Harell A, Jones R, Makonin S, Baji  ́ c IV. TraceGAN: synthesizing appliance power  signatures using generative adversarial networks. IEEE Trans Smart Grid 2021;12  (5):4553 – 63 . [16]   Li J, Chen Z, Cheng L, Liu X. Energy data generation with wasserstein deep  convolutional generative adversarial networks. Energy 2022;257:124694 . [17]   Rafiq H, Manandhar P, Rodriguez-Ubinas E, Qureshi OA, Palpanas T. A review of  current methods and challenges of advanced deep learning-based non-intrusive  load monitoring (NILM) in residential context. Energy Build 2024;305:113890 . [18]   Himeur Y, Alsalemi A, Bensaali F, Amira A, Al-Kababji A. Recent trends of smart  nonintrusive load monitoring in buildings: a review, open challenges, and future  directions. Int J Intell Syst 2022;37(10):7124 – 79 . [19]   Schirmer PA, Mporas I. Non-intrusive load monitoring: a review. IEEE Trans Smart  Grid 2022;14(1):769 – 84 . [20]   Croitoru F-A, Hondru V, Ionescu RT, Shah M. Diffusion models in vision: a survey.  IEEE Trans Pattern Anal Mach Intell 2023;45(9):10850 – 69 . [21]   Ciancetta F, Bucci G, Fiorucci E, Mari S, Fioravanti A. A new convolutional neural  network-based system for NILM applications. IEEE Trans Instrum Meas 2020;70:  1 – 12 .  [22]   Hwang H, Kang S. Nonintrusive load monitoring using an LSTM with feedback  structure. IEEE Trans Instrum Meas 2022;71:1 – 11 . [23]   Ho J, Jain A, Abbeel P. Denoising diffusion probabilistic models. Adv Neural Inf  Process Syst 2020;33:6840 – 51 . [24]   Dhariwal P, Nichol A. Diffusion models beat GANs on image synthesis. Adv Neural  Inf Process Syst 2021;34:8780 – 94 . [25]   Harvey W, Naderiparizi S, Masrani V, Weilbach C, Wood F. Flexible diffusion  modeling of long videos. Adv Neural Inf Process Syst 2022;35:27953 – 65 . [26]   Ho J, Salimans T, Gritsenko A, Chan W, Norouzi M, Fleet DJ. Video diffusion  models. Adv Neural Inf Process Syst 2022;35:8633 – 46 . [27]   Li X, Thickstun J, Gulrajani I, Liang PS, Hashimoto TB. Diffusion-LM improves  controllable text generation. Adv Neural Inf Process Syst 2022;35:4328 – 43 . [28]   Luo Z, Lin X, Qiu T, Li M, Zhong W, et al. Investigation of hybrid adversarial-  diffusion sample generation method of substations in district heating system.  Energy 2024;288:129731 . [29]   Pintilie I, Manolache A, Brad F. Time series anomaly detection using diffusion-  based models. In: 2023 IEEE international conference on data mining workshops  (ICDMW); 2023. p. 570 – 8 . [30]   Rasul K, Seward C, Schuster I, Vollgraf R. Autoregressive denoising diffusion  models for multivariate probabilistic time series forecasting. In: International  conference on machine learning; 2021. p. 8857 – 68 . [31]   Yuan X, Qiao Y. Diffusion-TS: interpretable diffusion for general time series  generation. In: The twelfth international conference on learning representations;  2024 . [32]   Klemenjak C, Kovatsch C, Herold M, Elmenreich W. A synthetic energy dataset for  non-intrusive load monitoring in households. Sci Data 2020;7(1):108 . [33]   Reinhardt A, Klemenjak C. How does load disaggregation performance depend on  data characteristics? Insights from a benchmarking study. In: Proceedings of the  eleventh ACM international conference on future energy systems; 2020. p. 167 – 77 . [34]   Goodfellow I, Pouget-Abadie J, Mirza M, Xu B, et al. Generative adversarial nets.  Adv Neural Inf Process Syst 2014;27 . [35]   Yoon J, Jarrett D, Van der Schaar M. Time-series generative adversarial networks.  Adv Neural Inf Process Syst 2019;32 . [36]   Liu Z, Ji T, Chen J, Zhang L, Zhang L, Wu Q. Conditional-TimeGAN for realistic and  high-quality appliance trajectories generation and data augmentation in non-  intrusive load monitoring. IEEE Trans Instrum Meas 2024;73:1 – 15 . [37]   Li M, Cai T, Cao J, Zhang Q, Cai H, Bai J, et al. Distrifusion: distributed parallel  inference for high-resolution diffusion models. In: Proceedings of the IEEE/CVF  conference on computer vision and pattern recognition; 2024. p. 7183 – 93 . [38]   Kong W, Dong ZY, Wang B, Zhao J, Huang J. A practical solution for non-intrusive  type II load monitoring based on deep learning and post-processing. IEEE Trans  Smart Grid 2019;11(1):148 – 60 . [39]   Yu W, Yang L, Liu X. AugLPN-NILM: augmented lightweight parallel network for  NILM embedding attention module over sequence to point. Sustain Energy Grids  Netw 2024;38:101378 . [40]   Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones Llion, et al. Attention is all you  need. Adv Neural Inf Process Syst 2017;30 . [41]   Han D, Ye T, Han Y, Xia Z, Song S, Huang G. Agent attention: on the integration of  softmax and linear attention. arXiv preprint arXiv:2312.08874 2023.  https://arxiv.  org/abs/2312.08874 . [42]   Zeng A, Chen M, Zhang L, Xu Q. Are transformers effective for time series  forecasting?. In: Proceedings of the AAAI conference on artificial intelligence, vol.  37; 2023. p. 11121 – 8 . [43]   Kelly J, Knottenbelt W. The UK-DALE dataset, domestic appliance-level electricity  demand and whole-house demand from five UK homes. Sci Data 2015;2(1):1 – 14 . [44]   Allik A, Annuk A. Interpolation of intra-hourly electricity consumption and  production data. In: 2017 IEEE 6th international conference on renewable energy  research and applications (ICRERA); 2017. p. 131 – 6 . [45]   Borji A. Pros and cons of GAN evaluation measures: new developments. Comput  Vis Image Underst 2022;215:103329 . [46]   Jeha P, Bohlke-Schneider M, Mercado P, Kapoor S, et al. PSA-GAN: progressive self  attention GANs for synthetic time series. In: The tenth international conference on  learning representations; 2022 . [47]   Rabin J, Peyr  ́ e G, Delon J, Bernot M. Wasserstein barycenter and its application to  texture mixing. In: Scale space and variational methods in computer vision: third  international conference, SSVM 2011, ein-gedi, Israel, may 29 – June 2, 2011,  revised selected papers 3; 2012. p. 435 – 46 . [48]   Brewitt C, Goddard N. Non-intrusive load monitoring with fully convolutional  networks. arXiv preprint arXiv:1812.03915 2018. arxiv.org/abs/1812.03915 .  Z. Geng et al.                                                                                                                                                                                                                                     Energy   320   (2025)   135423    13  