# DiffusionModel_NILM è¶…è¯¦ç»†å®Œæ•´æµç¨‹ï¼ˆè®­ç»ƒ+é‡‡æ ·å…¨æµç¨‹ï¼‰

## ğŸ“š  ç›®å½•ç»“æ„ä¸æ–‡ä»¶ç´¢å¼•

```
DiffusionModel_NILM/
â”œâ”€â”€ main.py                          # é¡¹ç›®å…¥å£
â”œâ”€â”€ engine/
â”‚   â””â”€â”€ solver.py                    # è®­ç»ƒ/é‡‡æ ·æ§åˆ¶å™¨
â”œâ”€â”€ Models/
â”‚   â””â”€â”€ diffusion/
â”‚       â”œâ”€â”€ gaussian_diffusion.py    # Diffusion åŒ…è£…ç±»
â”‚       â”œâ”€â”€ agent_transformer.py     # DiT Transformer ä¸»æ¨¡å‹
â”‚       â””â”€â”€ model_utils.py           # å·¥å…·å‡½æ•°(ä½ç½®ç¼–ç /AdaLNç­‰)
â””â”€â”€ Utils/
    â””â”€â”€ Data_utils/
        â””â”€â”€ real_datasets.py         # æ•°æ®é¢„å¤„ç†
```

---

```mermaid
flowchart TB
    %% ========== å…¥å£å±‚ ==========
    START["ğŸš€ å¯åŠ¨å‘½ä»¤<br/>python main.py --config Config/microwave.yaml --train/--sample"]
    
    subgraph MAIN ["ğŸ“„ main.py - é¡¹ç›®å…¥å£"]
        M1["parse_args()<br/>è§£æå‘½ä»¤è¡Œå‚æ•°"]
        M2["load_config(yaml_path)<br/>è¯»å– YAML é…ç½®"]
        M3["get_dataloader(cfg)<br/>åˆ›å»ºæ•°æ®åŠ è½½å™¨"]
        M4["å®ä¾‹åŒ– Trainer(cfg)<br/>åˆå§‹åŒ–è®­ç»ƒ/é‡‡æ ·æ§åˆ¶å™¨"]
        M1 --> M2 --> M3 --> M4
    end
    
    START --> MAIN
    
    %% ========== æ•°æ®é¢„å¤„ç†å±‚ ==========
    subgraph DATASET ["ğŸŸ¦ Utils/Data_utils/real_datasets.py - æ•°æ®é¢„å¤„ç†"]
        direction TB
        
        D1["load_csv(path)<br/>â†“<br/>np.loadtxt(delimiter=',', skiprows=1)<br/>è¾“å‡º: (N, 9)"]
        D2["minmax_scaler(data)<br/>â†“<br/>(x - min) / (max - min + 1e-7)<br/>è¾“å‡º: scaled(N,9), min(1,9), max(1,9)"]
        D3["create_windows(arr, 512, 'non_overlapping')<br/>â†“<br/>æ¯ 512 è¡Œåˆ‡ä¸€ä¸ªçª—å£<br/>è¾“å‡º: (W, 512, 9)"]
        D4["NILMDataset(windows)<br/>â†“<br/>torch.from_numpy().float()<br/>è¾“å‡º: PyTorch Dataset"]
        D5["DataLoader(dataset, batch_size=64)<br/>â†“<br/>è¾“å‡º batch: (B, L, 9)"]
        
        D1 --> D2 --> D3 --> D4 --> D5
    end
    
    MAIN --> DATASET
    
    %% ========== åˆ†æ”¯: è®­ç»ƒ vs é‡‡æ · ==========
    DATASET --> BRANCH{æ£€æŸ¥æ¨¡å¼}
    BRANCH -->|--train| TRAIN_PATH["è¿›å…¥è®­ç»ƒæµç¨‹ â†“"]
    BRANCH -->|--sample| SAMPLE_PATH["è¿›å…¥é‡‡æ ·æµç¨‹ â†“"]
    
    %% ========================================================
    %% è®­ç»ƒæµç¨‹
    %% ========================================================
    subgraph TRAINING ["ğŸŸ¨ è®­ç»ƒæµç¨‹å…¨æµç¨‹ (engine/solver.py + Models/)"]
        direction TB
        
        T0["Trainer.train(train_loader, test_loader)"]
        T1["ä» DataLoader è·å– batch<br/>shape: (B, L, 9) = (64, 512, 9)"]
        T2["éšæœºé‡‡æ · diffusion step<br/>t ~ Uniform(0, T-1)<br/>t: (B,) = (64,)"]
        
        subgraph SOLVER_TRAIN ["solver.py - è®­ç»ƒæ§åˆ¶"]
            T3["æå–æ¡ä»¶å‘é‡<br/>c = batch[..., 1:].mean(dim=1)<br/>è¾“å…¥: (B, L, 8)<br/>è¾“å‡º: (B, 8)"]
            T4["åŠ å™ª: q_sample(batch, t)<br/>è°ƒç”¨ â†“ gaussian_diffusion.py"]
        end
        
        %% åŠ å™ªè¿‡ç¨‹
        subgraph QSAMPLE ["gaussian_diffusion.py - q_sample()"]
            Q1["ç”Ÿæˆéšæœºå™ªå£°<br/>noise = torch.randn_like(x_start)<br/>shape: (B, L, 9)"]
            Q2["æå–æ‰©æ•£ç³»æ•°<br/>âˆšÎ±Ì…_t = extract(sqrt_alphas_cumprod, t, x_start.shape)"]
            Q3["æå–å™ªå£°ç³»æ•°<br/>âˆš(1-Î±Ì…_t) = extract(sqrt_one_minus_alphas_cumprod, t)"]
            Q4["åŠ å™ªå…¬å¼<br/>x_t = âˆšÎ±Ì…_t Â· x_start + âˆš(1-Î±Ì…_t) Â· noise<br/>è¾“å‡º: x_t (B, L, 9)"]
            Q1 --> Q2 --> Q3 --> Q4
        end
        
        T4 --> QSAMPLE
        
        QSAMPLE --> T5["å¾—åˆ° x_t (B, L, 9) å’Œ noise (B, L, 9)"]
        
        %% æ¨¡å‹å‰å‘
        T5 --> T6["è°ƒç”¨ Diffusion.forward(x_t, condition=c)"]
        
        subgraph DIFF_FORWARD ["gaussian_diffusion.py - Diffusion.forward()"]
            DF1["è¾“å…¥:<br/>â€¢ x: (B, L, 9) - å¸¦å™ªåºåˆ—<br/>â€¢ condition: (B, 8) - æ—¶é—´æ¡ä»¶"]
            DF2["éšæœºé‡‡æ ·æ—¶é—´æ­¥<br/>t = torch.randint(0, self.num_timesteps, (B,))"]
            DF3["æå–å™ªå£°<br/>noise = torch.randn_like(x)"]
            DF4["åŠ å™ª<br/>x_noisy = self.q_sample(x, t, noise)"]
            DF5["è°ƒç”¨æ ¸å¿ƒæ¨¡å‹<br/>predicted_noise = self.denoise_fn.forward(x_noisy, t, condition)"]
            
            DF1 --> DF2 --> DF3 --> DF4 --> DF5
        end
        
        T6 --> DIFF_FORWARD
        
        %% DiT æ¨¡å‹å‰å‘
        DIFF_FORWARD --> T7["self.denoise_fn = Transformer (agent_transformer.py)"]
        
        subgraph DIT_TRAIN ["agent_transformer.py - Transformer.forward()"]
            direction TB
            
            DIT_IN["è¾“å…¥:<br/>â€¢ x: (B, L, 9)<br/>â€¢ time: (B,)<br/>â€¢ cond: (B, 8)"]
            
            %% æ—¶é—´æ­¥åµŒå…¥
            subgraph TIME_EMB ["model_utils.py - SinusoidalPosEmb"]
                TE1["t (B,) â†’ Sinusoidal ç¼–ç "]
                TE2["half_dim = hidden_dim // 2"]
                TE3["emb = [sin(tÂ·Ï‰), cos(tÂ·Ï‰)]"]
                TE4["MLP: Linear â†’ SiLU â†’ Linear"]
                TE5["è¾“å‡º: time_emb (B, hidden_dim)"]
                TE1 --> TE2 --> TE3 --> TE4 --> TE5
            end
            
            %% æ¡ä»¶åµŒå…¥
            COND_EMB["æ¡ä»¶æŠ•å½±<br/>cond_emb = Linear(8 â†’ hidden_dim)(cond)<br/>è¾“å‡º: (B, hidden_dim)"]
            
            %% èåˆ
            MERGE["èåˆæ¡ä»¶<br/>cond_total = time_emb + cond_emb<br/>shape: (B, hidden_dim)"]
            
            %% è¾“å…¥æŠ•å½±
            PROJ["è¾“å…¥æŠ•å½±<br/>x = Linear(9 â†’ hidden_dim)(x)<br/>è¾“å‡º: (B, L, hidden_dim)"]
            
            DIT_IN --> TIME_EMB
            DIT_IN --> COND_EMB
            TIME_EMB --> MERGE
            COND_EMB --> MERGE
            DIT_IN --> PROJ
            
            %% DiT Blockå¾ªç¯
            MERGE --> BLOCK_LOOP
            PROJ --> BLOCK_LOOP
            BLOCK_LOOP["å¯¹æ¯ä¸ª DiT Block (i=1...num_layers):"]
            
            %% å•ä¸ª DiT Block
            subgraph DITBLOCK ["DiTBlock (agent_transformer.py)"]
                direction TB
                
                BLK_IN["è¾“å…¥: x(B,L,hidden_dim), cond(B,hidden_dim)"]
                
                %% Modulation Network
                subgraph MODULATION ["AdaLN-Zero Modulation Network"]
                    MOD1["cond â†’ SiLU()"]
                    MOD2["Linear(hidden_dim â†’ 6Ã—hidden_dim)<br/>âš ï¸ Zero-Init æƒé‡"]
                    MOD3["Split æˆ 6 ä»½:<br/>shift_msa, scale_msa, gate_msa<br/>shift_mlp, scale_mlp, gate_mlp<br/>æ¯ä»½: (B, hidden_dim)"]
                    MOD1 --> MOD2 --> MOD3
                end
                
                %% MSA è·¯å¾„
                subgraph MSA ["è·¯å¾„1: Multi-Head Self-Attention"]
                    MSA1["LayerNorm(x, affine=False)<br/>x_norm = (x - Î¼) / Ïƒ"]
                    MSA2["AdaLN è°ƒåˆ¶<br/>x_mod = x_norm Â· (1 + scale_msa.unsqueeze(1))<br/>       + shift_msa.unsqueeze(1)"]
                    MSA3["Multi-Head Attention<br/>attn_out = Attention(x_mod, x_mod, x_mod)<br/>num_heads=8"]
                    MSA4["Gate æ§åˆ¶<br/>gated = gate_msa.unsqueeze(1) Â· attn_out"]
                    MSA5["Residual<br/>x = x + gated"]
                    MSA1 --> MSA2 --> MSA3 --> MSA4 --> MSA5
                end
                
                %% MLP è·¯å¾„
                subgraph MLP ["è·¯å¾„2: Feed-Forward MLP"]
                    MLP1["LayerNorm(x, affine=False)"]
                    MLP2["AdaLN è°ƒåˆ¶<br/>x_mod = x_norm Â· (1 + scale_mlp.unsqueeze(1))<br/>       + shift_mlp.unsqueeze(1)"]
                    MLP3["FFN<br/>Linear(hidden_dim â†’ hidden_dimÃ—4)<br/>â†’ GELU<br/>â†’ Linear(hidden_dimÃ—4 â†’ hidden_dim)"]
                    MLP4["Gate æ§åˆ¶<br/>gated = gate_mlp.unsqueeze(1) Â· mlp_out"]
                    MLP5["Residual<br/>x = x + gated"]
                    MLP1 --> MLP2 --> MLP3 --> MLP4 --> MLP5
                end
                
                BLK_IN --> MODULATION
                BLK_IN --> MSA
                MSA --> MLP
                MODULATION -.->|æä¾›è°ƒåˆ¶å‚æ•°| MSA2
                MODULATION -.->|æä¾›è°ƒåˆ¶å‚æ•°| MSA4
                MODULATION -.->|æä¾›è°ƒåˆ¶å‚æ•°| MLP2
                MODULATION -.->|æä¾›è°ƒåˆ¶å‚æ•°| MLP4
                
                MLP --> BLK_OUT["è¾“å‡º: x (B, L, hidden_dim)"]
            end
            
            BLOCK_LOOP --> DITBLOCK
            DITBLOCK --> NEXT{è¿˜æœ‰ä¸‹ä¸€ä¸ªBlock?}
            NEXT -->|æ˜¯| DITBLOCK
            NEXT -->|å¦| FINAL
            
            %% æœ€ç»ˆè¾“å‡º
            subgraph FINAL ["æœ€ç»ˆè¾“å‡ºå±‚"]
                FIN1["Final LayerNorm(x)"]
                FIN2["Output Projection<br/>Linear(hidden_dim â†’ 9)"]
                FIN3["è¾“å‡º: predicted_noise (B, L, 9)"]
                FIN1 --> FIN2 --> FIN3
            end
        end
        
        T7 --> DIT_TRAIN
        
        DIT_TRAIN --> T8["è¿”å› predicted_noise (B, L, 9)"]
        
        %% Loss è®¡ç®—
        T8 --> T9["è®¡ç®— Loss<br/>loss_fn = MSE / Huber<br/>loss = ((predicted_noise - noise)Â²).mean()"]
        T9 --> T10["åå‘ä¼ æ’­<br/>loss.backward()"]
        T10 --> T11["optimizer.step()<br/>æ›´æ–°æ¨¡å‹å‚æ•°"]
        T11 --> T12["æ¯ log_interval æ‰“å° loss<br/>æ¯ save_interval ä¿å­˜ checkpoint"]
        
        T0 --> T1 --> T2 --> SOLVER_TRAIN --> T5 --> T6 --> T8 --> T9
    end
    
    TRAIN_PATH --> TRAINING
    
    %% ========================================================
    %% é‡‡æ ·æµç¨‹
    %% ========================================================
    subgraph SAMPLING ["ğŸŸ© é‡‡æ ·æµç¨‹å…¨æµç¨‹ (engine/solver.py + Models/)"]
        direction TB
        
        S0["Trainer.sample(sample_num=N)"]
        S1["åˆå§‹åŒ–çº¯å™ªå£°<br/>x = torch.randn(N, L, 9)<br/>x_T ~ N(0, I)"]
        S2["æ„é€ ç›®æ ‡æ—¶é—´æ¡ä»¶<br/>c = build_condition(target_time)<br/>å°† '2022-01-01 12:00' è½¬ä¸º 8ç»´ sin/cos<br/>c: (N, 8)"]
        
        S3["å¼€å§‹é€†æ‰©æ•£å¾ªç¯<br/>for t in range(T-1, -1, -1):"]
        
        subgraph DENOISE_LOOP ["é€†æ‰©æ•£å¾ªç¯ (T-1 â†’ 0)"]
            direction TB
            
            DL1["å½“å‰æ­¥æ•°: t<br/>t_tensor = torch.full((N,), t)"]
            DL2["è°ƒç”¨æ¨¡å‹é¢„æµ‹å™ªå£°<br/>predicted_noise = Diffusion.p_sample(x_t, t, c)"]
            
            %% p_sample è¯¦ç»†
            subgraph PSAMPLE ["gaussian_diffusion.py - p_sample()"]
                PS1["è°ƒç”¨ model_predictions(x_t, t)<br/>â†“<br/>Transformer.forward(x_t, t, c)<br/>è¿”å› predicted_noise (N, L, 9)"]
                PS2["é¢„æµ‹ x_0<br/>x_start = predict_start_from_noise(x_t, t, noise)<br/>å…¬å¼: x_0 = (x_t - âˆš(1-Î±Ì…_t)Â·Îµ) / âˆšÎ±Ì…_t"]
                PS3["è®¡ç®—åéªŒå‡å€¼å’Œæ–¹å·®<br/>Î¼_t, Ïƒ_t = q_posterior(x_start, x_t, t)"]
                PS4["é‡‡æ ·ä¸‹ä¸€æ­¥<br/>if t > 0:<br/>  noise = torch.randn_like(x_t)<br/>  x_{t-1} = Î¼_t + Ïƒ_t Â· noise<br/>else:<br/>  x_{t-1} = Î¼_t"]
                
                PS1 --> PS2 --> PS3 --> PS4
            end
            
            DL2 --> PSAMPLE
            PSAMPLE --> DL3["æ›´æ–° x â† x_{t-1}<br/>shape: (N, L, 9)"]
            DL3 --> DL4{t > 0?}
            DL4 -->|æ˜¯| DL1
            DL4 -->|å¦| DL5["å¾ªç¯ç»“æŸ<br/>å¾—åˆ° x_0 (N, L, 9)"]
            
            DL1 --> DL2
        end
        
        S3 --> DENOISE_LOOP
        
        DENOISE_LOOP --> S4["åå½’ä¸€åŒ–<br/>åŠ è½½ min_val, max_val from checkpoint<br/>x_real = x_0 Â· (max - min) + min<br/>è¾“å‡º: (N, L, 9) - çœŸå®åŠŸç‡å€¼"]
        S4 --> S5["ä¿å­˜ä¸º .npy æ–‡ä»¶<br/>for i in range(N):<br/>  np.save(f'sample_{i}.npy', x_real[i].cpu().numpy())"]
        S5 --> S6["âœ… é‡‡æ ·å®Œæˆ<br/>ç”Ÿæˆäº† N ä¸ªåˆæˆçª—å£"]
        
        S0 --> S1 --> S2 --> S3 --> S4
    end
    
    SAMPLE_PATH --> SAMPLING
    
    %% ========== æ ·å¼ ==========
    style START fill:#ff6b6b
    style MAIN fill:#ffe4b5,stroke:#ff8c00,stroke-width:3px
    style DATASET fill:#e6f3ff,stroke:#0066cc,stroke-width:3px
    style TRAINING fill:#fff9e6,stroke:#ccaa00,stroke-width:3px
    style SAMPLING fill:#e6ffe6,stroke:#00aa00,stroke-width:3px
    style DITBLOCK fill:#f3e5f5,stroke:#9c27b0,stroke-width:3px
    style MODULATION fill:#ede7f6,stroke:#673ab7,stroke-width:2px
    style MSA fill:#e8eaf6,stroke:#3f51b5,stroke-width:2px
    style MLP fill:#e8eaf6,stroke:#3f51b5,stroke-width:2px
    style PSAMPLE fill:#c8e6c9,stroke:#388e3c,stroke-width:2px
```

---

## ğŸ“Š è¶…è¯¦ç»†æ•°æ®æ ¼å¼å¯¹ç…§è¡¨

### é¢„å¤„ç†é˜¶æ®µ (real_datasets.py)

| æ­¥éª¤ | å‡½æ•° | è¾“å…¥ | è¾“å‡º | Shape |
|------|------|------|------|-------|
| 1 | `load_csv` | CSV æ–‡ä»¶è·¯å¾„ | `np.ndarray` | (N, 9) |
| 2 | `minmax_scaler` | (N, 9) | `scaled, min, max` | (N,9), (1,9), (1,9) |
| 3 | `create_windows` | (N, 9) | `windows` | (W, 512, 9) |
| 4 | `NILMDataset` | (W, 512, 9) | `torch.Tensor` | (W, 512, 9) |
| 5 | `DataLoader` | Dataset | `batch` | (64, 512, 9) |

### è®­ç»ƒé˜¶æ®µ (solver.py + Models/)

| æ­¥éª¤ | æ–‡ä»¶ | å‡½æ•°/ç±» | è¾“å…¥ | è¾“å‡º | Shape |
|------|------|---------|------|------|-------|
| 1 | `solver.py` | æå–æ¡ä»¶ | `batch` | `c` | (B, 8) |
| 2 | `solver.py` | éšæœºæ­¥æ•° | `B` | `t` | (B,) |
| 3 | `gaussian_diffusion.py` | `q_sample` | `x_start, t` | `x_t, noise` | (B,L,9), (B,L,9) |
| 4 | `model_utils.py` | `SinusoidalPosEmb` | `t (B,)` | `time_emb` | (B, hidden_dim) |
| 5 | `agent_transformer.py` | `Linear` | `c (B,8)` | `cond_emb` | (B, hidden_dim) |
| 6 | `agent_transformer.py` | èåˆ | `time_emb + cond_emb` | `cond_total` | (B, hidden_dim) |
| 7 | `agent_transformer.py` | `Linear` | `x (B,L,9)` | `x_proj` | (B, L, hidden_dim) |
| 8 | `agent_transformer.py` | `DiTBlock` | `x, cond` | `x` | (B, L, hidden_dim) |
| 9 | `agent_transformer.py` | Modulation | `cond` | `6 ä»½å‚æ•°` | æ¯ä»½ (B, hidden_dim) |
| 10 | `agent_transformer.py` | LayerNorm | `x` | `x_norm` | (B, L, hidden_dim) |
| 11 | `agent_transformer.py` | AdaLN | `x_norm, scale, shift` | `x_mod` | (B, L, hidden_dim) |
| 12 | `agent_transformer.py` | Attention | `x_mod` | `attn_out` | (B, L, hidden_dim) |
| 13 | `agent_transformer.py` | Gate+Res | `x + gateÂ·attn_out` | `x` | (B, L, hidden_dim) |
| 14 | `agent_transformer.py` | Output | `x` | `predicted_noise` | (B, L, 9) |
| 15 | `solver.py` | MSE Loss | `pred, noise` | `loss` | scalar |

### é‡‡æ ·é˜¶æ®µ (solver.py + Models/)

| æ­¥éª¤ | æ–‡ä»¶ | å‡½æ•° | è¾“å…¥ | è¾“å‡º | Shape |
|------|------|------|------|------|-------|
| 1 | `solver.py` | `torch.randn` | `(N, L, 9)` | `x_T` | (N, 512, 9) |
| 2 | `solver.py` | `build_condition` | `target_time` | `c` | (N, 8) |
| 3 | `gaussian_diffusion.py` | `p_sample` | `x_t, t, c` | `x_{t-1}` | (N, L, 9) |
| 4 | `agent_transformer.py` | `forward` | `x_t, t, c` | `pred_noise` | (N, L, 9) |
| 5 | `gaussian_diffusion.py` | `predict_start_from_noise` | `x_t, t, noise` | `x_0_pred` | (N, L, 9) |
| 6 | `gaussian_diffusion.py` | `q_posterior` | `x_0, x_t, t` | `Î¼_t, Ïƒ_t` | (N,L,9), (N,L,9) |
| 7 | `solver.py` | åå½’ä¸€åŒ– | `x_0, min, max` | `x_real` | (N, L, 9) |
| 8 | `solver.py` | `np.save` | `x_real[i]` | `sample_i.npy` | (512, 9) |

---

## ğŸ”‘ å…³é”®å…¬å¼è¯´æ˜

### 1. æ‰©æ•£å‰å‘è¿‡ç¨‹ (åŠ å™ª)
```
q(x_t | x_0) = N(x_t; âˆšÎ±Ì…_t Â· x_0, (1 - Î±Ì…_t) Â· I)

å®ç°:
x_t = âˆšÎ±Ì…_t Â· x_0 + âˆš(1-Î±Ì…_t) Â· Îµ,  Îµ ~ N(0, I)
```
**ä»£ç ä½ç½®**: `gaussian_diffusion.py` â†’ `q_sample()`

### 2. é€†æ‰©æ•£è¿‡ç¨‹ (å»å™ª)
```
p_Î¸(x_{t-1} | x_t) = N(x_{t-1}; Î¼_Î¸(x_t, t), Î£_Î¸(x_t, t))

å…¶ä¸­:
x_0_pred = (x_t - âˆš(1-Î±Ì…_t) Â· Îµ_Î¸(x_t, t)) / âˆšÎ±Ì…_t
Î¼_t = (1/âˆšÎ±_t) Â· (x_t - (Î²_t/âˆš(1-Î±Ì…_t)) Â· Îµ_Î¸)
Ïƒ_t = âˆšÎ²_t
```
**ä»£ç ä½ç½®**: `gaussian_diffusion.py` â†’ `p_sample()`, `predict_start_from_noise()`

### 3. AdaLN-Zero è°ƒåˆ¶
```
AdaLN(x, c) = LayerNorm(x) Â· (1 + scale(c)) + shift(c)

å…¶ä¸­:
scale(c), shift(c), gate(c) = MLP(c).split(3)
è¾“å‡º = x + gate(c) Â· Transformation(AdaLN(x, c))
```
**ä»£ç ä½ç½®**: `agent_transformer.py` â†’ `DiTBlock`

### 4. è®­ç»ƒæŸå¤±å‡½æ•°
```
L = E_{x_0, Îµ, t} [ ||Îµ - Îµ_Î¸(x_t, t, c)||Â²]

å…¶ä¸­:
Îµ_Î¸ æ˜¯æ¨¡å‹é¢„æµ‹çš„å™ªå£°
Îµ æ˜¯çœŸå®æ·»åŠ çš„å™ªå£°
```
**ä»£ç ä½ç½®**: `gaussian_diffusion.py` â†’ `_train_loss()`

---

## ğŸ¯ AdaLN-Zero çš„ 4 ä¸ªå…³é”®ä½œç”¨ç‚¹

| ä½ç½® | ä½œç”¨ | è¾“å…¥ | è¾“å‡º |
|------|------|------|------|
| **1. Modulation Network** | ç”Ÿæˆè°ƒåˆ¶å‚æ•° | `cond (B, hidden_dim)` | `6 ä¸ªå‚æ•°ï¼Œæ¯ä¸ª (B, hidden_dim)` |
| **2. MSA å‰çš„ Scale & Shift** | è°ƒåˆ¶å½’ä¸€åŒ–ç‰¹å¾ | `x_norm, scale, shift` | `x_mod (B, L, hidden_dim)` |
| **3. MSA åçš„ Gate** | æ§åˆ¶æ³¨æ„åŠ›æµ | `attn_out, gate` | `gated_attn` |
| **4. MLP è·¯å¾„** | åŒä¸Šï¼Œåº”ç”¨äº FFN | `mlp_out, gate` | `gated_mlp` |

### ä¸ºä»€ä¹ˆè¦ Zero-Initï¼Ÿ

```python
# åœ¨ agent_transformer.py çš„ __init__ ä¸­
nn.init.zeros_(self.modulation[-1].weight)
nn.init.zeros_(self.modulation[-1].bias)
```

**åŸå› **ï¼š
1. åˆå§‹æ—¶ `scale=0, shift=0, gate=0`
2. AdaLN é€€åŒ–ä¸ºæ™®é€š LayerNorm + Residual
3. æ¨¡å‹è®­ç»ƒç¨³å®šï¼Œä¸å—æœªè®­ç»ƒçš„æ¡ä»¶å¹²æ‰°
4. éšè®­ç»ƒè¿›è¡Œï¼Œæ¨¡å‹é€æ¸å­¦ä¼šä½¿ç”¨æ¡ä»¶ä¿¡æ¯

---

## ğŸš€ å¦‚ä½•ä½¿ç”¨

1. **åœ¨ VSCode ä¸­æ‰“å¼€** `detailed_flow.md`
2. **å®‰è£… Mermaid Viewer æ‰©å±•**
3. **ç‚¹å‡»é¢„è§ˆå›¾æ ‡** æˆ– `Ctrl+Shift+P` â†’ "Mermaid Viewer: Open Preview"
4. **å¯¼å‡ºå›¾ç‰‡**: åœ¨é¢„è§ˆçª—å£å·¥å…·æ é€‰æ‹© SVG/PNG/JPG

### ä¸»é¢˜å»ºè®®
- æ¨èä½¿ç”¨ **dark** æˆ– **forest** ä¸»é¢˜æŸ¥çœ‹
- å¯å‹¾é€‰ "Sync with VSCode theme" è‡ªåŠ¨åŒ¹é…

---

## ğŸ“ å°ç»“

è¿™ä¸ªæµç¨‹å›¾åŒ…å«äº†ï¼š
- âœ… **å®Œæ•´çš„è®­ç»ƒæµç¨‹** (ä» CSV â†’ æ¨¡å‹ â†’ Loss)
- âœ… **å®Œæ•´çš„é‡‡æ ·æµç¨‹** (ä»å™ªå£° â†’ å»å™ª â†’ ä¿å­˜)
- âœ… **æ‰€æœ‰ Models/ ç›®å½•ä¸‹çš„æ¨¡å—** (gaussian_diffusion, agent_transformer, model_utils)
- âœ… **æ¯ä¸€æ­¥çš„æ•°æ® Shape æ ‡æ³¨**
- âœ… **AdaLN-Zero çš„è¯¦ç»†å®ç°**
- âœ… **å…³é”®å…¬å¼å’Œä»£ç ä½ç½®å¯¹ç…§**

ç°åœ¨ä½ å¯ä»¥æ¸…æ¥šåœ°çœ‹åˆ°ï¼š
- æ•°æ®åœ¨æ¯ä¸ªæ¨¡å—ä¸­å¦‚ä½•æµåŠ¨
- æ¯ä¸ªå¼ é‡çš„ç»´åº¦å¦‚ä½•å˜åŒ–
- AdaLN-Zero åœ¨å“ªé‡Œèµ·ä½œç”¨
- è®­ç»ƒå’Œé‡‡æ ·çš„å®Œæ•´åŒºåˆ«
