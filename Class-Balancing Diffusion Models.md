

Class-Balancing Diffusion Models
## Yiming Qin
## 1
## Huangjie Zheng
## 2
## Jiangchao Yao
## 1,3
## Mingyuan Zhou
## 2
## Ya Zhang
## 1,3
## 1
Cooperative Medianet Innovation Center, Shanghai Jiao-Tong University
## 2
University of Texas, Austin
## 3
Shanghai AI Laboratory
## Abstract
Diffusion-based models have shown the merits of gener-
ating high-quality visual data while preserving better diver-
sity in recent studies. However, such observation is only jus-
tified with curated data distribution, where the data samples
are nicely pre-processed to be uniformly distributed in terms
of their labels.  In practice, a long-tailed data distribution
appears more common and how diffusion models perform
on such class-imbalanced data remains unknown.   In this
work, we first investigate this problem and observe signifi-
cant degradation in both diversity and fidelity when the dif-
fusion model is trained on datasets with class-imbalanced
distributions.    Especially  in  tail  classes,  the  generations
largely lose diversity and we observe severe mode-collapse
issues.  To tackle this problem, we set from the hypothesis
that  the  data  distribution  is  not  class-balanced,  and  pro-
pose Class-Balancing Diffusion Models (CBDM) that are
trained with a distribution adjustment regularizer as a so-
lution. Experiments show that images generated by CBDM
exhibit higher diversity and quality in both quantitative and
qualitative ways.  Our method benchmarked the generation
results on CIFAR100/CIFAR100LT dataset and shows out-
standing performance on the downstream recognition task.
## 1. Introduction
In  recent  years,  log-likelihood-based  diffusion  models
have evolved rapidly and established new benchmarks on a
range of generation tasks [1, 7]. Based on them, researchers
have been able to further control the model generation pro-
cess and the generation quality.  This improves the applica-
tions of generative models in numerous domains including
text-image generation [31], image editing [28, 38], speech
synthesis [17], medical imaging [26, 44], video generation
[13] and adversarial learning [18, 33], etc.
Although diffusion models are known for the power of
high fidelity and diversity in generation, most of the exist-
ing diffusion models are trained with the hypothesis that the
data are uniformly distributedw.r.t.their labels.  However,
Figure 1.   Generation degrades along with class frequency.   Se-
mantics of generated images become less recognizable when class
frequency decreases, while the FID score increases significantly.
in the real world, the distribution is often very skewed. Es-
pecially for many domain-specific generation tasks such as
medical images [14], fine-grained dataset for taxology [15]
and data grabbed from the web [24],  it is difficult to col-
lect large amounts of data for each class equally,  and the
size of the training set for head and tail categories can differ
by a factor of hundred or more.  For such datasets, uncon-
ditional diffusion models tend to produce a significant por-
tion of low-quality images.  Conditional models, as shown
in Figure 1, generate head class images with satisfying per-
formance,  while  conversely  the  generated  images  on  tail
classes are very likely to show unrecognizable semantics.
Concerning  training  generative  models  with  limited  data,
there  already  exist  several  methods  [21, 37, 50]  based  on
GAN models [2].  However, quite few studies examine the
impact of imbalanced class distribution [34] especially on
diffusion models, which is practical yet under-explored.
Our work first introduces diffusion models to imbalance
generation  tasks  on  several  long-tailed  datasets  [19],  and
then build some straightforward baselines according to the
common methods used in long-tailed recognition [27, 29].
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
## 18434

To  overcome  the  potential  degeneration  induced  by  the
skewed  distribution,  we  propose  a  novel  Class-Balancing
Diffusion Model (CBDM). Theoretically, CBDM resorts to
adjusting  the  conditional  transfer  probability  during  sam-
pling in order to implicitly force generated images to have
a  balanced  prior  distribution  during  every  sampling  step.
Technically, the adjusted transfer probability of CBDM re-
sults in an additional MSE-form loss for a conditional dif-
fusion model, which functions as a regularizer.  Intuitively,
this loss augments the similarity of generated images con-
ditioned on different classes, and turns out to be an effective
approach to transfer common information from head classes
to  tail  classes  without  hurting  the  model’s  expressiveness
on head classes. CBDM can be implemented within several
flines  of  codes,  and  its  lighter  version  admits  fine-tuning
an existing conditional model. We conducted extensive ex-
periments on CIFAR10/CIFAR100 and their corresponding
long-tailed dataset to show the promise of CBDM over ex-
isting state-of-the-art methods.  In a nutshell, the contribu-
tions of this work can be summarized as follows:
-  We identify the severe degeneration problem of diffu-
sion models in long-tailed generation tasks and bench-
mark some straightforward baselines in this direction.
-  We propose a new perspective to handle the genera-
tion quality collapse on tail classes, and derive a novel
Class-Balancing  Diffusion  Model,  which  is  effective
and lightweight as a regularizer to existing methods.
-  We validate that CBDM is capable of generating more
diverse images with convincing fidelity, especially for
datasets with large number of categories.  In addition,
CBDM is robust with accelerating algorithms such as
DDIM [43], and can be transplanted to different con-
ditional diffusion-based backbones easily.
## 2. Related Works
Diffusion modelsDiffusion models are recently proposed
generative models [42] based on non-equilibrium thermo-
dynamics.   Conditional diffusion models [7] encode label
information into the generation process and improve largely
the  generation  performance.   The  guidance  structure  pro-
posed in [7] makes it possible to control the generation pro-
cess through an external module.  Based on a similar intu-
ition, researchers arrive to realize diverse functions, such as
guided adversarial purification [33], few-shot generation [9]
and  so  on  [36, 41].   The  drawback  of  classifier  guidance
(noted as CG) [7] lies in its requirement of training another
auxiliary  classifier.To  address  the  issue,  classifier-free
guidance (noted as CFG) [12] proposed a mechanism that
uses the generator itself to express the class guidance in-
formation. CFG is proved to be a resource efficient method
and achieves outstanding performance on large models [31].
Moreover,  CFG only requires to add one line in training,
which can be easily transplanted on different models.
Long-tail  recognitionThe  problem  of  long-tailed  dis-
tribution  is  a  common  dilemma  in  machine  learning  and
have  been  widely  explored  in  the  area  of  discriminative
models,i.e.,long-tailed  recognition.   There  mainly  exist
three paradigms in this domain, namely Class Re-balancing
[27, 29], Information Augmentation [25, 47, 49] and Module
Improvement  [16, 32].   Among  them,  Class  Re-balancing
provides the best explainability, and its most common prac-
tice is re-sampling [27].  Thereafter, stemming from mod-
ifying the objective function from the global error rate to
the class average one, [29] propose logit adjustment which
has  shown  an  impressive  performance.Another  effec-
tive method is based on Information Augmentation, which
uses head class feature information to augment tail classes
[5, 25, 47, 49].   However,  discriminative models map data
from higher to lower dimensions, while generative models
map  images  from  lower  to  higher  dimensions.   Thus,  the
mechanism of class rebalancing between them may be com-
pletely different and how to design the balancing method
remains under-explored.
Generative models based on limited dataGAN [2, 6] is
the most dominant model in the field of image synthesis in
recent years.  Given the requirement of large-scale data to
train the generative models, a part of researchers focus on
improving the performance of GAN models under the small
datasets.   To address the overfitting issue of the discrimi-
nator, a number of regularization methods [22, 37, 45] has
been  proposed.   An  alternative  solution  is  Data  Augmen-
tation.  As augmentation information is very prone to leak
to the generator,  researchers proposed improved augmen-
tation strategies such as Differentiable Augmentation (Dif-
fAug) [50] and Adaptive Augmentation (ADA) [21]. These
augmentation ways can be transferred to diffusion models
as baselines to help the generation of tail-class samples.
## 3. Method
In this section, we first give the basic notations following
the classical DDPM model,  and then introduce the class-
imbalanced generation setting.  In the third part, we intro-
duce our CBDM algorithm and present its training details
including implementation and hyper-parameter settings.
## 3.1. Preliminary
Diffusion models leverage a pre-defined forward process
in training, where a clean image distributionq(x
## 0
)can be
corrupted  to  a  noisy  distributionq(x
t
## |x
## 0
)at  a  specified
timestept.  Given a pre-defined variance schedule{β
t
## }
## 1:T
## ,
## 18435

(a) Principle overview of CBDM
(b) DDPM(top)/CBDM(bottom) comparison
when denoising a same noised image in class
70/86.    We  use  both  generators  to  recover
a  noisy  image  of  two  classes  and  observe
that CBDM is able to produce more diversity
based on the same starting point.
Figure 2. Algorithm (left) and generation (right) visualization. In theleftfigure, we show that an extra regularization lossgL
r
proportional
to the diffusion steptis added during training. This loss function pushes the sampling distribution (curves on the surface) to wider region
while preventing it to be excessively distorted compared to the ground truth distribution (gradient color on the background).
the noisy distribution at any intermediate timestep is
q(x
t
## |x
## 0
## ) =N(
## √
## ̄α
t
x
## 0
## ,(1− ̄α
t
)I);     ̄α
t
## =
t
## Y
i=1
## (1−β
i
## ).
To  reverse  such  forward  process,  a  generative  modelθ
learns to estimate the analytical true posterior in order to
recoverx
t−1
fromx
t
as follows:
min
θ
## D
## KL
## [q(x
t−1
## |x
t
## ,x
## 0
## )||p
θ
## (x
t−1
## |x
t
)];∀t∈{1,...,T},
and such an objective can be reduced to a simple denoising
estimation loss [11]:
## L
## DDPM
## =E
t,x
## 0
## ∼q(x
## 0
),ε∼N(0,I)
## 
## ∥ε−ε
θ
## (
## √
## ̄αx
## 0
## +
## √
## 1− ̄α
t
ε,t)∥
## 2
## 
## (1)
For the case where label information is available, the model
is  trained  to  estimate  the  noise  as  above  in  both  condi-
tional  casesε
θ
## (x
t
,y,t)with  data-label  pairs(x
## 0
## ,y)and
unconditional  caseε
θ
## (x
t
,t).   In  the  sampling,  the  label-
guided model estimates the noise with a linear interpolation
## ˆ
ε= (1 +ω)ε
θ
## (x
t
## ,y,t)−ωε
θ
## (x
t
,t)to recoverx
t−1
, which
is often referred as Classifier-Free Guidance (CFG) [12].
3.2. Class-Balancing Diffusion Models
Current diffusion models assume the data distribution to
be uniform in class,  and thus equally treat samples in the
training  stage.   However,  based  on  our  observation,  such
training  strategy  leads  to  degradation  in  generation  qual-
ity. Below, we provide an analysis that motivates our Class-
Balancing Diffusion Models (CBDM).
Supposeq(x,y)is the data distribution that we need to
match  with  the  joint  distributionp
θ
(x,y)predicted  by  a
generative model.We analyze their difference from the
density  ratior=
q(x,y)
p
θ
## (x,y)
## =
q(x|y)
p
θ
## (x|y)
## ·
q(y)
p
θ
## (y)
.   When  the
true label distributionq(y)is the same as the priorp
θ
## (y),
which  is  usually  assumed  to  be  uniform,  the  density  ra-
tioris reduced the conditional term to learn a conditional
modelp
θ
(x|y).  However, when such a hypothesis is vio-
lated, for head classes,
q(y)
p
θ
## (y)
would result in a larger weight
that makes the model biased and hurt tail classes, and vice
versa. Empirically, we observe that the generation degrades
more on tail classes, as illustrated in Figure 1.  Moreover,
as shown in Figure 2a, compared to head classes, DDPM
cannot well capture the tail-class data distribution and the
mode is poorly covered during the sampling process.   As
a result, generations of tail classes often have poor quality
and diversity, shown in Figure 2b.
To tackle this issue, the most intuitive approach lies in
adjusting  the  prior  label  distribution  through  a  class  bal-
anced re-sampling. However, such abrupt adjustment easily
leads to negative improvement in experiments. The step-by-
step sampling nature of diffusion models provides another
aspect to adjust this distribution more softly.  In this spirit,
we  propose  to  calibrate  the  learning  process  through  the
conditional transfer probabilityp
θ
## (x
t−1
## |x
t
,y)when there
exists a gap between the class distribution and the prior.
## Letp
## ⋆
θ
## (x
t−1
## |x
t
,y)be the optimum trained in the case
that
q(y)
p
θ
## (y)
is correctly estimated, andp
θ
## (x
t−1
## |x
t
,y)be the
one  trained  in  a  class-imbalanced  case.   The  relation  be-
tween such two generative distributions can be described as
the following proposition.
Proposition 1.When training a diffusion model parameter-
ized withθon a class-imbalanced dataset, its conditional
reverse distributionp
θ
## (x
t−1
## |x
t
,y)can be corrected with
an adjustment schema:
p
## ⋆
θ
## (x
t−1
## |x
t
## ,y) =p
θ
## (x
t−1
## |x
t
## ,y)
p
θ
## (x
t−1
## )
p
## ⋆
θ
## (x
t−1
## )
q
## ⋆
## (x
t
## )
q(x
t
## )
## (2)
The  proposition  above  shows  that,  when  trained  on  a
## 18436

class-imbalanced  dataset,  a  diffusion  model  can  still  ap-
proach the true data distribution by applying a distribution
adjustment schema
p
θ
## (x
t−1
## )
p
## ⋆
θ
## (x
t−1
## )
q
## ⋆
## (x
t
## )
q(x
t
## )
at every reverse stept.
However, approximating this schema is not feasible at ev-
ery sampling step, so CBDM incorporates it into the train-
ing  loss  function  to  achieve  an  equivalent  objective,  and
thus gets rid of the model-free part
q
## ⋆
## (x
t
## )
q(x
t
## )
.  By further de-
composingp
θ
## (x
t−1
## )andp
## ⋆
θ
## (x
t−1
)to the expectation of the
conditional probabilityp
## ⋆
θ
## (x
t−1
## |x
t:T
,y), we present a up-
per bound to approximate this probability in Proposition 2.
Proposition 2.For the adjusted lossL
## ⋆
## DM
## =
## P
## T
t=1
## L
## ⋆
t−1
## ,
an upper-bound of the target training objective to calibrate
at timestept(i.e.L
## ⋆
t−1
) can be derived as:
## X
t≥1
## L
## ⋆
t−1
## =
## X
t≥1
## D
## KL
## [q(x
t−1
## |x
t
## ,x
## 0
## )||p
## ⋆
θ
## (x
t−1
## |x
t
## ,y)]
## ≤
## X
t≥1
## [D
## KL
## [q(x
t−1
## |x
t
## ,x
## 0
## )||p
θ
## (x
t−1
## |x
t
## ,y)]
## |
## {z}
Diffusion model lossL
## DM
+tE
y
## ′
## ∼q
## ⋆
## Y
## [D
## KL
## [p
θ
## (x
t−1
## |x
t
## )||p
θ
## (x
t−1
## |x
t
## ,y
## ′
## )]
## |{z}
Distribution adjustment lossL
r
## ]],
The upper bound in the above proposition can be con-
sidered as two parts.  The first termL
## DM
corresponds to an
ordinary DDPM loss [11]e.g.,Eqn. (1), and the second loss
## L
r
is used to adjust the distribution as a regularization term.
Roughly speaking,L
r
increases the similarity between the
model’s output and a random target class.  Thus, it reduces
the risk of overfitting on the head classes, and enlarges the
generation  diversity  for  tail  class  through  knowledge  ob-
tained from other classes.  Whenq
## ⋆
## Y
is less longtailed than
the dataset, this loss also increases the probability for un-
derrepresented tail samples to be chosen during training.
3.3. Training algorithm
The detailed training algorithm of CBDM is presented
in  Alg.  (1).   In  the  algorithm,  we  reduce  the  distribution
adjustment lossL
r
as a square error loss with Monte-Carlo
samples as indicated by Eqn.(3), whereYis a set of samples
that drawn following the distributionq
## ⋆
## Y
andydenotes the
image label. Note for CFG [12], there is a fixed probability
(usually 10%) to drop the condition,i.e.,y= None.
## L
r
## (x
t
## ,y,t) =
## 1
## |Y|
## X
y
## ′
## ∈Y
## [t||ε
θ
## (x
t
## ,y)−ε
θ
## (x
t
## ,y
## ′
## )||
## 2
## ],(3)
For  the  implementation  in  practice,   CBDM  can  be
plugged into any existing conditional diffusion models by
adopting their model architecture and adjusting the training
lossL
## DM
following lines6−11.  Specifically, the choice
Algorithm 1Training algorithm of CBDM.
1:forEvery batch of size Ndo
## 2:for(x
## (i)
## 0
## ,y
## (i)
)in this batchdo
3:Sampleε
## (i)
∼N(0,I),t∼U({0,1,...,T})
## 4:x
## (i)
t
## =
## √
## ̄α
t
x
## (i)
## 0
## +
## √
## 1− ̄α
t
ε
## (i)
5:CalculateL
## DM
## =∥ε
## (i)
## −ε
θ
## (x
## (i)
t
## ,y
## (i)
## )∥
## 2
6:Sampley
## ′(i)
fromq
## ⋆
## Y
7:Calculate the first regularization term
## 8:L
r
## =tτ||ε
θ
## (x
## (i)
t
## ,y
## (i)
## )−sg(ε
θ
## (x
## (i)
t
## ,y
## ′(i)
## ))||
## 2
9:Calculate the regularization commitment term
## 10:L
rc
## =tτ||sg(ε
θ
## (x
## (i)
t
## ,y
## (i)
## ))−ε
θ
## (x
## (i)
t
## ,y
## ′(i)
## )||
## 2
11:Update withL
## CBDM
## =L
## DM
## +L
r
+γL
rc
12:end for
13:end for
of the regularization weightτaffects the sharpness of the
density  ratio
p
θ
## (x
t
## )
p
## ⋆
θ
## (x
t
## )
.   For  the  theoretical  analysis,  please
refer to our proof of Prop. (2) in the Appendix.   In addi-
tion, the choice of the sampling setYis another important
perspective of CBDM, which depends on the target distri-
bution  we  wish  to  adjust.   Without  loss  of  generality,  we
discuss  two  cases  here.   On  the  one  hand,  we  can  adjust
the label distribution to a class-balanced label distribution,
where  we  sample  labels  to  constructY
bal
.   On  the  other
hand, if the data distribution is heavily long-tailed, we can
also target the adjusted distribution to a relatively less class-
imbalanced distribution for stabilized training.  In our ex-
periments, we show CBDM can work well for both cases in
different mechanisms.
Moreover, we observe that naively optimizing with this
loss  could  make  the  model  collapse  to  some  trivial  solu-
tions, where the model outputs the same result regardless of
the conditionyand thus degenerates the conditional gener-
ation performance. Therefore, we follow previous works to
apply a stop gradient operation [4, 46] to prevent this issue.
The final loss of CBDM is
## L
τ,γ,Y
## CBDM
## (x
t
## ,y,t,ε) =∥ε
θ
## (x
t
## ,y)−ε∥
## 2
## +
τt
## |Y|
## X
y
## ′
## ∈Y
## (∥ε
θ
## (x
t
## ,y)−sg(ε
θ
## (x
t
## ,y
## ′
## ))∥
## 2
## +γ∥sg(ε
θ
## (x
t
## ,y))−ε
θ
## (x
t
## ,y
## ′
## )∥
## 2
## ),
## (4)
where “sg(·)” denotes the stop gradient operation;τ,γare
weights for the regularization and the commitment term re-
spectively withγset to
## 1
## 4
in a default setting, and|Y|de-
notes the number of elements in the label set.
## 18437

(a) DDPM / Tail Class 94(b) CBDM / Tail Class 94
(c) DDPM / Body Class 62(d) CBDM / Body Class 62
Figure  3.Comparison  of  image  generation  on  heavily  tail-
distributed  (94)  and  mild  tail-distributed  (62)  classes  between
DDPM and CBDM.
- Experimental results
4.1. Experimental setup
DatasetsWe  first  chose  two  common  datasets  that  are
widely   used   in   the   domain   of   image   synthesis,   CI-
FAR10/CIFAR100, and their corresponding long-tailed ver-
sions CIFAR10LT and CIFAR100LT. The construction of
CIFAR10LT and CIFAR100LT follows [3], where the size
decreases exponentially with its category index according
to the imbalance factorimb= 0.01.  We also conduct ex-
periments on 3 higher-resolution datasets, whose details and
results could be found in the Appendix.
Implementation  detailsWe  strictly  follow  the  training
configurations  of  the  baseline  models.    For  DDPM,  we
set  diffusion  scheduleβ
## 1
## =  10
## −4
andβ
## T
## =  0.02with
T=  1,000, and optimize the network with an Adam op-
timizer  whose  learning  rate  is0.0002after  5,000  epochs
of warmup.   Considering that the size and semantic com-
plexity of the datasets vary greatly, we choose appropriate
epochs for each dataset. In the ablation studies, we also in-
clude EDM [20] and Diffusion-ViT [48] as our backbones
to show the compatibility with different architectures and
score-matching methods. Note that, when a model does not
use conditional input, we follow [7] to slightly modify the
backbone  [8] by adding an extra embedding layer.
BaselinesWe  first  adopt  following  classic  methods  to
build baselines:  re-sampling (RS), soft re-sampling meth-
ods (RS-SQRT) [27], and augmentation-based methods in-
cluding  Differentiable  Augmentation  (DiffAug)  [50]  and
Adaptive  Augmentation  (ADA)  [21].   Besides  diffusion-
based baselines, previous state-of-the-art generative models
that study the long-tailed distribution,e.g.,CBGAN [35],
and group spectral regularization for GANs [34], are also
included in the comparison.  Precisely, RS follows the uni-
form  class  distribution  and  RS-SQRT  [27]  samples  with
a  probability  determined  by  the  square  root  of  class  fre-
quency. And DiffAug [50] is directly applied to all training
images following their default threshold.   Following [20],
ADA [21] is not only applied to images, the augmentation
pipeline is also encoded as a condition via an additional em-
bedding layer in the U-Net.
MetricsCBDM  and  corresponding  baselines  are  evalu-
ated in terms of both generation diversity and fidelity via
Frechet Inception Distance (FID) [10], Inception Score (IS)
[40], Recall [23] andF
β
[39].  We measure Recall andF
β
using Inception-V3 features, and take respectivelyK= 5
for Recall, 1/8 and 8 for the threshold inF
β
, and 20 times
of class number as the clustering number ofF
β
to capture
the inner class variance.  To this end, Recall andF
## 8
can be
regarded as diversity metrics and IS andF
## 1/8
are more in-
clined to measure fidelity.  In evaluation, we take the class-
balanced  version  corresponding  to  those  used  in  training,
and  all  metrics  are  measured  with  50k  generated  images
(10k for ablation experiments). The classifier-free guidance
is adapted in sampling and we tune the guidance strengthω
for both baselines and our method to ensure the best perfor-
mance.  Specifically, we takeω= 1.6,0.8,1.0,0.8respec-
tively for the four CIFAR datasets mentioned above.
4.2. Main results
In this part, we consider DDPM as the most direct base-
line.   As  shown  in  Table  1,  CBDM  overall  outperforms
DDPM on all datasets except the IS is slightly lower on CI-
FAR10.  For datasets with more classes, the improvements
are more significant in terms of both diversity and fidelity,
as  on  CIFAR100LT  and  CIFAR100.   We  further  investi-
gate the performance of diffusion models when combined
with  augmentation-based  and  re-sampling  methods.   Sur-
prisingly, except for ADA, we consistently observe a perfor-
mance degradation when these methods are compared with
the vanilla DDPM. Moreover, adding the ADA augmenta-
tion with CBDM training leads to a further improvement in
terms of both diversity and fidelity.
As a qualitative justification, Figure 3 provides the visu-
alization comparison between DDPM and CBDM on a rel-
atively mild tail-distributed class (62) and a tail-distributed
class (94).  We remark that CBDM generates more diverse
images.   For  example,  on  the  tail  class  94,  the  cabinets
## 18438

DatasetModelFID↓F
## 8
↑Recall↑IS↑F
## 1/8
## ↑
## CIFAR100LTDDPM [11]7.380.850.5213.110.88
## +ADA [21]6.160.910.5712.710.90
+DiffAug [50]9.190.880.4711.560.86
## + RS [27]10.500.650.4912.600.83
## +SQRT- RS [27]9.720.660.4713.470.83
CBDM (ours)6.26
## (−1.12)
## 0.91
## (+0.06)
## 0.57
## (+0.05)
## 13.24
## (+0.13)
## 0.89
## (+0.01)
## +ADA [21]5.81
## (−1.57)
## 0.91
## (+0.06)
## 0.57
## (+0.05)
## 13.34
## (+0.23)
## 0.90
## (+0.02)
## CIFAR100DDPM3.110.970.6513.650.96
CBDM (ours)2.72
## (−0.39)
## 0.97
## (±0)
## 0.67
## (+0.02)
## 14.03
## (+0.38)
## 0.96
## (±0)
## CIFAR10LTDDPM5.760.970.579.170.95
CBDM (ours)5.46
## (−0.30)
## 0.97
## (±0)
## 0.59
## (+0.02)
## 9.28
## (+0.11)
## 0.95
## (±0)
## CIFAR10DDPM3.160.990.649.800.98
CBDM (ours)3.03
## (−0.13)
## 0.99
## (±0)
## 0.65
## (+0.01)
## 9.63
## (−0.17)
## 0.98
## (±0)
Table 1.  CBDM performance on different datasets.  In the table, the first three columns are diversity-related, and we mark the best results
in bold. As the comparison between CBDM and DDPM is more straightforward, we mark the performance gain in parentheses next to the
CBDM results, using blue and red to indicate improvements and degradation respectively.
## PTFTYFIDF
## 8
RecallIS
## CIFAR-Y
train
## 6.260.910.5713.24
## 100LTY
bal
## 6.100.940.6412.29
## Y
sqrt
## 6.000.930.5913.00
## CIFARCIFAR-7.380.850.5213.11
## 100LT100LTY
train
## 6.200.880.5413.36
## Y
bal
## 5.850.920.6013.29
## CIFARCIFAR-5.390.660.589.43
## 10010LTY
train
## 5.050.670.609.48
## Y
bal
## 5.900.660.609.49
Table 2.  CBDM performance for 3 mechanisms under different
regularization sampling setY.  ColumnPTindicates the dataset
used  in  (pre)training,  columnFTindicates  the  dataset  used  for
fine-tuning.   The rows whereYmarked as “-“ represent the re-
sults with DDPM, and rows whereFTmarked as “-“ represent the
results of CBDM trained from scratch.  Three diversity-preferred
metrics and the best results are in bold for emphasis.
shown in Figure 3b have more color and texture, which jus-
tifies the improvements of diversity-related metrics in Ta-
ble 1.  On the contrary, DDPM only generates images that
are highly similar to the training data.
Case-by-case study.To better understand the generation
conditioned  on  each  class,  we  compare  the  FID  case-by-
case between DDPM and CBDM on each class. The results
are shown in Figure 4. Compared to DDPM, CBDM shows
more  consistent  improvements  for  the  class  index  greater
than 40. In the tail classes, CBDM outperforms than DDPM
the FID by a more significant margin.
## 020406080100
## Class Index
## 10
## 0
## 10
## 20
## FID(DDPM) - FID(CBDM)
## Better
## Worse
Figure 4.  FID improvement per class compared to DDPM. The
curve is smoothened by a moving average of 5 unit.
The choice of label setYWe investigate the effects of
label distribution for CBDM under three mechanisms.  The
first mechanism is the vanilla CBDM given in Algorithm 1
which  trains  a  diffusion  model  with  regularization  from
scratch. Besides, we also study CBDM with two fine-tuning
configurations.  The first setting concerns using CBDM to
adjust an existing model pre-trained on a long-tailed dataset
to improve its performance.  The second one refers to fine-
tuning a pre-trained model to adapt to a smaller dataset with
an  imbalanced  class  distribution  using  CBDM.  For  those
mechanisms, we consider three settings for sampling the la-
bel  set:  (1)  a  label  distribution  similar  to  the  training  set
(denotedY
train
);  (2)  a  totally  uniform  label  distribution
(denotedY
bal
); (3) a less long-tailed distribution compared
toY
train
, whose class frequency is the square root of the
original one (denotedY
sqrt
## ).
The results are shown in Table 2, where we found that
CBDM performs well on all these configurations.   As we
## 18439

DatasetModelFIDIS
CIFARBigGAN+DA+R
## LC
## [45]
## (2021)
## 2.99-
## 100DDPM3.1113.65
CBDM (ours)2.7214.03
## CIFARCBGAN [35]
## (2021)
## 28.17-
## 100LTDDPM12.3112.69
CBDM (ours)+ADA [21]10.5012.81
## CIFARCBGAN [35]
## (2021)
## 32.93-
## 10LT
SNGAN [30]+gSR [34]
## (2022)
## 18.587.80
## DDPM9.689.00
CBDM (ours)9.389.12
Table 3. Comparision with long-tailed SoTAs on CIFAR. Follow-
ing their setting, all methods are evaluated with 10k generated im-
ages and ground truth images are from their validation dataset. The
publish year of every baselines are marked in next to the citation.
Training dataPrecisonRecall
## CIFAR1000.700.67
## CIFAR100LT0.450.39
+ DDPM gens (50k)0.48
## (+0.03)
## 0.44
## (+0.05)
+ CBDM gens (50k)0.49
## (+0.04)
## 0.47
## (+0.08)
Table 4.  Recognition results of different training data.  All con-
figurations are evaluated on the testing set of normal CIFAR100.
Gains based on CIFAR100LT dataset is noted in blue.
analyzed in section 3.3, when the empirical distribution of
Yhas a significant difference from the label distribution of
the dataset, adjusting withY
bal
could hurt training stabil-
ity and result in performance degradation.  We observe that
## Y
bal
only  shows  better  performance  regarding  generation
diversity, while the generation fidelity has an obvious gap
than the other settings. Using a relatively mild setY
sqrt
has
better performance in terms of IS and preserves the genera-
tion diversity, which leads to the best FID among all these
settings. On the contrary, when a pre-trained model is avail-
able, fine-tuning on top of it ensures better stability, and we
can observe in such case usingY
bal
has significant improve-
ment than the vanilla DDPM or fine-tuning usingY
train
## .
When adapting to a different dataset, we also observe using
## Y
bal
causes stability issue and produce undesired results.
Enhancement of training classifiers on long-tailed data
As training models on long-tailed data usually leads to un-
desired  classification  results,  we  investigate  whether  the
generated  data  could  help  improve  the  classifier  trained
on long-tailed data as a complementary evaluation metrics.
Here,  we  train  ResNet-32  models,  respectively  with  CI-
FAR100, CIFAR100LT and CIFAR100LT augmented with
DDPM  and  CBDM  generations  (50k  samples).   Table  4
shows  that  the  training  on  long-tailed  data  results  in  se-
BackboneFIDF
## 8
RecallISF
## 1/8
## EDM [20]8.640.820.4812.160.86
+CBDM (ours)7.970.880.5212.010.86
Diffuse-ViT [48]37.60.760.457.660.62
+CBDM (ours)30.00.810.518.000.65
Table 5.   CBDM performance using different backbones on CI-
FAR100LT dataset. Three diversity-biased metrics.
DatasetFID↓F
## 8
↑Recall↑IS↑F
## 1/8
## ↑
## CIFAR3.440.970.6913.110.96
## 100
## (+0.72)(±0)(+0.02)(−0.92)(±0)
## CIFAR6.270.930.6112.560.89
## 100LT(+0.01)(+0.02)(+0.04)(−0.68)(±0)
Table 6.  Sampling with 100 DDIM steps on CIFAR100 and CI-
FAR100LT.  The  difference  compared  with  those  using  DDPM
steps are reported below each result.
vere performance degradation. When augmenting the long-
tailed data with the generated data,  we observe fairly im-
provements in the trained classifier.  Moreover, comparing
the results between DDPM and CBDM, we observe that the
improvements by using CBDM are more significant, which
validates the effectiveness of our approach.  Especially, the
gain on recall is more remarkable than the gain on preci-
sion, which means that the generated images are more di-
verse than DDPM and justifies the results in Table 1.
Comparison  with  other  benchmarksWe  include  the
representative state-of-the-art long-tailed generative model-
ing approaches in the comparison, most of which are based
on GANs since they suffer more severe problems when the
training data is skewed.  For a fair comparison, we strictly
follow their evaluation setting and the results are reported
in Table 3.  From the results, DDPM surpasses the perfor-
mance of the baselines on all the datasets, and CBDM ex-
hibits even stronger performance.
## 4.3. Ablations
Compatibility  with  different  backbonesWe  first  in-
vestigate  the  performance  of  our  method  with  different
diffusion  models.    Without  loss  of  generality,  we  adopt
EDM  [20]  and  Diffusion-ViT  [48],  which  use  a  differ-
ent score-matching method and different denoising network
backbone.  As shown in Table 5, compared with the corre-
sponding baselines, CBDM improves the generation result
on CIFAR100LT dataset for both backbones,  demonstrat-
ing the compatibility with a variety of backbones and the
effectiveness of handling long-tailed data.
CBDM with DDIM samplingApart from DDPM reverse
sampling  and  neural  SDE  methods,  recent  deterministic
ODE methods such as DDIM [43] are widely used in the
## 18440

Figure 5. FID/IS score under different regularization weightτ
Figure 6. FID/IS score under different guidance strengthω
generation process of diffusion models.   We also conduct
experiments  to  study  whether  CBDM  is  compatible  with
DDIM steps. Table 6 shows the comparison of using DDPM
and DDIM reverse steps when the generation steps are com-
pressed to1/10.  We note on normal CIFAR100, FID and
IS become slightly worse than using 1000 DDPM steps. On
CIFAR100LT, except for IS, all metrics remain almost the
same as using DDPM steps.
Effects of hyperparametersWe tested the effects of reg-
ularization weightτin CBDM, which is set in defaultτ=
τ
## 0
= 0.001so that the weight does not surpass1at all steps.
As shown in Figure 5, we search weights across different
scales and found that the optimal weight isτ=τ
## 0
## .  The
results also point out thatτshould not be too large or too
small. We also tested the difference between CBDM models
trained using commitment lossL
rc
and those trained with-
out it. Our experiments show that the FID score of the mod-
els using the commitment loss (8.30) are significantly lower
than those of the models not using it (8.84).
Guidance  strengthωAs  we  adapt  the  Classifier-Free
Guidance (CFG) during sampling, we also show the effects
of guidance strengthω.  In Figure 6, we searchedωfrom0
to2with an interval of0.2and compare FID and IS score
of  DDPM  and  CBDM  models.   We  observe  that  the  FID
of CBDM remains decreasing at larger guidance strengths
compared to DDPM; and although the IS of CBDM is con-
sistently weaker than that of DDPM, they reach the same
level when the guidance strength reaches2.0, where the FID
of CBDM (6.75) is still significantly lower than the best FID
of DDPM (7.38).
Fidelity-diversity    control[12]    describes    the    phe-
nomenon  where  excessive  guidance  strengthωtends  to
lead to better fidelity at the price of overfitting and diver-
sity  degradation,  as  a  fidelity-diversity  tradeoff.   CBDM
Figure  7.    Image  fidelity  and  diversity  controlled  by  guidance
strengthωand regularization weightτ.
additionally  provides  another  hyperparameter:   the  regu-
larization weightτthat promotes model diversity through
cross-class  information  interaction.    Figure  7  shows  the
body class (53) generation results under different guidance
strengths and different regularization weight.  We observe
that  a  DDPM  model  with  high  guidance  strength  is  able
to  generate  very  realistic  results,  but  its  images  overlap
almost exactly with a sample in the training set. In contrast,
the  guidance  term  of  CBDM  does  not  drastically  change
the image content,  but rather refines the image content to
be  closer  to  the  selected  class  than  the  unguided  result.
This  means  that  CBDM  retains  more  of  the  diversity  of
the  unguided  case  and  effectively  enhances  the  category
information with guide terms.
## 5. Conclusion
In this paper, we focus on the problem when the train-
ing  data  is  imbalanced  and  the  diffusion  model  drops  in
generation  quality  on  tail  classes.We  first  establish  a
low baseline for this task by examining the most common
paradigms in long-tail scenarios and in training generative
models with limited data. Thereafter, we propose the Class-
Balancing  Diffusion  Model  (CBDM)  through  theoretical
analysis.  This approach can be implemented very cleanly
in the training of any conditional diffusion model and thus
has the potential to be widely used in other fields.  Our ex-
periments show that the CBDM approach significantly im-
proves model generation diversity with high fidelity, on both
class-balanced and class-imbalanced datasets.
## 6. Acknowledgement
This    work    is    supported    by    the    National    Key
R&D  Program  of  China  (No.2022ZD0160703,   No.
2022ZD0160702),  National  Natural  Science  Foundation
of  China  (62271308),  STCSM  (No.    22511106101,  No.
18DZ2270700,  No.   22511105700,  No.   21DZ1100100),
111 plan (No.  BP0719010), and State Key Laboratory of
UHD Video and Audio Production and Presentation.
## 18441

## References
[1]  Jan Kautz Arash Vahdat, Karsten Kreis.  Score-based gener-
ative modeling in latent space. InNeurIPS, 2021. 1
[2]  Andrew Brock, Jeff Donahue, and Karen Simonyan.  Large
scale GAN training for high fidelity natural image synthesis.
InICLR, 2019. 1, 2
## [3]  Kaidi  Cao,  Colin  Wei,  Adrien  Gaidon,  Nikos  Arechiga,
and Tengyu Ma.   Learning imbalanced datasets with label-
distribution-aware margin loss.NeurIPS, 32, 2019. 5
[4]  Xinlei Chen and Kaiming He. Exploring simple siamese rep-
resentation learning. InCVPR, pages 15750–15758, 2021. 4
[5]  Peng  Chu,  Xiao  Bian,  Shaopeng  Liu,  and  Haibin  Ling.
Feature  space  augmentation  for  long-tailed  data.    In  An-
drea   Vedaldi,   Horst   Bischof,   Thomas   Brox,   and   Jan-
Michael  Frahm,   editors,ECCV,  pages  694–710,   Cham,
## 2020. Springer International Publishing. 2
[6]  Ishan Deshpande,  Ziyu Zhang,  and Alexander G Schwing.
Generative modeling using the sliced Wasserstein distance.
InCVPR, pages 3483–3491, 2018. 2
[7]  Prafulla Dhariwal and Alexander Quinn Nichol.   Diffusion
models beat GANs on image synthesis.  In A. Beygelzimer,
Y.  Dauphin,  P.  Liang,  and  J.  Wortman  Vaughan,  editors,
NeurIPS, 2021. 1, 2, 5
## [8]  Alexey  Dosovitskiy,  Lucas  Beyer,  Alexander  Kolesnikov,
## Dirk   Weissenborn,   Xiaohua   Zhai,   Thomas   Unterthiner,
## Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al.   An image is worth 16x16 words:  Trans-
formers  for  image  recognition  at  scale.arXiv  preprint
arXiv:2010.11929, 2020. 5
[9]  Didrik Nielsen Giorgio Giannone and Ole Winther. Few-shot
diffusion models.arXiv preprint arXiv:2205.15463, 2021. 2
## [10]  Martin   Heusel,   Hubert   Ramsauer,   Thomas   Unterthiner,
Bernhard Nessler, and Sepp Hochreiter.   Gans trained by a
two time-scale update rule converge to a local nash equilib-
rium.  InNeurIPS, page 6629–6640, Red Hook, NY, USA,
## 2017. Curran Associates Inc. 5
[11]  Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. InNeurIPS, 2020. 3, 4, 6
[12]  Jonathan  Ho  and  Tim  Salimans.    Classifier-free  diffusion
guidance.arXiv preprint arXiv:2207.12598, 2022. 2, 3, 4, 8
## [13]  Jonathan  Ho,  Tim  Salimans,  Alexey  Gritsenko,  William
Chan, Mohammad Norouzi, and David J Fleet.   Video dif-
fusion models.arXiv preprint arXiv:2204.03458, 2022. 1
## [14]  Greg  Holste,  Song  Wang,  Ziyu  Jiang,  Thomas  C.  Shen,
George  L.  Shih,  Ronald  M.  Summers,  Yifan  Peng,  and
Zhangyang Wang.   Long-tailed classification of thorax dis-
eases on chest x-ray: A new benchmark study.Data augmen-
tation, labelling, and imperfections : second MICCAI work-
shop,  DALI 2022,  held in conjunction with MICCAI 2022,
Singapore, September 22, 2022, proceedings. DALI (Work-
shop), 13567:22–32, 2022. 1
## [15]  Grant  Van  Horn,  Oisin  Mac  Aodha,  Yang  Song,  Yin  Cui,
## Chen Sun,  Alexander Shepard,  Hartwig Adam,  Pietro Per-
ona,  and Serge J. Belongie.   The inaturalist species classi-
fication and detection dataset.  InCVPR, pages 8769–8778,
## 2018. 1
[16]  Chen  Huang,  Yining  Li,  Chen  Change  Loy,  and  Xiaoou
Tang.   Learning deep representation for imbalanced classi-
fication. InCVPR, pages 5375–5384, 2016. 2
## [17]  Myeonghun  Jeong,  Hyeongju  Kim,  Sung  Jun  Cheon,  By-
oung Jin Choi, and Nam Soo Kim.  Diff-TTS: A Denoising
Diffusion Model for Text-to-Speech.   InProc. Interspeech
2021, pages 3605–3609, 2021. 1
[18]  Alexia Jolicoeur-Martineau, Remi Piche-Taillefer, R
## ́
emi Ta-
chet  des  Combes,  and  Ioannis  Mitliagkas.Adversarial
score  matching  and  improved  sampling  for  image  genera-
tion.arXiv preprint arXiv:2009.05475. 1
[19]  Haeyong  Kang,  Thang  Vu,  and  Chang  D.  Yoo.   Learning
imbalanced datasets with maximum margin loss.   InICIP,
pages 1269–1273, 2021. 1
[20]  Tero  Karras,  Miika  Aittala,  Timo  Aila,  and  Samuli  Laine.
Elucidating  the  design  space  of  diffusion-based  generative
models.arXiv preprint arXiv:2206.00364, 2022. 5, 7
## [21]  Tero  Karras,  Miika  Aittala,  Janne  Hellsten,  Samuli  Laine,
Jaakko Lehtinen, and Timo Aila.  Training generative adver-
sarial networks with limited data.   InNeurIPS, Red Hook,
NY, USA, 2020. Curran Associates Inc. 1, 2, 5, 6, 7
[22]  Minsu Ko,  Eunju Cha,  Sungjoo Suh,  Huijin Lee,  Jae-Joon
Han, Jinwoo Shin, and Bohyung Han. Self-supervised dense
consistency regularization for image-to-image translation. In
CVPR, pages 18280–18289, 2022. 2
## [23]  Tuomas Kynk
## ̈
a
## ̈
anniemi, Tero Karras, Samuli Laine, Jaakko
Lehtinen, and Timo Aila. Improved precision and recall met-
ric for assessing generative models.NeurIPS, 32, 2019. 5
[24]  Wen Li, Limin Wang, Wei Li, Eirikur Agustsson, and Luc
Van Gool.  Webvision database:  Visual learning and under-
standing from web data.arXiv preprint arXiv:1708.02862,
## 2017. 1
[25]  Jialun  Liu,  Yifan  Sun,  Chuchu  Han,  Zhaopeng  Dou,  and
Wenhui Li. Deep representation learning on long-tailed data:
A learnable embedding augmentation perspective. InCVPR,
pages 2967–2976, 2020. 2
[26]  Guanxiong Luo, Martin Heide, and Martin Uecker.  MRI re-
construction via data driven markov chain with joint uncer-
tainty estimation.CoRR, abs/2202.01479, 2022. 1
## [27]  Dhruv   Mahajan,   Ross   Girshick,   Vignesh   Ramanathan,
## Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe,
and Laurens van der Maaten. Exploring the limits of weakly
supervised pretraining.   In Vittorio Ferrari, Martial Hebert,
Cristian Sminchisescu, and Yair Weiss, editors,ECCV, pages
## 185–201, Cham, 2018. Springer International Publishing.  1,
## 2, 5, 6
## [28]  Chenlin Meng, Yang Song, Jiaming Song, Jiajun Wu, Jun-
Yan Zhu, and Stefano Ermon.  Sdedit:  Image synthesis and
editing with stochastic differential equations.arXiv preprint
arXiv:2108.01073, 2021. 1
## [29]  Aditya  Krishna  Menon,  Sadeep  Jayasumana,  Ankit  Singh
Rawat,  Himanshu  Jain,  Andreas  Veit,  and  Sanjiv  Kumar.
Long-tail learning via logit adjustment.  InICLR. OpenRe-
view.net, 2021. 1, 2
[30]  Takeru  Miyato,  Toshiki  Kataoka,  Masanori  Koyama,  and
Yuichi  Yoshida.   Spectral  normalization  for  generative  ad-
versarial networks. InICLR, 2018. 7
## 18442

## [31]  Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh,
## Pranav   Shyam,    Pamela   Mishkin,    Bob   Mcgrew,    Ilya
Sutskever,  and Mark Chen.   GLIDE: Towards photorealis-
tic image generation and editing with text-guided diffusion
models.  In Kamalika Chaudhuri, Stefanie Jegelka, Le Song,
Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors,In-
ternational Conference on Machine Learning (ICML), vol-
ume  162  ofProceedings  of  Machine  Learning  Research,
pages 16784–16804. PMLR, 17–23 Jul 2022. 1, 2
[32]  Wanli Ouyang, Xiaogang Wang, Cong Zhang, and Xiaokang
Yang.  Factors in finetuning deep model for object detection
with long-tail distribution.  InCVPR, pages 864–873, 2016.
## 2
[33]  Yuntian  Gu  Quanlin  Wu,   Hang  Ye.Guided  diffusion
model for adversarial purification from random noise.arXiv
preprint arXiv:2206.10875, 2022. 1, 2
## [34]  Harsh  Rangwani,  Naman  Jaswani,  Tejan  Karmali,  Varun
Jampani, and R. Venkatesh Babu.  Improving gans for long-
tailed  data  through  group  spectral  regularization.   In  Shai
## Avidan, Gabriel Brostow, Moustapha Ciss
## ́
e, Giovanni Maria
Farinella, and Tal Hassner, editors,ECCV, pages 426–442,
## Cham, 2022. Springer Nature Switzerland. 1, 5, 7
[35]  Harsh Rangwani, Konda Reddy Mopuri, and R. Venkatesh
Babu.  Class balancing gan with a classifier in the loop.  In
Cassio de Campos and Marloes H. Maathuis, editors,Pro-
ceedings  of  the  Thirty-Seventh  Conference  on  Uncertainty
in Artificial Intelligence, volume 161 ofProceedings of Ma-
chine Learning Research, pages 1618–1627. PMLR, 27–30
## Jul 2021. 5, 7
## [36]  Robin  Rombach,   Andreas  Blattmann,   Dominik  Lorenz,
Patrick Esser, and Bj
## ̈
orn Ommer. High-resolution image syn-
thesis with latent diffusion models.  InCVPR, pages 10684–
## 10695, 2022. 2
[37]  Kevin  Roth,   Aurelien  Lucchi,   Sebastian  Nowozin,   and
Thomas Hofmann.  Stabilizing training of generative adver-
sarial  networks  through  regularization.    InNeurIPS,  page
2015–2025, Red Hook, NY, USA, 2017. Curran Associates
## Inc. 1, 2
## [38]  Chitwan  Saharia,  William  Chan,  Huiwen  Chang,  Chris  A.
Lee,  Jonathan Ho,  Tim Salimans,  David J. Fleet,  and Mo-
hammad Norouzi.  Palette:  Image-to-image diffusion mod-
els.ACM SIGGRAPH 2022 Conference Proceedings, 2022.
## 1
## [39]  Mehdi S. M. Sajjadi, Olivier Bachem, Mario Lucic, Olivier
Bousquet, and Sylvain Gelly.  Assessing generative models
via precision and recall.  InNeurIPS, page 5234–5243, Red
Hook, NY, USA, 2018. Curran Associates Inc. 5
## [40]  Tim  Salimans,  Ian  Goodfellow,  Wojciech  Zaremba,  Vicki
Cheung,  Alec Radford,  Xi Chen,  and Xi Chen.   Improved
techniques for training gans.   In D. Lee,  M. Sugiyama,  U.
Luxburg,  I. Guyon,  and R. Garnett,  editors,NeurIPS, vol-
ume 29. Curran Associates, Inc., 2016. 5
## [41]  Vikash  Sehwag,  Caner  Hazirbas,  Albert  Gordo,  Firat  Oz-
genel,  and  Cristian  Canton.   Generating  high  fidelity  data
from low-density regions using diffusion models.  InCVPR,
pages 11492–11501, 2022. 2
[42]  Jascha    Sohl-Dickstein,Eric    A.    Weiss,Niru    Mah-
eswaranathan,   and  Surya  Ganguli.Deep  unsupervised
learning  using  nonequilibrium  thermodynamics.arXiv
preprint arXiv:1503.03585, 2015. 2
[43]  Jiaming Song, Chenlin Meng, and Stefano Ermon.  Denois-
ing diffusion implicit models, 2021. 2, 7
[44]  Yang Song, Liyue Shen, Lei Xing, and Stefano Ermon. Solv-
ing inverse problems in medical imaging with score-based
generative models.arXiv preprint arXiv:2111.08005, 2022.
## 1
[45]  Hung-Yu Tseng, Lu Jiang, Ce Liu, Ming-Hsuan Yang, and
Weilong Yang. Regularizing generative adversarial networks
under limited data. InCVPR, pages 7917–7927, 2021. 2, 7
[46]  Aaron    van    den    Oord,Oriol    Vinyals,and    Koray
Kavukcuoglu.Neural   discrete   representation   learning.
InNeurIPS,  NIPS’17,  page  6309–6318,  Red  Hook,  NY,
USA, 2017. Curran Associates Inc. 4
## [47]  Jianfeng  Wang,  Thomas  Lukasiewicz,  Xiaolin  Hu,  Jianfei
Cai, and Zhenghua Xu.   Rsg:  A simple but effective mod-
ule for learning imbalanced datasets. InCVPR, pages 3783–
## 3792, 2021. 2
[48]  Xiulong Yang, Sheng-Min Shih, Yinlin Fu, Xiaoting Zhao,
and Shihao Ji.  Your vit is secretly a hybrid discriminative-
generative diffusion model.ArXiv, abs/2208.07791, 2022. 5,
## 7
[49]  Xi Yin, Xiang Yu, Kihyuk Sohn, Xiaoming Liu, and Man-
mohan Chandraker. Feature transfer learning for face recog-
nition with under-represented data.  InCVPR, pages 5697–
## 5706, 2019. 2
[50]  Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, and Song
Han.Differentiable  augmentation  for  data-efficient  gan
training.   InNeurIPS, Red Hook,  NY, USA, 2020. Curran
## Associates Inc. 1, 2, 5, 6
## 18443